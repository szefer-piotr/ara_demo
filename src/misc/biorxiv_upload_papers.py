#! /usr/bin/env python3

import logging
import requests
import json
import boto3
import os
import argparse
import zipfile
import io

from botocore.config import Config
from botocore.exceptions import ClientError, ParamValidationError
from datetime import datetime, timedelta
from typing import List, Optional, BinaryIO
from minio import Minio
from dotenv import load_dotenv
from dataclasses import dataclass

load_dotenv()

S3_BUCKET = "biorxiv-src-monthly"
DEFAULT_REGION = "us-east-1"

# MinIO configuration - use port 9002 for API
MINIO_ENDPOINT = os.getenv("MINIO_ENDPOINT", "localhost:9002")
MINIO_ACCESS_KEY = os.getenv("MINIO_ACCESS_KEY", "minioadmin")
MINIO_SECRET_KEY = os.getenv("MINIO_SECRET_KEY", "minioadmin")
MINIO_SECURE = os.getenv("MINIO_SECURE", "false").lower() == "true"
MINIO_BUCKET = os.getenv("MINIO_BUCKET", "biorxiv-papers")

@dataclass
class PaperInfo:
    doi: str
    title: str
    authors: str
    author_corresponding: str = ""
    author_corresponding_institution: str = ""
    date: str = ""
    version: str = ""
    type: str = ""
    license: str = ""
    category: str = ""
    abstract: str = ""
    pdf_url: str = ""
    s3_key: Optional[str] = None


def download_and_extract_meca_to_minio(
    s3_client,
    minio_client,
    s3_keys: List[str],
    target_month: str,
    bucket_name: str,
    file_type: str = "pdf"
) -> None:
    """
    Download and extract MECA packages from S3 to MinIO.
    """
    if not s3_keys:
        logging.error("No S3 keys provided")
        return

    logging.info(f"Downloading and extracting {len(s3_keys)} MECA packages from S3 to MinIO...")

    processed_packages = 0
    total_files = 0
    uploaded_files = 0
    skipped_files = 0
    skipped_packages = 0

    for i, package_uuid in enumerate(s3_keys, 1):
        try:
            logging.info(f"Processing package {i}/{len(s3_keys)}: {package_uuid}")

            if package_exists_in_minio(minio_client, bucket_name, package_uuid):
                logging.info(f"Package {package_uuid} already exists in MinIO, skipping...")
                skipped_packages += 1
                continue
            
            # Construct the full S3 key
            full_s3_key = f"Current_Content/{target_month}/{package_uuid}.meca"
            meca_data = download_meca_from_s3(s3_client, full_s3_key)

            if not meca_data:
                logging.error(f"Failed to download MECA package {package_uuid} from S3")
                skipped_packages += 1
                continue

            files_uploaded = extract_and_upload_to_minio(
                minio_client,
                bucket_name,
                package_uuid,
                meca_data,
                file_type
            )

            uploaded_files += files_uploaded
            processed_packages += 1

            logging.info(f"Uploaded {files_uploaded} files from {package_uuid}")
        
        except Exception as e:
            logging.error(f"Error processing package {package_uuid}: {e}")
            skipped_packages += 1
            continue

        logging.info(f"Total packages: {processed_packages}")
        logging.info(f"Total packages skipped: {skipped_packages}")
        logging.info(f"Total files uploaded: {uploaded_files}")
        logging.info(f"Total files skipped: {skipped_files}")
        logging.info(f"Total files: {total_files}")



def download_meca_from_s3(s3_client, s3_key: str) -> Optional[bytes]:
    """
    Download a MECA package from S3.
    """
    try:
        response = s3_client.get_object(
            Bucket=S3_BUCKET, 
            Key=s3_key,
            RequestPayer='requester'
        )
        return response['Body'].read()
    except Exception as e:
        logging.error(f"Failed to download MECA package {s3_key} from S3: {e}")
        return None


def extract_and_upload_to_minio(
    minio_client: Minio,
    bucket_name: str,
    package_uuid: str,
    meca_data: bytes,
    file_type: str = "pdf"
) -> int:
    """
    Extract and upload a MECA package from memory and upload files directly to MinIO.

    Returns:
        Number of files uploaded to MinIO
    """
    uploaded_count = 0

    try:
        meca_buffer = io.BytesIO(meca_data)
        with zipfile.ZipFile(meca_buffer, 'r') as zip_ref:
            for file_info in zip_ref.infolist():
                if file_info.is_dir():
                    continue

                file_name = file_info.filename
                file_extension = os.path.splitext(file_name)[1].lower()

                if file_type == "pdf" and file_extension != ".pdf":
                    continue
                    
                file_content = zip_ref.read(file_info)

                object_key = f"{package_uuid}/{file_name}"

                minio_client.put_object(
                    bucket_name,
                    object_key,
                    io.BytesIO(file_content),
                    len(file_content),
                    content_type=get_content_type(file_extension)
                )

                uploaded_count += 1
                logging.info(f"Uploaded {uploaded_count} files to MinIO")
    
    except Exception as e:
        logging.error(f"Error extracting and uploading to MinIO: {e}")
        return uploaded_count

    return uploaded_count


def package_exists_in_minio(
    minio_client: Minio,
    bucket_name: str,
    package_uuid: str
) -> bool:
    """
    Check if a package exists in MinIO.
    """
    try:
        objects = list(minio_client.list_objects(bucket_name, prefix=f"{package_uuid}/", recursive=True))
        return len(objects) > 0
    except Exception as e:
        logging.error(f"Package does not exist in MinIO: {e}")
        return False

def get_content_type(file_extension: str) -> str:
    """
    Get the content type for a file extension.
    """
    content_types = {
        ".pdf": "application/pdf",
        ".xml": "application/xml",
        ".json": "application/json",
        ".txt": "text/plain",
        ".html": "text/html",
    }

    return content_types.get(file_extension.lower(), "application/octet-stream")


 
    


def setup_logger(verbosity: int) -> None:
    level = logging.WARNING
    if verbosity == 1:
        level = logging.INFO
    elif verbosity >= 2:
        level = logging.DEBUG
    logging.basicConfig(level=level, format="%(asctime)s %(levelname)s %(message)s", datefmt="%H:%M:%S")

def create_s3_client(aws_access_key_id: str = None, aws_secret_access_key: str = None, 
                     aws_region: str = None, profile: str = None):
    """
    Create an S3 client for accessing bioRxiv bucket.
    
    Args:
        aws_access_key_id: AWS access key (or use environment variable AWS_ACCESS_KEY_ID)
        aws_secret_access_key: AWS secret key (or use environment variable AWS_SECRET_ACCESS_KEY)
        aws_region: AWS region (defaults to us-east-1)
        profile: AWS profile name (alternative to raw keys)
    
    Returns:
        boto3 S3 client configured for requester-pays access
    """
    cfg = Config(region_name=aws_region or DEFAULT_REGION, signature_version="s3v4")
    
    if profile:
        session = boto3.Session(profile_name=profile, region_name=aws_region or DEFAULT_REGION)
    else:
        session = boto3.Session(
            aws_access_key_id=aws_access_key_id or os.getenv("AWS_ACCESS_KEY_ID"),
            aws_secret_access_key=aws_secret_access_key or os.getenv("AWS_SECRET_ACCESS_KEY"),
            region_name=aws_region or DEFAULT_REGION,
        )
    
    return session.client("s3", config=cfg)

def find_meca_packages_in_s3(s3_client, doi: Optional[List[str]], target_month: str = "June_2025") -> Optional[List[str]]:
    """
    Find MECA packages in the bioRxiv S3 bucket by month or a list of DOIs.
    
    Args:
        s3_client: Configured boto3 S3 client
        doi: Paper DOI list (e.g., ["10.1101/2024.01.15.123456", "10.1101/2024.01.15.123457"])
        target_month: Optional month to search in (e.g., "August_2025"), default is June 2025
    
    Returns:
        List of S3 keys if found, None otherwise
    """
    s3_keys = []
    try:
        months_to_search = [f"Current_Content/{target_month}/"]
        
        for month_prefix in months_to_search:
            print(f"   ðŸ“… Searching in {month_prefix}...")
            
            try:
                # Use paginator to handle more than 1000 objects
                paginator = s3_client.get_paginator('list_objects_v2')
                page_iterator = paginator.paginate(
                    Bucket=S3_BUCKET,
                    Prefix=month_prefix,
                    RequestPayer='requester'
                )

                total_objects = 0
                for page in page_iterator:
                    if 'Contents' in page:
                        page_objects = len(page['Contents'])
                        total_objects += page_objects
                        print(f"   ðŸ“„ Found {page_objects} objects in this page...")
                        for obj in page['Contents']:
                            key = obj['Key']
                            s3_keys.append(key)

                print(f"   ðŸ“„ Found {total_objects} objects in {month_prefix}")
                
            except ClientError as e:
                if e.response['Error']['Code'] == 'NoSuchBucket':
                    print(f"   âš ï¸  Month folder {month_prefix} not found")
                else:
                    print(f"   âŒ Error accessing {month_prefix}: {e}")
                continue
        
    except Exception as e:
        print(f"âŒ Error searching for paper: {e}")
        return None

    return s3_keys

def extract_keys_from_s3_keys(s3_keys: list[str]) -> list[str]:
    """
    Extract keys from a list of S3 keys.
    """
    return [k.replace("Current_Content/June_2025/", "").replace(".meca", "") for k in s3_keys]

def check_if_packages_exist_in_minio(minio_client: Minio, bucket_name: str, s3_keys: list[str]) -> list[str]:
    """
    Check if packages exist in the MinIO bucket.
    """
    exists_in_minio = []
    if minio_client.bucket_exists(bucket_name):
        for obj in minio_client.list_objects(bucket_name, prefix="", recursive=True):
            if obj.object_name in s3_keys:
                exists_in_minio.append(True)
            else:
                exists_in_minio.append(False)
    return exists_in_minio

def check_if_unzipped_packages_exist_locally(output_dir: str, s3_keys: list[str]) -> list[str]:
    """
    Check if packages exist locally.
    """
    exists_locally = []
    for k in s3_keys:
        if os.path.exists(os.path.join(output_dir, k)):
            exists_locally.append(True)
        else:
            exists_locally.append(False)
    return exists_locally

def check_if_meca_packages_are_downloaded(output_dir: str, s3_keys: list[str]) -> list[str]:
    """
    Check if packages are unzipped.
    """
    exists_unzipped = []
    for k in s3_keys:
        if os.path.exists(os.path.join(output_dir, k) + '.meca'):
            exists_unzipped.append(True)
        else:
            exists_unzipped.append(False)
    return exists_unzipped

# def download_MECA_packages_from_s3(s3_client, s3_keys: List[str], output_dir: str = "./MECA_packages") -> None:
#     """
#     Download listed MECA packages from the bioRxiv S3 bucket.
    
#     Args:
#         s3_client: Configured boto3 S3 client
#         s3_keys: List of uuids
#         output_dir: Local directory to save the packages
#     """
#     if not os.path.exists(output_dir):
#         os.makedirs(output_dir)
    
#     print(f"ðŸ“¥ Verifying {len(s3_keys)} MECA packages locally and on S3...")
    
#     downloaded_packages = 0
#     skipped_packages = 0
    
#     for i, s3_uuid in enumerate(s3_keys, 1):
#         try:
#             # Reconstruct the full S3 key
#             full_s3_key = f"Current_Content/June_2025/{s3_uuid}.meca"
#             # Create a safe filename from the S3 key
#             safe_filename = s3_uuid.replace('/', '_')
#             output_path = os.path.join(output_dir, safe_filename) + ".meca"
            
#             # Check if file already exists locally
#             exists_packed = check_if_meca_packages_are_downloaded(output_dir, [s3_uuid])
#             if exists_packed[0]:
#                 print(f"   â­ï¸  Package {i}/{len(s3_keys)}: {safe_filename} already exists, skipping...")
#                 skipped_packages += 1
#                 continue
            
#             else:
#                 print(f"   ðŸ“¦ Downloading package {i}/{len(s3_keys)}: {safe_filename}")
#                 s3_client.download_file(
#                     Bucket=S3_BUCKET,
#                     Key=full_s3_key,
#                     Filename=output_path,
#                     ExtraArgs={"RequestPayer": "requester"}
#                 )
#                 downloaded_packages += 1
                
#         except Exception as e:
#             print(f"   âŒ Failed to download {s3_uuid}: {e}")
#             continue
    
#     print(f"âœ… Download summary:")
#     print(f"   ðŸ“¥ Newly downloaded: {downloaded_packages} packages")
#     print(f"   â­ï¸  Skipped (already exist): {skipped_packages} packages")
#     print(f"   ðŸ“Š Total processed: {len(s3_keys)} packages")

# def unzip_MECA_packages(output_dir: str = "./MECA_packages") -> None:
#     """
#     Unzip MECA packages from the local directory.
    
#     Args:
#         output_dir: Local directory to save the packages
#     """
#     extracted_dirs = []
#     meca_files = [f for f in os.listdir(output_dir) if f.endswith(".meca")]
#     print(f"   âœ… Found {len(meca_files)} MECA files in {output_dir}")

#     for i, meca_file in enumerate(meca_files, 1): 
#         try:
#             meca_path = os.path.join(output_dir, meca_file)
#             extract_dir = os.path.join(output_dir, meca_file.replace(".meca", ""))

#             if os.path.exists(extract_dir):
#                 print(f"   â­ï¸  Package {i}/{len(meca_files)}: {meca_file} already unzipped locally, skipping...")
#                 continue

#             print(f"   ðŸ“¦ Extracting MECA package {i}/{len(meca_files)}: {meca_file}")

#             os.makedirs(extract_dir, exist_ok=True)
#             with zipfile.ZipFile(meca_path, 'r') as zip_ref:
#                 zip_ref.extractall(extract_dir)
#             extracted_dirs.append(extract_dir)
#             print(f"   âœ… Successfully extracted to {extract_dir}")

#         except Exception as e:
#             print(f"   âŒ Error extracting {meca_file}: {e}")
#             continue

#     print(f"   âœ… Successfully extracted {len(extracted_dirs)} MECA packages")
#     return extracted_dirs

# def print_MECA_summary(extracted_dirs: List[str]) -> None:
#     """
#     Print a summary of extracted MECA packages.

#     Args:
#     extracted_dirs: List of paths to extracted directories
#     """
#     print("\nðŸ“Š MECA Packages Summary")
#     print("=" * 50)

#     for i, extracted_dir in enumerate(extracted_dirs, 1):
#         print(f"\nðŸ“¦ Package {i}: {os.path.basename(extracted_dir)}")
#         print(f"   ðŸ“ Directory: {extracted_dir}")

#         # Count files by type
#         total_files = 0
#         pdf_files = 0
#         xml_files = 0
#         other_files = 0

#         for root, dirs, files in os.walk(extracted_dir):
#             for file in files:
#                 total_files += 1
#                 if file.endswith('.pdf'):
#                     pdf_files += 1
#                 elif file.endswith('.xml'):
#                     xml_files += 1
#                 else:
#                     other_files += 1

#         print(f"   ðŸ“„ Total files: {total_files}")
#         print(f"   ðŸ“– PDF files: {pdf_files}")
#         print(f"   ðŸ“‹ XML files: {xml_files}")
#         print(f"   ðŸ”§ Other files: {other_files}")   

def test_minio_connection(endpoint: str = None, access_key: str = None, secret_key: str = None, secure: bool = None) -> bool:
    """
    Test the connection to the MinIO server by listing buckets.
    
    Args:
        endpoint: MinIO server endpoint (defaults to MINIO_ENDPOINT)
        access_key: Access key for MinIO (defaults to MINIO_ACCESS_KEY)
        secret_key: Secret key for MinIO (defaults to MINIO_SECRET_KEY)
        secure: Set to True if using HTTPS (defaults to MINIO_SECURE)
    
    Returns:
        True if connection is successful, False otherwise
    """
    try:
        # Use provided parameters or fall back to global configuration
        test_endpoint = endpoint or MINIO_ENDPOINT
        test_access_key = access_key or MINIO_ACCESS_KEY
        test_secret_key = secret_key or MINIO_SECRET_KEY
        test_secure = secure if secure is not None else MINIO_SECURE
        
        print(f"ðŸ” Testing MinIO connection to: {test_endpoint}")
        print(f"   ðŸ”‘ Access Key: {test_access_key}")
        print(f"   ðŸ”’ Secure: {test_secure}")
        
        # Initialize the MinIO client
        client = Minio(
            test_endpoint,
            access_key=test_access_key,
            secret_key=test_secret_key,
            secure=test_secure
        )
        
        # Attempt to list buckets to verify the connection
        buckets = client.list_buckets()
        print(f"âœ… Successfully connected to MinIO!")
        print(f"   ðŸ“¦ Found {len(buckets)} buckets:")
        for bucket in buckets:
            print(f"      - {bucket.name} (created: {bucket.creation_date})")
        
        # Test if our target bucket exists
        if client.bucket_exists(MINIO_BUCKET):
            print(f"   ðŸŽ¯ Target bucket '{MINIO_BUCKET}' exists")
        else:
            print(f"   âš ï¸  Target bucket '{MINIO_BUCKET}' does not exist")
        
        return True
        
    except Exception as e:
        print(f"âŒ MinIO connection test failed: {e}")
        return False

def create_minio_client() -> Minio:
    """
    Create a MinIO client for uploading files.
    
    Returns:
        Configured MinIO client
    """
    try:
        client = Minio(
            MINIO_ENDPOINT,
            access_key=MINIO_ACCESS_KEY,
            secret_key=MINIO_SECRET_KEY,
            secure=MINIO_SECURE
        )
        
        # Ensure the bucket exists
        if not client.bucket_exists(MINIO_BUCKET):
            client.make_bucket(MINIO_BUCKET)
            print(f"âœ… Created MinIO bucket: {MINIO_BUCKET}")
            
        return client
        
    except Exception as e:
        print(f"âŒ Failed to create MinIO client: {e}")
        raise
    
# def upload_files_to_minio(s3_keys: List[str], file_type: str = "pdf") -> None:
#     """
#     Upload given s3 keys, extracted locally or already downloaded from S3, to MinIO.
    
#     Args:
#         s3_keys: List of paths to extracted MECA directories
#         file_type: Type of files to upload ('pdf' for PDF files only, 'all' for all files)
#     """
#     if not s3_keys:
#         print("âš ï¸  No extracted directories provided for upload")
#         print(f"   ðŸ” s3_keys list: {s3_keys}")
#         return
    
#     try:
#         minio_client = create_minio_client()
#         print(f"âœ… MinIO client created successfully")
        
#         total_files = 0
#         uploaded_files = 0
#         skipped_files = 0
        
#         print(f"ðŸ“¤ Starting upload to MinIO bucket: {MINIO_BUCKET}")
#         print(f"ðŸŽ¯ File type filter: {file_type}")
        
#         # extracted_dirs = [f.replace("./MECA_packages/", "") for f in s3_keys]
#         print(f"   ðŸ” Extracted directories LOCALLY: {s3_keys}")
#         s3_keys = [f.replace(".meca", "") for f in s3_keys]
        
#         print(f"   ðŸ” Extracted directories AFTER .meca extension replacement: {s3_keys}")
#         # Debug: Check what each directory contains
#         for i, dir_path in enumerate(s3_keys):
#             print(f"    Directory {i+1}: {dir_path}")
#             print(f"   ðŸ” Exists locally: {os.path.exists(dir_path)}")
#             if os.path.exists(dir_path):
#                 print(f"    Contents: {os.listdir(dir_path)}")
#             else:
#                 print(f"   âŒ Directory does not exist locally!")
        
#         # extracted_dirs = [f.replace("./MECA_packages/", "") for f in extracted_dirs]
        
#         print(f"   ðŸ” Extracted directories AFTER replacement: {s3_keys}")
        
#         for extracted_dir in s3_keys:
#             print(f"\nðŸ“ Processing directory: {extracted_dir}")
#             if not os.path.exists(extracted_dir):
#                 print(f"   âš ï¸  Directory from EXTRACTED_DIRS not found LOCALLY: {extracted_dir}")
#                 continue
            
#             print(f"   âœ… Directory exists, walking through files...")
            
#             # Walk through all files in the extracted directory
#             for root, dirs, files in os.walk(extracted_dir):
#                 print(f"   ðŸ” Files in directory: {files}")
#                 print(f"   ðŸ” Root: {root}")
#                 print(f"   ðŸ” Dirs: {dirs}")
#                 for file in files:
#                     file_path = os.path.join(root, file)
#                     file_extension = os.path.splitext(file)[1].lower()
                    
#                     # Apply file type filtering
#                     if file_type == "pdf" and file_extension != ".pdf":
#                         skipped_files += 1
#                         continue
                    
#                     # Create MinIO object key (maintain directory structure)
#                     rel_path = os.path.relpath(file_path, extracted_dir)
#                     object_key = f"{os.path.basename(extracted_dir)}/{rel_path}"
                    
#                     try:
#                         # Upload file to MinIO using fput_object for better performance
#                         minio_client.fput_object(
#                             MINIO_BUCKET,
#                             object_key,
#                             file_path
#                         )
                        
#                         uploaded_files += 1
#                         print(f"   âœ… Uploaded: {file} -> {object_key}")
                        
#                     except Exception as e:
#                         print(f"   âŒ Failed to upload {file}: {e}")
#                         continue
                    
#                     total_files += 1
        
#         # Print upload summary
#         print(f"\nðŸ“Š Upload Summary:")
#         print(f"   ðŸ“ Directories processed: {len(s3_keys)}")
#         print(f"   ðŸ“„ Total files found: {total_files}")
#         print(f"   ðŸ“¤ Files uploaded: {uploaded_files}")
#         print(f"   â­ï¸  Files skipped: {skipped_files}")
#         print(f"   ðŸŽ¯ File type filter: {file_type}")
#         print(f"   â˜ï¸  MinIO bucket: {MINIO_BUCKET}")
        
#         if uploaded_files > 0:
#             print(f"\nðŸŽ‰ Upload completed successfully!")
#         else:
#             print(f"\nâš ï¸  No files were uploaded")
            
#     except Exception as e:
#         print(f"âŒ Error during MinIO upload: {e}")
#         logging.error(f"MinIO upload error: {e}")

def parse_arguments():
    """
    Parse command line arguments with enhanced options.
    """
    parser = argparse.ArgumentParser(
        description="BioRxiv MECA Package Downloader and MinIO Uploader (No Local Storage)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
            Examples:
            python biorxiv-upload-papers.py                    # Download 5 packages (default)
            python biorxiv-upload-papers.py -n 10              # Download 10 packages
            python biorxiv-upload-papers.py --num-packages 20  # Download 20 packages
            python biorxiv-upload-papers.py --test-minio       # Test MinIO connection only
            python biorxiv-upload-papers.py --force            # Force re-download existing packages
            python biorxiv-upload-papers.py --file-type all    # Upload all file types, not just PDFs
        """
    )
    
    parser.add_argument(
        "-n", "--num-packages",
        type=int,
        default=5,
        help="Number of MECA packages to download and process (default: 5)"
    )
    
    parser.add_argument(
        "--month",
        type=str,
        default="June_2025",
        help="Target month to search for packages (default: June_2025)"
    )
    
    parser.add_argument(
        "--file-type",
        type=str,
        choices=["pdf", "all"],
        default="pdf",
        help="Type of files to upload to MinIO (default: pdf)"
    )
    
    parser.add_argument(
        "--force",
        action="store_true",
        help="Force re-download and re-upload packages that already exist in MinIO"
    )
    
    parser.add_argument(
        "--test-minio",
        action="store_true",
        help="Test MinIO connection only and exit"
    )
    
    parser.add_argument(
        "-v", "--verbose",
        action="count",
        default=0,
        help="Increase verbosity (use -v for INFO, -vv for DEBUG)"
    )
    
    return parser.parse_args()

def test_minio_only() -> None:
    """
    Standalone function to test only MinIO connection.
    Useful for debugging MinIO issues without running the full workflow.
    """
    logging.info("MinIO Connection Test Only with default configuration")
    
    # Test with default configuration
    success = test_minio_connection()
    
    if not success:
        logging.info(f"Testing alternative configurations...")
        
        # Test different ports
        test_ports = ["9001", "9003", "9004"]
        for port in test_ports:
            endpoint = f"localhost:{port}"
            logging.info(f"Testing port {port}...")
            if test_minio_connection(endpoint=endpoint):
                logging.info(f"Found working MinIO on port {port}")
                logging.info(f"Update your MINIO_ENDPOINT to: {endpoint}")

        else:
            logging.info(f"No working MinIO found on tested ports")
            logging.info(f"Make sure MinIO is running and accessible")




# def main(num_packages: int = 5, target_month: str = "June_2025", 
#          output_dir: str = "./MECA_packages", file_type: str = "pdf") -> None:
#     """
#     Main function to download and process MECA packages.
    
#     Args:
#         num_packages: Number of MECA packages to download
#         target_month: Target month to search for packages
#         output_dir: Output directory for downloaded packages
#         file_type: Type of files to upload to MinIO
#     """
#     setup_logger(verbosity=1)
    
#     try:
#         print("ðŸ” BioRxiv MECA Package Workflow")
#         print("=" * 50)
#         print(f" Configuration:")
#         print(f"    Number of packages: {num_packages}")
#         print(f"   Target month: {target_month}")
#         print(f"   Output directory: {output_dir}")
#         print(f"   File type filter: {file_type}")
        
#         s3_client = create_s3_client()
#         print("âœ… S3 client created successfully")
        
#         print("\n1. ðŸ” Searching for MECA packages...")
#         s3_keys = find_MECA_packages_in_s3(s3_client, doi=None, target_month=target_month)
        
#         if s3_keys is not None:
#             print(f"âœ… Found {len(s3_keys)} MECA packages in {target_month}")
#             if s3_keys:
#                 # Limit the number of packages to process
#                 packages_to_process = min(num_packages, len(s3_keys)) if num_packages > 0 else len(s3_keys)
                
#                 print(f"\nðŸ“š Processing {packages_to_process} MECA packages found in {target_month}:")
                
#                 for i, key in enumerate(s3_keys[:packages_to_process], 1):
#                     print(f"   {i}. {key}")
#                 if len(s3_keys) > packages_to_process:
#                     print(f"   ... and {len(s3_keys) - packages_to_process} more MECA packages")

#                 # Checking if meca packages already exist in the minio bucket
#                 minio_client = create_minio_client()
#                 if minio_client.bucket_exists(MINIO_BUCKET):
#                     print(f"   ðŸ” Checking if the packages are already in MinIO bucket...")
#                     s3_uuids_to_upload_to_minio = []
#                     for s3_key in s3_keys[:packages_to_process]:
#                         package_name = s3_key.split("/")[-1].replace(".meca", "")
#                         # Check if any object in MinIO starts with this package name
#                         package_exists = any(obj.object_name.startswith(package_name + "/") \
#                             for obj in minio_client.list_objects(MINIO_BUCKET, prefix="", recursive=True))
#                         if package_exists:
#                             print(f"    âœ… Package {package_name} already exists in MinIO bucket")
#                         else:
#                             print(f"    âŒ Package {package_name} does not exist in MinIO bucket")
#                             s3_uuids_to_upload_to_minio.append(package_name) # Raw uuid

#                 # Step 2: Check if MECA packages are already downloaded and unzipped
#                 if num_packages > 0:  # Only download if num_packages > 0
#                     print(f"\n2. Verifying locally packages missing from MinIO bucket...")

#                     is_s3_key_downloaded = check_if_meca_packages_are_downloaded(
#                         output_dir=output_dir,
#                         s3_keys=s3_uuids_to_upload_to_minio
#                     )

#                     s3_uuids_to_download = [
#                         s3_key for i, s3_key in enumerate(s3_uuids_to_upload_to_minio)
#                         if not is_s3_key_downloaded[i]
#                     ]

#                     download_MECA_packages_from_s3(
#                         s3_client, 
#                         s3_keys=s3_uuids_to_download)
#                 else:
#                     print(f"\n2. Skipping download (num_packages = 0)")

#                 # Step 3: Extract MECA packages (this will extract all .meca files in the directory)
#                 print(f"\n3. Extracting MECA packages...")
#                 extracted_dirs = unzip_MECA_packages(output_dir=output_dir)
                
#                 # Step 4: Upload the specific packages that need to be uploaded to MinIO
#                 if s3_uuids_to_upload_to_minio:
#                     print(f"\n4. Uploading {len(s3_uuids_to_upload_to_minio)} packages to MinIO...")
#                     local_extracted_dirs = [os.path.join(output_dir, uuid) for uuid in s3_uuids_to_upload_to_minio]
#                     existing_dirs = [dir_path for dir_path in local_extracted_dirs if os.path.exists(dir_path)]

#                     if existing_dirs:
#                         print(f"    Found {len(existing_dirs)} existing directories to upload to MinIO:")
#                         for dir_path in existing_dirs:
#                             print(f"      - {dir_path}")

#                         print(f"\n5. Testing MinIO connection...")
#                         if test_minio_connection():
#                             upload_files_to_minio(existing_dirs, file_type=file_type)
#                             print(f"   Files uploaded to MinIO")
#                         else:
#                             print(f"   MinIO connection test failed")
#                     else:
#                         print(f"    No existing directories to upload to MinIO")
#                 else:
#                     print(f"\n4. No packages need to be uploaded to MinIO (all already exist)")

#                 # Step 5: Analyze and summarize (if any packages were extracted)
#                 if extracted_dirs:
#                     print(f"\n6. Analyzing MECA package structure...")
#                     print_MECA_summary(extracted_dirs)

#                 print(f"\n MECA workflow complete!")
#                 print(f" Original packages: {output_dir}/")
#                 print(f" Extracted content: {len(extracted_dirs)} directories")
#         else:
#             print(" No MECA packages found or error occurred")
            
#     except Exception as e:
#         print(f" Error in main function: {e}")
#         logging.error(f"Main function error: {e}")


def main(num_packages: int = 5, target_month: str = "June_2025", file_type: str = "pdf") -> None:
    """
    Main function to download and extract MECA packages from S3 to MinIO.
    """
    setup_logger(verbosity=1)
    try: 
        logging.info(f"Starting MECA workflow...")
        logging.info(f"Target month: {target_month}")
        logging.info(f"File type: {file_type}")
        logging.info(f"Number of packages: {num_packages}")
        
        s3_client = create_s3_client()
        logging.info(f"S3 client created successfully")
        minio_client = create_minio_client()
        logging.info(f"MinIO client created successfully")
        s3_keys = find_meca_packages_in_s3(s3_client, doi=None, target_month=target_month)
        
        if s3_keys is not None:
            logging.info(f"Found {len(s3_keys)} MECA packages in {target_month}")
            if s3_keys:
                packages_to_process = min(num_packages, len(s3_keys)) if num_packages > 0 else len(s3_keys)
                logging.info(f"Processing {packages_to_process} packages...")
                for i, key in enumerate(s3_keys[:packages_to_process], 1):
                    package_name = key.split("/")[-1].replace(".meca", "")
                    logging.info(f"Processing package {i}/{packages_to_process}: {package_name}")

                if len(s3_keys) > packages_to_process:
                    logging.info(f"Found {len(s3_keys) - packages_to_process} additional packages to process...")

                s3_uuids = [key.split("/")[-1].replace(".meca", "") for key in s3_keys[:packages_to_process]]

                logging.info(f"Downloading and extracting {len(s3_uuids)} MECA packages from S3 to MinIO...")
                download_and_extract_meca_to_minio(
                    s3_client,
                    minio_client,
                    s3_uuids,
                    target_month,
                    MINIO_BUCKET,
                    file_type
                )

                logging.info(f"MECA workflow complete!")
                logging.info(f"Total packages processed: {packages_to_process} and stored in {MINIO_BUCKET}")

                        else:
                logging.info(f"No MECA packages found in {target_month}")
            
    except Exception as e:
        logging.error(f"Error in main function: {e}")


if __name__ == "__main__":
    args = parse_arguments()
    
    if args.test_minio:
        test_minio_only()
    else:
        main(
            num_packages=args.num_packages,
            target_month=args.month,
            file_type=args.file_type
        )