[
  {
    "heading": "Unknown",
    "content": "Automated Detection of Antarctic Benthic Organisms in High-Resolution In Situ"
  },
  {
    "heading": "Unknown",
    "content": "Imagery to Aid Biodiversity Monitoring"
  },
  {
    "heading": "Unknown",
    "content": "Cameron Trotter\u2020\u2217"
  },
  {
    "heading": "Unknown",
    "content": "Huw Griffiths\u2020"
  },
  {
    "heading": "Unknown",
    "content": "Tasnuva Ming Khan\u2020\u2021"
  },
  {
    "heading": "Rowan Whittle\u2020",
    "content": "\u2020British Antarctic Survey\n\u2021University of Cambridge\ncater@bas.ac.uk*\nhjg@bas.ac.uk\ntfmk2@cam.ac.uk\nroit@bas.ac.uk"
  },
  {
    "heading": "Unknown",
    "content": "Abstract"
  },
  {
    "heading": "Monitoring benthic biodiversity in Antarctica is vital for",
    "content": "understanding ecological change in response to climate-\ndriven pressures. This work is typically performed using\nhigh-resolution imagery captured in situ, though manual\nannotation of such data remains laborious and specialised,\nimpeding large-scale analysis. We present a tailored object\ndetection framework for identifying and classifying Antarc-\ntic benthic organisms in high-resolution towed camera im-\nagery, alongside the first public computer vision dataset for\nbenthic biodiversity monitoring in the Weddell Sea. Our\napproach addresses key challenges associated with marine\necological imagery, including limited annotated data, vari-\nable object sizes, and complex seafloor structure. The pro-\nposed framework combines resolution-preserving patching,\nspatial data augmentation, fine-tuning, and postprocessing\nvia Slicing Aided Hyper Inference. We benchmark multiple\nobject detection architectures and demonstrate strong per-\nformance in detecting medium and large organisms across\n25 fine-grained morphotypes, significantly more than other\nworks in this area. Detection of small and rare taxa remains\na challenge, reflecting limitations in current detection archi-\ntectures. Our framework provides a scalable foundation for\nfuture machine-assisted in situ benthic biodiversity moni-\ntoring research.\n1. Introduction"
  },
  {
    "heading": "Benthic communities, comprised of organisms that live in,",
    "content": "on or around the seafloor, are highly biodiverse, play key\nroles within global nutrient cycling, and are a valuable\nfood source [25]. Global anthropogenic change, e.g., ocean\nwarming and acidification [44, 47], coupled with direct lo-\ncal and regional pressures such as harvesting and pollution,\nare negatively impacting the structure and function of ben-\nthic communities [11]."
  },
  {
    "heading": "The Antarctic benthos is uniquely adapted to its isolated",
    "content": "and frozen environment [2]. These cold-adapted species\n*Corresponding author\nface additional pressures through changes to the cryosphere\nthat dominates their ocean, e.g., glacial melt and ice shelf\ncollapse."
  },
  {
    "heading": "These changes are most notable in the shal-",
    "content": "low benthic communities of the West Antarctic Peninsula,\nwhere changes to biodiversity, trophic structure, biomass,\nand distribution have been observed [19]."
  },
  {
    "heading": "Historically, the exploration and monitoring of benthic",
    "content": "environments has relied on invasive, non-quantitative meth-\nods such as dredging, or more quantitative yet slow-to-\ndeploy instruments like corers and grabs [45]. In recent\nyears, the adoption of imaging technologies, delivered via"
  },
  {
    "heading": "SCUBA, submersibles, towed or drop camera systems, re-",
    "content": "motely operated vehicles, and autonomous platforms, has\nsignificantly increased both the rate and scale of data acqui-\nsition. Photographic and video data enable rapid, in situ,\nand quantitative surveys of extensive seafloor areas."
  },
  {
    "heading": "Imaging techniques represent a non-destructive and re-",
    "content": "peatable survey method to monitor ecosystem change. To\ndate, the usefulness of collected data has been restricted\nby the need for expert assessment of every image, which is\ntime consuming [3, 52] and prone to fatigue and annotation\nbias [12, 15, 41]. This bottleneck is particularly evident for"
  },
  {
    "heading": "Antarctica, with highly diverse and endemic benthic species",
    "content": "[2] and relatively few taxonomic experts capable of provid-\ning confident image-based identifications."
  },
  {
    "heading": "Additionally, the high logistical and financial costs asso-",
    "content": "ciated with deep-sea data collection often results in com-\nparatively small amounts of collected data."
  },
  {
    "heading": "Antarctica\u2019s",
    "content": "geographic isolation and extreme environmental conditions\nmake fieldwork highly resource-intensive, limiting collec-\ntion to infrequent, short-duration missions typically led by\nnational research programmes."
  },
  {
    "heading": "Recent advances in deep learning and computer vision",
    "content": "have enabled the development of machine-assisted in situ\nbiodiversity monitoring tools, designed to automate parts of\nthe data curation process and mitigate the annotation bottle-\nneck faced by marine ecologists [49]. By leveraging man-\nually curated data from previous surveys, researchers can\nnow train models to detect benthic organisms commonly\nencountered in their study regions, supporting applications\n1\narXiv:2507.21665v1  [cs.CV]  29 Jul 2025\nsuch as first-pass annotation workflows [26, 39, 40]."
  },
  {
    "heading": "Given the high data collection and annotation costs asso-",
    "content": "ciated with the Antarctic benthos however, there is a notable\nlack of publicly available datasets suitable for training au-\ntomated biodiversity monitoring tools for these ecosystems."
  },
  {
    "heading": "This limitation is further compounded by the region\u2019s high",
    "content": "levels of endemism, which reduces the relevance and trans-\nferability of models trained on data from other regions."
  },
  {
    "heading": "In scenarios requiring the detection of small or densely",
    "content": "aggregated organisms, high-resolution imagery is often\nutilised to enable finer-scale ecological observations. How-\never, such data introduces additional complexity to the\nmodel development pipeline. High-resolution images place\nsubstantial computational demands on both training and in-\nference processes, and the accurate detection of small or\nclosely packed objects remains a persistent challenge for\ndeep learning-based object detection systems [30]."
  },
  {
    "heading": "In this study, we present an object detection framework",
    "content": "designed to identify benthic organisms in high-resolution\nseafloor imagery from the Weddell Sea, Antarctica. Our ap-\nproach accommodates large-scale inputs without downscal-\ning through a patch-based processing methodology. The\nmodel is trained on a manually annotated dataset of only\n100 images, which we release publicly as the first computer\nvision\u2013ready benthic dataset from the Weddell Sea. The re-\nsulting model is capable of detecting a wide variety of ben-\nthic organisms, more than previous works in this area and\nto a higher level of granularity.\n2. Related Work"
  },
  {
    "heading": "Early studies into machine-assisted in situ benthic biodi-",
    "content": "versity monitoring used local features and hand-engineered\npipelines to identify specific organisms of interest [13, 48]."
  },
  {
    "heading": "Such methods require extensive adaptation for new organ-",
    "content": "isms or environments, limiting their use in broad biodiver-\nsity monitoring surveys enabled by advances in underwater\nimaging and affordable data storage."
  },
  {
    "heading": "Data-driven deep learning models capable of gener-",
    "content": "alised, automated feature extraction, e.g., Convolutional"
  },
  {
    "heading": "Neural Networks (CNNs), have accelerated the pace of",
    "content": "machine-assisted in situ benthic biodiversity monitoring re-\nsearch [49]. Such models are often task-specific: image\nclassifiers for taxonomic ID [41, 56], object detectors for\nabundance estimation [32, 55], semantic segmenters for\nhabitat mapping and behaviour analysis [20, 35, 38], and\ninstance segmenters for biomass estimation [31]."
  },
  {
    "heading": "Few studies focus specifically on the Antarctic benthos,",
    "content": "likely due to its remoteness and high fieldwork costs. [32]\nmake use of a YOLOv5 [23] model pre-trained on the"
  },
  {
    "heading": "COCO dataset [28] to provide abundance estimates for two",
    "content": "coarse-grained morphotypes, organism groupings classified\nbased on shared morphological characteristics, in imagery\nfrom a stationary camera deployed in the Ross Sea. As im-\nagery is downscaled, detection of small organisms may not\nbe feasible using the presented methodology.\n[31] apply a patching strategy to towed-camera imagery\nfrom the Weddell Sea to evaluate the effectiveness of syn-\nthetic data augmentation in training a CenterMask [27]\nmodel for instance segmentation of three coarse-grained\nmorphotypes. However, their approach does not address\npotential detection failures at patch boundaries or support\nfull-image ecological analysis. To address these limitations,\nwe extend patching with overlap and postprocessing tech-\nniques that improve detection accuracy at patch edges. Ad-\nditionally, we reproject detections back onto the original\nlarge-scale imagery, preserving spatial context for ecolog-\nical analysis. Detailed methodology is provided in Sec. 4."
  },
  {
    "heading": "Further, while the aforementioned works demonstrate",
    "content": "the feasibility of identifying Antarctic organisms using deep\nlearning, they are limited to a small number of coarse-\ngrained morphotypes. These studies also do not consider\ntaxa with low abundance, potentially overlooking ecolog-\nically important but infrequently observed organisms. In\ncontrast, we explore the development of object detection\nmodels that capture fine-grained taxonomic and morpholog-\nical range, including rare organisms, examining the effect of\nabundance on model detection capability.\n3. The Weddell Sea Benthic Dataset"
  },
  {
    "heading": "Data used in this study were collected during expedi-",
    "content": "tion PS118 (cruises 69-1 and 6-9; see Fig. 1) of the"
  },
  {
    "heading": "Unknown",
    "content": "RV Polarstern [43]. High-resolution benthic imagery (22"
  },
  {
    "heading": "Megapixel, average filesize = 6.94 Megabyte (MB)) was",
    "content": "captured in a top-down view using the Ocean Floor Ob-\nservation and Bathymetry System (OFOBS) [42], a towed\ncamera system operating just above the seafloor. The im-\nagery captures a diverse range of environmental conditions,\nincluding variable turbidity, illumination levels, and sub-\nstrate types (hard and soft). Some images exhibit mild dis-\ntortion due to the motion of the OFOBS during capture."
  },
  {
    "heading": "A subset of the collected imagery, selected for their eco-",
    "content": "logical rather than model training merit, was manually an-\nnotated to facilitate benthic community composition anal-\nysis [24]; this forms the ground truth dataset used in this\nstudy, which we name The Weddell Sea Benthic Dataset\n(WSBD). This dataset comprises 100 annotated images cap-\ntured at a range of depths (421\u20132202 m) and seafloor in-\nclinations (0\u201380\u25e6)."
  },
  {
    "heading": "Where images were not comprehen-",
    "content": "sively annotated, e.g., due to distortion, the unlabelled re-\ngions were cropped, resulting in images of varying sizes\n(average = 3364\u00d74545 px, 1.15 MB). Images are distinctly\nseparated, with no overlap present between them. The orig-\ninal annotations were consolidated into 25 morphologically\ndistinct classes, ranging from broad taxonomic groups to\nspecies level (see App. A)."
  },
  {
    "heading": "The dataset presents substantial visual complexity, with",
    "content": "2\nFigure 1. Map of PS118 image acquisition sites included in the"
  },
  {
    "heading": "Weddell Sea Benthic Dataset. Yellow dots show data collected",
    "content": "during cruise 69-1, while red points correspond to cruise 6-9.\nimagery characterised by high levels of background clutter,\nvariable illumination, shadowing, and overlapping objects."
  },
  {
    "heading": "These factors, plus the presence of fine-grained and mor-",
    "content": "phologically similar taxa, make the WSBD a challenging\nand ecologically realistic benchmark for evaluating benthic\nobject detection frameworks."
  },
  {
    "heading": "Imagery is biased towards soft-substrate environments at",
    "content": "shallower depths (420\u2013500 m), comprising 61.00% of im-\nages but only 4.24% of annotations. The dataset also shows\na bias towards low-inclination areas, with 52.00% of im-\nages taken on slopes <=10\u25e6. Certain taxa are restricted to\nspecific substrate."
  },
  {
    "heading": "Owing to the remoteness of the study site and limited an-",
    "content": "thropogenic disturbance under the Antarctic Treaty System,\nthe WSDB contains high organism densities. The dataset\ncontains 31,280 total bounding box annotations, with indi-\nvidual images containing between 5 and 1693 annotations\n(average = 312.8)."
  },
  {
    "heading": "This includes numerous overlapping",
    "content": "bounding boxes, a known challenge for object detection\nsystems [8, 21]."
  },
  {
    "heading": "Further, the dataset exhibits significant",
    "content": "class imbalance, following a long-tailed distribution con-\nsistent with ecological patterns. The number of annotated\ninstances per class ranges from 13,295 for stylasterids to 10\nfor the ascidian Cnemidocarpa verrucosa. Addressing rare-\nclass detection remains a critical issue in machine-assisted\nbiodiversity monitoring [34, 50, 51]."
  },
  {
    "heading": "Small object detection remains a challenging and largely",
    "content": "unsolved problem in computer vision [30]."
  },
  {
    "heading": "The WSBD",
    "content": "dataset exemplifies these difficulties, exhibiting substantial\ninter-class size variation. Average bounding box areas range\nfrom 520 px2 for cup corals to 68,092 px2 for the ascid-\nian Distaplia. Further, intra-class size variability is intro-\nduced by fluctuations in the OFOBS\u2019 altitude during im-\nage capture, resulting in inconsistent scales which further\ncomplicate detection tasks. We release the WSBD under an"
  },
  {
    "heading": "Unknown",
    "content": "OGL-UK-3.0 license: https://doi.org/10.5285/"
  },
  {
    "heading": "1BA97E4B-EFB7-460B-9F2D-90437E33CE09.",
    "content": "4. Method"
  },
  {
    "heading": "Our proposed methodology (see Fig. 2) enables us to exploit",
    "content": "the high spatial fidelity of the WSBD whilst maintaining\ndetection efficacy.\n4.1. Dataset Preparation"
  },
  {
    "heading": "To account for the imbalance in annotation volume be-",
    "content": "tween the two substrate types, the train, validation, and\ntest sets were generated based on the proportion of total\nannotations rather than the number of images. These sets\nwere then refined to ensure they remained representative\nof the geographic and environmental diversity present in\nthe dataset, including variation in depth and seafloor in-\nclination."
  },
  {
    "heading": "This adjustment was made to enhance model",
    "content": "generalisability and help prevent overfitting to specific en-\nvironmental conditions, which is critical in biodiversity\nmonitoring applications [37, 53]."
  },
  {
    "heading": "The final annotation-",
    "content": "level train\u2013validation\u2013test split was 68.71%, 18.93%, and\n12.36%, respectively.\n4.2. Image Patching"
  },
  {
    "heading": "The WSBD provides high-resolution benthic imagery",
    "content": "which, while crucial for classifying small and morpholog-\nically similar organisms, introduces substantial computa-\ntional overhead."
  },
  {
    "heading": "Conventional object detection architec-",
    "content": "tures are typically optimised for lower-resolution inputs\n[29, 46] and thus struggle to process full-resolution WSBD\nimages without exceeding memory constraints. Downscal-\ning such imagery to meet these limitations results in the loss\nof visual features which is particularly detrimental to the de-\ntection of small organisms (see Sec. 5.7.1)."
  },
  {
    "heading": "To retain visual features we implement a patch-based de-",
    "content": "tection strategy, subdividing the original large-scale image\ninto sub-images of uniform size via a sliding window with\nfixed horizontal and vertical strides. Image patching is a\nwell-established technique within machine-assisted in situ\nbenthic biodiversity monitoring research, though it has pri-\nmarily been employed for coarse-grained tasks [18, 22, 31]."
  },
  {
    "heading": "Patching also standardises input dimensions, mitigating im-",
    "content": "age size variability from dataset generation and enabling\nmore efficient training and inference."
  },
  {
    "heading": "To extend patch-based processing to object-level tasks,",
    "content": "we adopt the Slicing Aided Hyper Inference (SAHI)\nmethodology [1]."
  },
  {
    "heading": "Designed to improve the performance",
    "content": "of object detection models on high-resolution data, SAHI\nworks by dividing large images into overlapping patches,\napplying patch-based detection, and subsequently merging\nresults via postprocessing. This approach retains resolution\n3\nFigure 2. A high level overview of the proposed Antarctic benthic organism detection and classification framework. For large-scale\nvisualisation of the output, see App. C.\nand aids small object detection, a common problem in eco-\nlogically complex imagery. SAHI has demonstrated strong\nperformance in object-level tasks involving high-resolution\ndata across other domains [9, 17, 36]. We evaluate the op-\ntimal SAHI patching configuration, including patch size,\noverlap stride, and minimum bounding box visibility (the\nproportion of a ground truth bounding box required within\na patch to be considered a valid object instance).\n4.3. Object Detection"
  },
  {
    "heading": "An object detection model is trained using patches as in-",
    "content": "put, allowing for the retention of fine-grained features nec-\nessary for accurately detecting small and densely clustered\norganisms. The model generates bounding boxes around\nproposed regions of interest per patch, accompanied by a\npredicted class label, corresponding to one of 25 defined\norganism morphotypes, and a confidence score. We eval-\nuate a range of object detection architectures, including\nsingle-stage and two-stage detectors as well as CNN and\ntransformer-based models, alongside various data augmen-\ntation strategies and model fine-tuning.\n4.4. Postprocessing"
  },
  {
    "heading": "Following inference, patch-level detections are mapped",
    "content": "back to their original coordinates on the large-scale in-\nput image. To resolve redundant detections resulting from\noverlapping patches, we apply a Non-Maximum Merging\n(NMM) procedure, consolidating multiple detections of the\nsame object into a single bounding box.\n5. Experiments"
  },
  {
    "heading": "We evaluate various methodological configurations to deter-",
    "content": "mine the optimal framework setup for the WSBD. Specif-\nically, we examine the impact of different SAHI parame-\nters, augmentation strategies, architectures, and the use of\npre-trained weights for model fine-tuning. Throughout, we\nexamine the effect of organism abundance on model per-\nformance. The final optimal setup uncovered represents a\nbaseline benchmark for the WSBD.\n5.1. Experimental Setup"
  },
  {
    "heading": "Experiments were run on a single High Performance Com-",
    "content": "puting node using one NVIDIA A2 GPU. Object detec-\ntion models were implemented in Python using MMDetec-\ntion [10], with data augmentation via Albumentations [6]."
  },
  {
    "heading": "Training was performed for up to 200 epochs, with early",
    "content": "stopping if no improvement was seen after 10 epochs. Our\ncode is available at: https://github.com/Trotts/\nantarctic-benthic-organism-detection/."
  },
  {
    "heading": "Unknown",
    "content": "4"
  },
  {
    "heading": "Table 1. Test set Mean Average Precision (mAP) across key In-",
    "content": "tersection over Union (IoU) thresholds and object sizes for each\ndataset configuration. Bold indicates top performance per metric."
  },
  {
    "heading": "Mean Average Precision (mAP)",
    "content": "@0.5:0.95\n@0.5\nAll"
  },
  {
    "heading": "Unknown",
    "content": "Small"
  },
  {
    "heading": "Unknown",
    "content": "Medium"
  },
  {
    "heading": "Unknown",
    "content": "Large"
  },
  {
    "heading": "Unknown",
    "content": "Parameters"
  },
  {
    "heading": "Num. Classes",
    "content": "10\n25\n10\n25\n10\n25\n10\n25\n10\n25"
  },
  {
    "heading": "SAHI Patching",
    "content": "0.22\n0.18\n0.45\n0.34\n0.20\n0.24\n0.48\n0.35\n0.54\n0.42\n+ SAHI Postprocessing\n0.21\n0.19\n0.45\n0.37\n0.20\n0.23\n0.50\n0.33\n0.52\n0.44\n+ Spatial Augmentation\n0.21\n0.18\n0.45\n0.33\n0.22\n0.19\n0.50\n0.32\n0.49\n0.44"
  },
  {
    "heading": "Unknown",
    "content": "Unless stated otherwise, all experiments used a Faster R-"
  },
  {
    "heading": "CNN architecture [46]. Evaluation used Mean Average Pre-",
    "content": "cision (mAP) across multiple Intersection over Union (IoU)\nthresholds and object sizes (Small, Medium, and Large),\nfollowing the COCO format [28]. Models were evaluated\non both the full 25-class set and a 10-class subset compris-\ning the most abundant taxa. Metrics were computed after\npatch-level detections were reprojected back to their origi-\nnal large-scale image coordinates and postprocessed using"
  },
  {
    "heading": "NMM.",
    "content": "5.2. SAHI Patching Parameters"
  },
  {
    "heading": "To implement SAHI effectively, several parameters must be",
    "content": "defined to control how images and annotations are divided\ninto patches. To determine the optimal configuration for\nthe WSBD, we conducted a series of experiments training a\nmodel for each combination of three key parameters: patch\nsize (250\u00d7250, 500\u00d7500, 750\u00d7750, and 1000\u00d71000 px),\noverlap stride (0.0, 0.25, and 0.50), and minimum bound-\ning box visibility (0.10, 0.25, and 0.50). The NMM IoU\nthreshold was fixed at 0.5 across all configurations."
  },
  {
    "heading": "Evaluation revealed that a patch size of 500\u00d7500 px with",
    "content": "a 0.50 stride and a minimum bounding box visibility of\n0.25 achieved the highest overall performance (see Tab. 1)."
  },
  {
    "heading": "This configuration maintained robust performance across",
    "content": "all object sizes relative to other tested permutations. While\nlarger patch sizes yielded comparable results for detecting\nlarger organisms, they were less effective for smaller taxa,\nsuggesting a trade-off between patch size and sensitivity\nto fine-grained features. Additionally, larger patches and\nhigher stride increased computational demands due to in-\ncreased dataset size and model input parameters. A min-\nimum bounding box visibility of 0.25 yielded the highest\nmAP. Lower thresholds introduced training noise by retain-\ning extremely cropped objects, while higher thresholds ex-\ncluded valid examples, disproportionately affecting small\norganisms that frequently occur near patch boundaries."
  },
  {
    "heading": "Unknown",
    "content": "This setup generated 25,184 patches from the 100"
  },
  {
    "heading": "WSBD images, with 17,819 used for training, 4310 for val-",
    "content": "idation, and 3055 for testing.\n5.3. SAHI Postprocessing Parameters"
  },
  {
    "heading": "During SAHI postprocessing, predicted patch-level bound-",
    "content": "ing boxes are reprojected to their original coordinates\nwithin the large-scale input image. Bounding boxes of the\nsame class that overlap by at least a specified IoU threshold\nare merged using NMM to reduce duplicate detections. To\nidentify the optimal NMM IoU threshold, we applied SAHI\npostprocessing to the optimal patching model across a range\nof IoU values from 0.05 to 0.50 in 0.05 increments."
  },
  {
    "heading": "An IoU threshold of 0.20 was found to be sufficient, in-",
    "content": "dicating a relatively low overlap threshold is optimal for\nmerging duplicate detections after reprojection, particularly\ngiven the prevalence of small, densely clustered objects\nfound within hard substrate imagery. Higher thresholds of-\nten failed to merge duplicate detections, resulting in inflated\nfalse positives, while lower thresholds erroneously merge\ndistinct but nearby objects.\n5.4. Data Augmentation Strategy"
  },
  {
    "heading": "Given the limited volume of training data in the WSBD, we",
    "content": "evaluated the effect of data augmentation, generating new\nsamples by perturbing existing data, on model performance."
  },
  {
    "heading": "Data scarcity is a persistent challenge for the development",
    "content": "of machine-assisted in situ benthic biodiversity monitoring\ntools, though prior studies have shown that augmentation\ncan enhance model performance [14, 16, 31, 38]."
  },
  {
    "heading": "We evaluated three augmentation strategies: pixel-level,",
    "content": "spatial-level, and a combined approach using both (see App."
  },
  {
    "heading": "B). Each was assessed against a non-augmented baseline",
    "content": "defined in Sec. 5.3. Spatial transformations yielded the best\noverall results. This is likely due to the WSBD\u2019s existing\nartefacts, such as motion blur and shadow, reducing the ef-\nfectiveness of additional pixel-level perturbations. In con-\ntrast, spatial transformations introduced beneficial variabil-\nity without further obscuring key visual features. This ap-\nproach achieved the highest mAP@0.5 for small objects and\ntied for the best performance on medium objects in the 10-\nclass setup. These improvements are particularly valuable\nin the WSBD, where small, densely packed organisms are\nfrequent and difficult to detect. Accordingly, spatial aug-\nmentation was adopted for all subsequent experiments.\n5.5. Architecture Search"
  },
  {
    "heading": "To explore a broader range of detection capabilities be-",
    "content": "yond the Faster R-CNN baseline, we evaluated a mix\nof single-stage and two-stage detectors, as well as CNN\nand transformer-based architectures, against WSBD perfor-\nmance."
  },
  {
    "heading": "The DINO [54] architecture achieved the highest",
    "content": "mAP@0.5:0.95 for both the 10-class and 25-class config-\nurations, sharing the top score with Cascade R-CNN [7] in\nthe latter (See Tab. 2 Top). When evaluating at mAP@0.5,"
  },
  {
    "heading": "DINO also outperformed other models in the 10-class setup,",
    "content": "while Deformable-DETR [57] achieved the highest per-\nformance in the 25-class setting."
  },
  {
    "heading": "While no architecture",
    "content": "surpassed the Faster R-CNN baseline in detecting small\nand medium objects under the 10-class configuration, Co-"
  },
  {
    "heading": "Unknown",
    "content": "5"
  },
  {
    "heading": "Table 2. Test set Mean Average Precision (mAP) across key In-",
    "content": "etrsection over Union (IoU) thresholds and object sizes for vari-\nous model architectures (ordered by release), trained with optimal\ndataset settings, with and without COCO fine-tuning. Bold indi-\ncates top performance per metric; italics denote average perfor-\nmance."
  },
  {
    "heading": "Mean Average Precision (mAP)",
    "content": "@0.5:0.95\n@0.5\nAll"
  },
  {
    "heading": "Unknown",
    "content": "Small"
  },
  {
    "heading": "Unknown",
    "content": "Medium"
  },
  {
    "heading": "Unknown",
    "content": "Large"
  },
  {
    "heading": "Unknown",
    "content": "Fine-tuned"
  },
  {
    "heading": "Unknown",
    "content": "Architecture"
  },
  {
    "heading": "Num. Classes",
    "content": "10\n25\n10\n25\n10\n25\n10\n25\n10\n25\n\u2717"
  },
  {
    "heading": "Faster R-CNN [46]",
    "content": "0.21\n0.18\n0.45\n0.33\n0.22\n0.19\n0.50\n0.32\n0.49\n0.44"
  },
  {
    "heading": "Cascade R-CNN [7]",
    "content": "0.20\n0.19\n0.42\n0.35\n0.13\n0.10\n0.45\n0.30\n0.59\n0.55"
  },
  {
    "heading": "RetinaNet [29]",
    "content": "0.18\n0.12\n0.38\n0.24\n0.09\n0.11\n0.42\n0.24\n0.61\n0.35"
  },
  {
    "heading": "Deformable-DETR [57]",
    "content": "0.19\n0.18\n0.45\n0.36\n0.21\n0.15\n0.47\n0.31\n0.52\n0.46"
  },
  {
    "heading": "DINO [54]",
    "content": "0.22\n0.19\n0.46\n0.35\n0.16\n0.14\n0.49\n0.32\n0.53\n0.44"
  },
  {
    "heading": "CoDETR [33]",
    "content": "0.19\n0.16\n0.45\n0.34\n0.18\n0.22\n0.48\n0.31\n0.52\n0.43"
  },
  {
    "heading": "Average",
    "content": "0.20\n0.17\n0.44\n0.33\n0.17\n0.15\n0.47\n0.30\n0.54\n0.45\n\u2713"
  },
  {
    "heading": "Faster R-CNN [46]",
    "content": "0.21\n0.18\n0.48\n0.36\n0.22\n0.21\n0.49\n0.31\n0.56\n0.44"
  },
  {
    "heading": "Cascade R-CNN [7]",
    "content": "0.22\n0.17\n0.46\n0.34\n0.18\n0.18\n0.48\n0.29\n0.53\n0.45"
  },
  {
    "heading": "RetinaNet [29]",
    "content": "0.20\n0.19\n0.41\n0.34\n0.11\n0.12\n0.46\n0.31\n0.50\n0.45"
  },
  {
    "heading": "Deformable-DETR [57]",
    "content": "0.22\n0.19\n0.49\n0.39\n0.21\n0.27\n0.51\n0.33\n0.55\n0.47"
  },
  {
    "heading": "DINO [54]",
    "content": "0.24\n0.19\n0.47\n0.33\n0.19\n0.13\n0.49\n0.29\n0.57\n0.49"
  },
  {
    "heading": "CoDETR [33]",
    "content": "0.22\n0.19\n0.46\n0.33\n0.17\n0.12\n0.49\n0.30\n0.60\n0.44"
  },
  {
    "heading": "Average",
    "content": "0.22\n0.19\n0.46\n0.35\n0.18\n0.17\n0.49\n0.31\n0.55\n0.46"
  },
  {
    "heading": "DETR [33] exhibited the best performance on small objects",
    "content": "in the 25-class evaluation."
  },
  {
    "heading": "For medium-sized objects under the same configuration,",
    "content": "the top-performing model was shared between Faster R-"
  },
  {
    "heading": "CNN and DINO. In the case of large object detection, Reti-",
    "content": "naNet [29] delivered the best results in the 10-class evalua-\ntion. However, its performance dropped substantially in the\n25-class setting, where Cascade R-CNN significantly out-\nperformed all other architectures.\n5.6. Effect of Fine-tuning"
  },
  {
    "heading": "Alongside data augmentation (see Sec. 5.4), fine-tuning is",
    "content": "an effective strategy for improving model generalisability\nin object detection tasks where training data is limited [4]."
  },
  {
    "heading": "Rather than initialising model weights randomly, requir-",
    "content": "ing the network to learn fundamental visual representations\nfrom scratch, fine-tuned models leverage weights obtained\nfrom previous training on large-scale datasets. This facil-\nitates transfer learning, wherein knowledge acquired in a\nsource domain is applied to enhance performance in a tar-\nget domain [4]."
  },
  {
    "heading": "Given the challenge of limited labelled data in the de-",
    "content": "velopment of machine-assisted in situ benthic biodiversity\nmonitoring tools, fine-tuning has become a standard ap-\nproach within the field [49]. Here, we evaluate the impact\nof fine-tuning on WSBD performance by comparing mod-\nels initialised with random weights to those initialised on\nweights derived after training on the COCO dataset [28]."
  },
  {
    "heading": "Overall, COCO fine-tuning resulted in a slight improve-",
    "content": "ment in average model performance (see Tab. 2 Bottom)."
  },
  {
    "heading": "With the exception of large object detection in the 25-class",
    "content": "evaluation, the highest metrics across evaluation categories\nwere achieved by fine-tuned models. Notably, in contrast\nto the non-fine-tuned models where optimal performance\nvaried depending on the specific evaluation scenario (e.g.,\nobject size or number of classes), the use of fine-tuning\nconsistently elevated Deformable-DETR to either the top-\nFigure 3. Example WSBD test set image output. Predicted organ-\nism bounding boxes, class labels, and confidence scores shown af-\nter reprojection and postprocessing. Confidence threshold = 0.60."
  },
  {
    "heading": "For large-scale visualisations, see App. C.",
    "content": "performing model or among the top three performers across\nnearly all categories. This suggests that while the absolute\ngains from fine-tuning may be limited, the approach con-\ntributes to increased robustness and consistency in model\nperformance. Crucially, it enables the identification of a\nsingle architecture, Deformable-DETR, as the most effec-\ntive model overall, providing a clear candidate for subse-\nquent deployment. An example output from this model can\nbe seen in Fig. 3.\n5.7. Ablation Study"
  },
  {
    "heading": "To further evaluate model performance and verify that each",
    "content": "component of the proposed framework contributes posi-\ntively, we conducted a series of additional ablation studies.\n5.7.1. Image-level Downscaling"
  },
  {
    "heading": "Although the use of patching is intended to aid the detec-",
    "content": "tion of small objects, the results presented in Tab. 2 indi-\ncate that all evaluated model architectures continue to ex-\nhibit limited performance when detecting those present in\nthe WSBD. To verify patching contributed positively, an\nadditional Deformable-DETR model was trained using non-\npatched imagery, spatially augmented and fine-tuned on the"
  },
  {
    "heading": "COCO dataset, for comparison. To ensure consistent image",
    "content": "size we downscale the data to 1635\u00d71635 px, the smallest"
  },
  {
    "heading": "Unknown",
    "content": "WSBD image."
  },
  {
    "heading": "Substantial declines in detection performance were ob-",
    "content": "served across all object categories (see Tab. 3), indicating a\ncritical loss of discriminative features resulting from image"
  },
  {
    "heading": "Unknown",
    "content": "6"
  },
  {
    "heading": "Table 3. Test set Mean Average Precision (mAP) across key In-",
    "content": "tersection over Union (IoU) thresholds and object sizes for each\nablation study. The baseline model corresponds to the optimal"
  },
  {
    "heading": "Deformable-DETR configuration. Bold indicates top performance",
    "content": "per metric."
  },
  {
    "heading": "Mean Average Precision (mAP)",
    "content": "@0.5:0.95\n@0.5\nAll"
  },
  {
    "heading": "Unknown",
    "content": "Small"
  },
  {
    "heading": "Unknown",
    "content": "Medium"
  },
  {
    "heading": "Unknown",
    "content": "Large"
  },
  {
    "heading": "Unknown",
    "content": "Experiment"
  },
  {
    "heading": "Num. Classes",
    "content": "10\n25\n10\n25\n10\n25\n10\n25\n10\n25"
  },
  {
    "heading": "Baseline",
    "content": "0.22\n0.19\n0.49\n0.39\n0.21\n0.27\n0.51\n0.33\n0.56\n0.47"
  },
  {
    "heading": "Image-level Downscaling",
    "content": "0.06\n0.07\n0.13\n0.12\n0.03\n0.02\n0.12\n0.06\n0.19\n0.19"
  },
  {
    "heading": "Non-Maximum Suppression",
    "content": "0.21\n0.19\n0.48\n0.34\n0.20\n0.20\n0.51\n0.33\n0.53\n0.39"
  },
  {
    "heading": "No Postprocessing",
    "content": "0.13\n0.13\n0.26\n0.22\n0.13\n0.16\n0.27\n0.19\n0.32\n0.29\ndownscaling. These findings reinforce the necessity of em-\nploying a patching strategy to preserve resolution and main-\ntain object-level detail.\n5.7.2. SAHI Postprocessing Algorithm"
  },
  {
    "heading": "Following patch-level detection, bounding boxes are repro-",
    "content": "jected to their corresponding large-scale image coordinates."
  },
  {
    "heading": "To address duplicate detections resulting from patch over-",
    "content": "lap, boxes with identical class labels, overlapping with an"
  },
  {
    "heading": "Unknown",
    "content": "IoU >= 0.20, are postprocessed using NMM. However,"
  },
  {
    "heading": "SAHI also allows for the use of Non-Maximum Suppres-",
    "content": "sion (NMS), where only the overlapping box with the high-\nest confidence level is retained."
  },
  {
    "heading": "Additionally, detections",
    "content": "may be reprojected without any postprocessing applied. To\nverify merging was the correct approach, we evaluated the\nuse of NMS and no postprocessing after reprojection."
  },
  {
    "heading": "Substituting NMM with NMS resulted in either a slight",
    "content": "decrease or no measurable improvement in detection per-\nformance, particularly under the more challenging 25-class\nevaluation (see Tab. 3). The decline was most evident for\nboth small and large object categories, where the baseline\nmodel employing merging demonstrated superior perfor-\nmance. Further, the use of no postprocessing significantly\nreduces performance across all evaluated metrics. These\nfindings suggest that merging methods more effectively pre-\nserve localisation quality in cases where object instances are\nfragmented across overlapping patches.\n6. Discussion"
  },
  {
    "heading": "Based on the results presented in Sec. 5, we propose an op-",
    "content": "timal framework configuration for the fine-grained detec-\ntion of benthic organisms in high-resolution towed camera\nimagery from the Weddell Sea, Antarctica."
  },
  {
    "heading": "The recom-",
    "content": "mended approach involves subdividing large-scale images\ninto 500\u00d7500 px patches with a 0.50 horizontal and verti-\ncal overlap stride, alongside a minimum bounding box vis-\nibility threshold of 0.25. Dataset splitting is stratified by\nsubstrate type, depth, and seafloor inclination to ensure ge-\nographic and environmental diversity. The resulting patches\nare spatially augmented and used to train a Deformable-"
  },
  {
    "heading": "DETR object detection model, with initial weights de-",
    "content": "rived from the COCO dataset. Following inference, detec-\ntions are reprojected to their original locations on the full-\nresolution image. Overlapping same-class bounding boxes\nare then postprocessed using NMM with an IoU threshold\nof 0.20.\n6.1. Small Object Detection"
  },
  {
    "heading": "The resulting model is trained to detect 25 distinct mor-",
    "content": "photypes found in the Weddell Sea. However, notable per-\nformance limitations for small organisms are present, even\nwhen employing the SAHI methodology. While these limi-\ntations may partly stem from the restricted size of the train-\ning dataset, a common constraint in machine-assisted in situ\nbenthic biodiversity monitoring, they are likely exacerbated\nby the logistical and environmental challenges of data col-\nlection in Antarctica."
  },
  {
    "heading": "The observed underperformance for small object detec-",
    "content": "tion, despite the use of high-resolution imagery, advanced\npatching strategies, data augmentation, and fine-tuning,\nsuggests current object detection architectures are limited\nin their ability to extract meaningful features from small\ninstances in visually complex benthic environments. This\nrestricts accurate learning and detection of ecologically im-\nportant taxa, and highlights the need for new architectural\napproaches tailored to small object representation.\n6.2. Effect of Abundance and Morphology"
  },
  {
    "heading": "Additionally, organism abundance was found to have a no-",
    "content": "table influence on model performance. This is evident when\ncomparing the results of the 10-class and 25-class evalua-\ntions. The average number of annotations per class in the\n10-class evaluation is 2068.5. In contrast, overall average\nabundance for the 25-class configuration is 859.4, dropping\nto just 53.4 for organisms present in the 25-class set only."
  },
  {
    "heading": "Examining the optimal model\u2019s class confusion reveals that",
    "content": "although the overall number of missed detections is high,\nespecially for rare organisms, the rate of misclassification\namong detected abundant instances is low (see App. D)."
  },
  {
    "heading": "This suggests that when the model places a bounding box,",
    "content": "it is likely to contain a valid organism and to assign it the\ncorrect label."
  },
  {
    "heading": "Where misclassification does occur it is typically be-",
    "content": "tween morphologically similar organisms, e.g., demo-\nsponges and glass sponges, which share structural features\nand can be difficult to distinguish visually, even for trained\nexperts. In contrast, misclassifications between taxonomi-\ncally related but visually distinct organisms, e.g., Ophios-\nabine and other ophiuroids, are relatively rare. This indi-\ncates that the model relies primarily on visual cues rather\nthan taxonomic proximity when assigning class labels. In-\nterestingly, we observe strong detection performance for\npycnogonids, despite this class being the fourth least abun-\ndant. However this may be due to a lack of morphological\nvariation between the dataset splits for this class.\n7\n6.3. SAHI Postprocessing Limitations"
  },
  {
    "heading": "Unlike domains where SAHI is commonly applied, e.g.,",
    "content": "satellite imagery [9, 17, 36], data in the WSBD is captured\nfrom varying altitudes above the seafloor due to changes\nin OFOBS platform depth and seafloor topography. This\nintroduces significant intra-class size variation. For large\norganisms, a single instance may span many patches. Ex-\nperimental observations show SAHI occasionally struggles\nto accurately postprocess duplicate detections into a single\ncoherent bounding box when an organism is divided across\na large number of patches (see App. E). This negatively af-\nfects overall model performance and may bias abundance\nestimates if not addressed during manual post-hoc review.\n6.4. Expert Labelling Agreement"
  },
  {
    "heading": "It is important to note that evaluation metrics reported in",
    "content": "this study, as in other automated biodiversity monitoring re-\nsearch, e.g., [5], reflect the degree of agreement between the\nmodel and the human annotator rather than an absolute mea-\nsure of detection accuracy. Given the high densities of or-\nganisms, the prevalence of small-bodied taxa, and the well-\ndocumented issues of fatigue and subjectivity in manual an-\nnotation processes for benthic imagery [12, 15, 41], it is\nlikely some valid organisms were omitted from the ground\ntruth, leading to correct model detections being penalised\nand artificially lowering performance metrics."
  },
  {
    "heading": "Due to the high level of taxonomic expertise required",
    "content": "to accurately annotate Antarctic benthic fauna and the sig-\nnificant time investment needed (averaging approximately\neight hours per image), it was not feasible to obtain multiple\nindependent expert annotations to reduce potential labelling\nbias. With the time savings afforded by our framework, fu-\nture labelling can incorporate consensus agreement.\n6.5. Potential Framework Application"
  },
  {
    "heading": "Despite these challenges, the resulting model remains",
    "content": "highly valuable to benthic ecologists. The proposed frame-\nwork offers the potential for substantial time and cost sav-\nings, particularly when applied to the processing of exten-\nsively backlogged survey data, totalling in the tens of thou-\nsands of images. As a result, the framework is well-suited\nfor use in first-pass, human-in-the-loop analyses, allowing\necologists to focus on completing remaining annotations in-\nstead of reviewing full images manually."
  },
  {
    "heading": "A promising direction for future work involves integrat-",
    "content": "ing the proposed framework into an active learning pipeline,\nwherein unlabelled imagery is prioritised for annotation\nbased on predefined selection criteria, automatically anno-\ntated using the framework, then refined by expert ecolo-\ngists. Selection strategies may incorporate both ecological\nrelevance and expected contribution to framework perfor-\nmance. As more archival data is processed through this it-\nerative approach, the resulting enlarged dataset could serve\nas a valuable fine-tuning resource, especially for currently\nrare organisms where model performance may benefit from\nincreased abundance, enabling iterative improvements in\nmodel performance as annotation efforts progress.\n7. Conclusion"
  },
  {
    "heading": "Unknown",
    "content": "We address the challenge of detecting and classifying"
  },
  {
    "heading": "Antarctic benthic organisms in high-resolution, top-down",
    "content": "imagery captured using a towed camera system in the Wed-\ndell Sea. Through the creation of the first publicly available\ncomputer vision\u2013ready dataset of Antarctic seafloor ecol-\nogy, we develop and assess a comprehensive object detec-\ntion framework specifically designed for the complexities\nof benthic imagery. The proposed pipeline integrates the"
  },
  {
    "heading": "SAHI methodology, patching to retain spatial resolution and",
    "content": "reduce computational expense, alongside spatial data aug-\nmentation and model fine-tuning to support generalisability\nunder data-scarce conditions. Postprocessing using NMM\nenhances detection coherence after bounding box reprojec-\ntion from patch-level back to the original large-scale image."
  },
  {
    "heading": "Our framework demonstrates strong performance in detect-",
    "content": "ing medium and large benthic morphotypes."
  },
  {
    "heading": "Persistent underperformance on small and rare taxa, even",
    "content": "when enhancement strategies were applied, highlights fun-\ndamental limitations in current object detection architec-\ntures when applied to ecologically complex imagery. These\nfindings underscore the need for targeted research into small\nobject representation, as well as the potential value of active\nlearning approaches to enable faster processing of back-\nlogged, unprocessed field imagery, refining rare organism\nperformance while uncovering new ecological insights. By\nproviding our data and models open-source, we hope to en-\ncourage community efforts to improve the detection of such\norganisms in complex marine imagery. Nevertheless, our\nproposed framework offers a scalable, generalised solution\nfor automated analysis of high-resolution benthic imagery,\nwith significant potential to accelerate biodiversity moni-\ntoring and enable better protection of the unique benthic\necosystems found in Antarctica and beyond."
  },
  {
    "heading": "Unknown",
    "content": "Acknowledgements"
  },
  {
    "heading": "We thank Miao Fan, Alfred Wegener Institutue (AWI), for",
    "content": "providing the bathymetry maps used to subset the WSBD\nby seafloor inclination. We also thank Autun Purser, AWI,\nand all crew of the RV Polarstern PS118 cruise for their\ndata collection efforts. CT, HJG and RJW are funded by\nthe UKRI Future Leaders Fellowship MR/W01002X/1 \u2018The\npast, present and future of unique cold-water benthic (sea\nfloor) ecosystems in the Southern Ocean\u2019 awarded to RJW."
  },
  {
    "heading": "For the purpose of open access, the author(s) has applied",
    "content": "a Creative Commons Attribution (CC BY) license to any"
  },
  {
    "heading": "Unknown",
    "content": "Accepted Manuscript version arising."
  },
  {
    "heading": "Unknown",
    "content": "8"
  },
  {
    "heading": "References",
    "content": "[1] Fatih Cagatay Akyon, Sinan Onur Altinuc, and Alptekin"
  },
  {
    "heading": "Unknown",
    "content": "Temizel. Slicing Aided Hyper Inference and Fine-Tuning for"
  },
  {
    "heading": "Small Object Detection. In 2022 IEEE International Confer-",
    "content": "ence on Image Processing (ICIP), pages 966\u2013970, Bordeaux,"
  },
  {
    "heading": "France, 2022. IEEE. 3",
    "content": "[2] M Alcaraz. Biogeographic Atlas of the Southern Ocean. Sci-\nentific Committee on Antarctic Research, Cambridge, XII,\npage 498, 2014. 1\n[3] Romero-Ramirez Alicia, Morales Luna Hadrys Laura, Kuk-\nlinski Piotr, Chelchowski Maciej, and Balazy Piotr.\nIm-\nage analysis and benthic ecology: Proceedings to analyze in\nsitu long-term image series. Limnology and Oceanography:"
  },
  {
    "heading": "Methods, 21(4):169\u2013177, 2023. 1",
    "content": "[4] Tadas Baltrusaitis, Chaitanya Ahuja, and Louis-Philippe"
  },
  {
    "heading": "Morency. Multimodal Machine Learning: A Survey and Tax-",
    "content": "onomy. IEEE Transactions on Pattern Analysis and Machine"
  },
  {
    "heading": "Intelligence, 41(2):423\u2013443, 2019. 6",
    "content": "[5] Justine"
  },
  {
    "heading": "Unknown",
    "content": "Boulent,"
  },
  {
    "heading": "Unknown",
    "content": "Bertrand"
  },
  {
    "heading": "Unknown",
    "content": "Charry,"
  },
  {
    "heading": "Unknown",
    "content": "Malcolm"
  },
  {
    "heading": "Unknown",
    "content": "McHugh"
  },
  {
    "heading": "Unknown",
    "content": "Kennedy, Emily Tissier, Raina Fan, Marianne Marcoux,"
  },
  {
    "heading": "Unknown",
    "content": "Cortney A. Watt, and Antoine Gagn\u00b4e-Turcotte."
  },
  {
    "heading": "Scaling",
    "content": "whale monitoring using deep learning: A human-in-the-loop\nsolution for analyzing aerial datasets. Frontiers in Marine"
  },
  {
    "heading": "Science, 10:1099479, 2023. 8",
    "content": "[6] Alexander Buslaev, Vladimir I. Iglovikov, Eugene Khved-\nchenya, Alex Parinov, Mikhail Druzhinin, and Alexandr A."
  },
  {
    "heading": "Kalinin. Albumentations: Fast and Flexible Image Augmen-",
    "content": "tations. Information, 11(2):125, 2020. 4, 2\n[7] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delv-\ning into high quality object detection. In Proceedings of the"
  },
  {
    "heading": "IEEE Conference on Computer Vision and Pattern Recogni-",
    "content": "tion, pages 6154\u20136162, 2018. 5, 6\n[8] Prithvijit Chattopadhyay, Ramakrishna Vedantam, Ram-\nprasaath R. Selvaraju, Dhruv Batra, and Devi Parikh. Count-\ning Everyday Objects in Everyday Scenes. In 2017 IEEE"
  },
  {
    "heading": "Conference on Computer Vision and Pattern Recognition",
    "content": "(CVPR), pages 4428\u20134437, Honolulu, HI, 2017. IEEE. 3\n[9] Divyansh Chaurasia and B.D.K. Patro. Real-time Detection\nof Birds for Farm Surveillance Using YOLOv7 and SAHI. In\n2023 3rd International Conference on Computing and Infor-\nmation Technology (ICCIT), pages 442\u2013450, Tabuk, Saudi"
  },
  {
    "heading": "Arabia, 2023. IEEE. 4, 8",
    "content": "[10] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu"
  },
  {
    "heading": "Unknown",
    "content": "Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu,"
  },
  {
    "heading": "Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tian-",
    "content": "heng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu,"
  },
  {
    "heading": "Unknown",
    "content": "Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang,"
  },
  {
    "heading": "Unknown",
    "content": "Chen Change Loy, and Dahua Lin."
  },
  {
    "heading": "Unknown",
    "content": "MMDetection: Open"
  },
  {
    "heading": "MMLab Detection Toolbox and Benchmark, 2019. 4",
    "content": "[11] Daniel Crespo and Miguel \u02c6Angelo Pardal. Ecological and"
  },
  {
    "heading": "Economic Importance of Benthic Communities. In Life Be-",
    "content": "low Water, pages 313\u2013323. Springer International Publish-\ning, Cham, 2022. 1\n[12] Phil F. Culverhouse. Human and machine factors in algae\nmonitoring performance. Ecological Informatics, 2(4):361\u2013\n366, 2007. 1, 8\n[13] Matthew Dawkins, Charles Stewart, Scott Gallager, and Am-\nber York. Automatic scallop detection in benthic environ-\nments. In 2013 IEEE Workshop on Applications of Com-\nputer Vision (WACV), pages 160\u2013167, Clearwater Beach, FL,"
  },
  {
    "heading": "USA, 2013. IEEE. 2",
    "content": "[14] Heather Doig, Oscar Pizarro, Jacquomo Monk, and Stefan"
  },
  {
    "heading": "Unknown",
    "content": "Williams."
  },
  {
    "heading": "Detecting Endangered Marine Species in Au-",
    "content": "tonomous Underwater Vehicle Imagery Using Point Anno-\ntations and Few-Shot Learning, 2024. 5\n[15] Jm Durden, Bj Bett, T Schoening, Kj Morris, Tw Nattkem-\nper, and Ha Ruhl. Comparison of image annotation data gen-\nerated by multiple investigators for benthic ecology. Marine"
  },
  {
    "heading": "Ecology Progress Series, 552:61\u201370, 2016. 1, 8",
    "content": "[16] Jennifer M. Durden, Brett Hosking, Brian J. Bett, Danelle"
  },
  {
    "heading": "Cline, and Henry A. Ruhl. Automated classification of fauna",
    "content": "in seabed photographs: The impact of training and valida-\ntion dataset size, with considerations for the class imbalance."
  },
  {
    "heading": "Progress in Oceanography, 196:102612, 2021. 5",
    "content": "[17] Bao Tran Gia, Tuong Bui Cong Khanh, Hien Ho Trong,"
  },
  {
    "heading": "Unknown",
    "content": "Thuyen Tran Doan, Tien Do, Duy-Dinh Le, and Thanh Duc"
  },
  {
    "heading": "Ngo. Enhancing Road Object Detection in Fisheye Cam-",
    "content": "eras: An Effective Framework Integrating SAHI and Hybrid"
  },
  {
    "heading": "Inference. In 2024 IEEE/CVF Conference on Computer Vi-",
    "content": "sion and Pattern Recognition Workshops (CVPRW), pages\n7227\u20137235, Seattle, WA, USA, 2024. IEEE. 4, 8\n[18] Manuel"
  },
  {
    "heading": "Unknown",
    "content": "Gonz\u00b4alez-Rivero,"
  },
  {
    "heading": "Unknown",
    "content": "Oscar"
  },
  {
    "heading": "Unknown",
    "content": "Beijbom,"
  },
  {
    "heading": "Unknown",
    "content": "Alberto"
  },
  {
    "heading": "Unknown",
    "content": "Rodriguez-Ramirez, Dominic E. P. Bryant, Anjani Ganase,"
  },
  {
    "heading": "Unknown",
    "content": "Yeray Gonzalez-Marrero, Ana Herrera-Reveles, Emma V."
  },
  {
    "heading": "Unknown",
    "content": "Kennedy, Catherine J. S. Kim, Sebastian Lopez-Marcano,"
  },
  {
    "heading": "Unknown",
    "content": "Kathryn Markey, Benjamin P. Neal, Kate Osborne, Catalina"
  },
  {
    "heading": "Unknown",
    "content": "Reyes-Nivia, Eugenia M. Sampayo, Kristin Stolberg, Abbie"
  },
  {
    "heading": "Unknown",
    "content": "Taylor, Julie Vercelloni, Mathew Wyatt, and Ove Hoegh-"
  },
  {
    "heading": "Unknown",
    "content": "Guldberg."
  },
  {
    "heading": "Unknown",
    "content": "Monitoring of Coral Reefs Using Artificial"
  },
  {
    "heading": "Unknown",
    "content": "Intelligence:"
  },
  {
    "heading": "Unknown",
    "content": "A Feasible and Cost-Effective Approach."
  },
  {
    "heading": "Remote Sensing, 12(3):489, 2020. 3",
    "content": "[19] Huw J. Griffiths, Vonda J. Cummings, Anton Van de Putte,"
  },
  {
    "heading": "Rowan J. Whittle, and Catherine L. Waller. Antarctic benthic",
    "content": "ecological change. Nature Reviews Earth & Environment, 5\n(9):645\u2013664, 2024. 1\n[20] Dominica Harrison, Fabio Cabrera De Leo, Warren J. Gallin,"
  },
  {
    "heading": "Unknown",
    "content": "Farin Mir, Simone Marini, and Sally P. Leys."
  },
  {
    "heading": "Unknown",
    "content": "Machine"
  },
  {
    "heading": "Learning Applications of Convolutional Neural Networks",
    "content": "and Unet Architecture to Predict and Classify Demosponge"
  },
  {
    "heading": "Behavior. Water, 13(18):2512, 2021. 2",
    "content": "[21] Jeroen P. A. Hoekendijk, Benjamin Kellenberger, Geert"
  },
  {
    "heading": "Unknown",
    "content": "Aarts, Sophie Brasseur, Suzanne S. H. Poiesz, and Devis"
  },
  {
    "heading": "Tuia. Counting using deep learning regression gives value",
    "content": "to ecological surveys. Scientific Reports, 11(1):23209, 2021.\n3\n[22] Chris Jackett, Franziska Althaus, Kylie Maguire, Moshiur"
  },
  {
    "heading": "Unknown",
    "content": "Farazi, Ben Scoulding, Candice Untiedt, Tim Ryan, Peter"
  },
  {
    "heading": "Shanks, Pamela Brodie, and Alan Williams. A benthic sub-",
    "content": "strate classification method for seabed images using deep\nlearning: Application to management of deep-sea coral reefs."
  },
  {
    "heading": "Journal of Applied Ecology, 60(7):1254\u20131273, 2023. 3",
    "content": "[23] Glenn Jocher, Ayush Chaurasia, Alex Stoken, Jirka Borovec,"
  },
  {
    "heading": "NanoCode012, Yonghye Kwon, Kalen Michael, TaoXie, Ji-",
    "content": "acong Fang, Imyhxy, Lorna, Zeng Yifu, Colin Wong, Ab-\nhiram V, Diego Montes, Zhiqiang Wang, Cristi Fati, Je-\nbastin Nadar, Laughing, UnglvKitDe, Victor Sonck, Tkianai,"
  },
  {
    "heading": "Unknown",
    "content": "9"
  },
  {
    "heading": "Unknown",
    "content": "YxNONG, Piotr Skalski, Adam Hogan, Dhruv Nair, Max"
  },
  {
    "heading": "Unknown",
    "content": "Strobel, and Mrinal Jain."
  },
  {
    "heading": "Unknown",
    "content": "Ultralytics/yolov5:"
  },
  {
    "heading": "Unknown",
    "content": "V7.0 -"
  },
  {
    "heading": "YOLOv5 SOTA Realtime Instance Segmentation. Zenodo,",
    "content": "2022. 2\n[24] Tasnuva Ming Khan, Huw J. Griffiths, Rowan J. Whittle,"
  },
  {
    "heading": "Nile P. Stephenson, Katie M. Delahooke, Autun Purser, An-",
    "content": "drea Manica, and Emily G. Mitchell. Network analyses on\nphotographic surveys reveal that invertebrate predators do\nnot structure epibenthos in the deep (\u02dc2000m) rocky Powell"
  },
  {
    "heading": "Basin, Weddell Sea, Antarctica. Frontiers in Marine Science,",
    "content": "11:1408828, 2024. 2\n[25] Orlando Lam-Gordillo, Ryan Baring, and Sabine Dittmann."
  },
  {
    "heading": "Ecosystem functioning and functional approaches on marine",
    "content": "macrobenthic fauna: A research synthesis towards a global\nconsensus. Ecological Indicators, 115:106379, 2020. 1\n[26] Daniel Langenk\u00a8amper, Martin Zurowietz, Timm Schoening,\nand Tim W. Nattkemper. BIIGLE 2.0 - Browsing and Anno-\ntating Large Marine Image Collections. Frontiers in Marine"
  },
  {
    "heading": "Science, 4:83, 2017. 2",
    "content": "[27] Youngwan Lee and Jongyoul Park. CenterMask: Real-Time"
  },
  {
    "heading": "Unknown",
    "content": "Anchor-Free Instance Segmentation."
  },
  {
    "heading": "Unknown",
    "content": "In 2020 IEEE/CVF"
  },
  {
    "heading": "Conference on Computer Vision and Pattern Recognition",
    "content": "(CVPR), pages 13903\u201313912, Seattle, WA, USA, 2020."
  },
  {
    "heading": "IEEE. 2",
    "content": "[28] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,"
  },
  {
    "heading": "Unknown",
    "content": "Pietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C. Lawrence"
  },
  {
    "heading": "Unknown",
    "content": "Zitnick. Microsoft COCO: Common Objects in Context. In"
  },
  {
    "heading": "Computer Vision \u2013 ECCV 2014, pages 740\u2013755. Springer In-",
    "content": "ternational Publishing, Cham, 2014. 2, 5, 6\n[29] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and"
  },
  {
    "heading": "Piotr Doll\u00b4ar. Focal Loss for Dense Object Detection. Pro-",
    "content": "ceedings of the IEEE International Conference on Computer"
  },
  {
    "heading": "Vision (ICCV), 2017. 3, 6",
    "content": "[30] Yang Liu, Peng Sun, Nickolas Wergeles, and Yi Shang. A\nsurvey and performance evaluation of deep learning methods\nfor small object detection. Expert Systems with Applications,\n172:114602, 2021. 2, 3\n[31] Mona L\u00a8utjens and Harald Sternberg."
  },
  {
    "heading": "Deep Learning",
    "content": "based Detection, Segmentation and Counting of Benthic"
  },
  {
    "heading": "Unknown",
    "content": "Megafauna in Unconstrained Underwater Environments."
  },
  {
    "heading": "IFAC-PapersOnLine, 54(16):76\u201382, 2021. 2, 3, 5",
    "content": "[32] Simone Marini, Federico Bonofiglio, Lorenzo P. Corgnati,"
  },
  {
    "heading": "Unknown",
    "content": "Andrea Bordone, Stefano Schiaparelli, and Andrea Peirano."
  },
  {
    "heading": "Long-term automated visual monitoring of Antarctic benthic",
    "content": "fauna. Methods in Ecology and Evolution, 13(8):1746\u20131764,\n2022. 2\n[33] Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng,"
  },
  {
    "heading": "Unknown",
    "content": "Houqiang Li, Yuhui Yuan, Lei Sun, and Jingdong Wang."
  },
  {
    "heading": "Conditional DETR for Fast Training Convergence, 2023. 6",
    "content": "[34] Zhongqi Miao, Ziwei Liu, Kaitlyn M. Gaynor, Meredith S."
  },
  {
    "heading": "Palmer, Stella X. Yu, and Wayne M. Getz. Iterative human",
    "content": "and automated identification of wildlife images. Nature Ma-\nchine Intelligence, 3(10):885\u2013895, 2021. 3\n[35] Katsunori Mizuno, Kei Terayama, Seiichiro Hagino, Shigeru"
  },
  {
    "heading": "Tabeta, Shingo Sakamoto, Toshihiro Ogawa, Kenichi Sug-",
    "content": "imoto, and Hironobu Fukami."
  },
  {
    "heading": "An efficient coral survey",
    "content": "method based on a large-scale 3-D structure model obtained\nby Speedy Sea Scanner and U-Net segmentation. Scientific"
  },
  {
    "heading": "Reports, 10(1):12416, 2020. 2",
    "content": "[36] M. Muzammul, Abdulmohsen Algarni, Yazeed Yasin Ghadi,\nand Muhammad Assam."
  },
  {
    "heading": "Unknown",
    "content": "Enhancing UAV Aerial Image"
  },
  {
    "heading": "Unknown",
    "content": "Analysis:"
  },
  {
    "heading": "Unknown",
    "content": "Integrating Advanced SAHI Techniques With"
  },
  {
    "heading": "Unknown",
    "content": "Real-Time Detection Models on the VisDrone Dataset. IEEE"
  },
  {
    "heading": "Access, 12:21621\u201321633, 2024. 4, 8",
    "content": "[37] Ennio Ottaviani, Marco Francescangeli, Nikolla Gjeci,"
  },
  {
    "heading": "Unknown",
    "content": "Joaquin Del Rio Fernandez, Jacopo Aguzzi, and Simone"
  },
  {
    "heading": "Unknown",
    "content": "Marini. Assessing the Image Concept Drift at the OBSEA"
  },
  {
    "heading": "Coastal Underwater Cabled Observatory. Frontiers in Ma-",
    "content": "rine Science, 9:840088, 2022. 3\n[38] G. Pavoni, M. Corsini, N. Pedersen, V. Petrovic, and P."
  },
  {
    "heading": "Unknown",
    "content": "Cignoni."
  },
  {
    "heading": "Challenges in the deep learning-based semantic",
    "content": "segmentation of benthic communities from Ortho-images."
  },
  {
    "heading": "Applied Geomatics, 13(1):131\u2013146, 2021. 2, 5",
    "content": "[39] Gaia Pavoni, Massimiliano Corsini, Federico Ponchio,"
  },
  {
    "heading": "Unknown",
    "content": "Alessandro Muntoni, Clinton Edwards, Nicole Pedersen,"
  },
  {
    "heading": "Stuart Sandin, and Paolo Cignoni. TagLab: AI-assisted an-",
    "content": "notation for the fast and accurate semantic segmentation of\ncoral reef orthoimages."
  },
  {
    "heading": "Journal of Field Robotics, 39(3):",
    "content": "246\u2013262, 2022. 2\n[40] Malte Pedersen, Joakim Bruslund Haurum, Rikke Gade, and"
  },
  {
    "heading": "Unknown",
    "content": "Thomas B Moeslund. Detection of Marine Animals in a New"
  },
  {
    "heading": "Unknown",
    "content": "Underwater Dataset with Varying Visibility."
  },
  {
    "heading": "In Proceed-",
    "content": "ings of the IEEE/CVF Conference on Computer Vision and"
  },
  {
    "heading": "Unknown",
    "content": "Pattern Recognition Workshops, pages 18\u201326, Long Beach,"
  },
  {
    "heading": "USA, 2019. 2",
    "content": "[41] N Piechaud, C Hunt, Pf Culverhouse, Nl Foster, and Kl How-\nell. Automated identification of benthic epifauna with com-\nputer vision. Marine Ecology Progress Series, 615:15\u201330,\n2019. 1, 2, 8\n[42] Autun Purser, Yann Marcon, Simon Dreutter, Ulrich Hoge,"
  },
  {
    "heading": "Unknown",
    "content": "Burkhard Sablotny, Laura Hehemann, Johannes Lemburg,"
  },
  {
    "heading": "Unknown",
    "content": "Boris Dorschel, Harald Biebow, and Antje Boetius. Ocean"
  },
  {
    "heading": "Unknown",
    "content": "Floor Observation and Bathymetry System (OFOBS): A"
  },
  {
    "heading": "Unknown",
    "content": "New Towed Camera/Sonar System for Deep-Sea Habitat"
  },
  {
    "heading": "Surveys. IEEE Journal of Oceanic Engineering, 44(1):87\u2013",
    "content": "99, 2019. 2\n[43] Autun Purser, Simon Dreutter, Huw Griffiths, Laura Hehe-\nmann, Kerstin Jerosch, Axel Nordhausen, Dieter Piepenburg,"
  },
  {
    "heading": "Unknown",
    "content": "Claudio Richter, Henning Schr\u00a8oder, and Boris Dorschel."
  },
  {
    "heading": "Seabed video and still images from the northern Weddell Sea",
    "content": "and the western flanks of the Powell Basin. Earth System"
  },
  {
    "heading": "Science Data, 13(2):609\u2013615, 2021. 2",
    "content": "[44] Carl J. Reddin, Martin Aberhan, Nussa\u00a8\u0131bah B. Raja, and\n\u00b4Ad\u00b4am T. Kocsis. Global warming generates predictable ex-\ntinctions of warm- and cold-water marine benthic inverte-\nbrates via thermal habitat loss. Global Change Biology, 28\n(19):5793\u20135807, 2022. 1\n[45] H. L. Rees, editor. Guidelines for the Study of the Epibenthos\nof Subtidal Environments. Number 42 in ICES Techniques\nin Marine Environmental Sciences. International Council for\nthe Exploration of the Sea, Copenhagen, 2009. 1\n[46] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun."
  },
  {
    "heading": "Faster R-CNN: Towards real-time object detection with re-",
    "content": "gion proposal networks. In Advances in Neural Information"
  },
  {
    "heading": "Processing Systems. Curran Associates, Inc., 2015. 3, 5, 6",
    "content": "10\n[47] Yuntian Shi and Yaowu Li. Impacts of ocean acidification\non physiology and ecology of marine invertebrates: A com-\nprehensive review. Aquatic Ecology, 58(2):207\u2013226, 2024.\n1\n[48] Daniel Smith and Matthew Dunbabin. Automated Count-\ning of the Northern Pacific Sea Star in the Derwent Using"
  },
  {
    "heading": "Shape Recognition. In 9th Biennial Conference of the Aus-",
    "content": "tralian Pattern Recognition Society on Digital Image Com-\nputing Techniques and Applications (DICTA 2007), pages\n500\u2013507, 2007. 2\n[49] Cameron Trotter, Huw J. Griffiths, and Rowan J. Whittle."
  },
  {
    "heading": "Surveying the deep: A review of computer vision in the ben-",
    "content": "thos. Ecological Informatics, 86:102989, 2025. 1, 2, 6\n[50] Grant Van Horn and Pietro Perona. The Devil is in the Tails:"
  },
  {
    "heading": "Fine-grained Classification in the Wild, 2017. 3",
    "content": "[51] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui,"
  },
  {
    "heading": "Unknown",
    "content": "Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and"
  },
  {
    "heading": "Unknown",
    "content": "Serge Belongie. The iNaturalist Species Classification and"
  },
  {
    "heading": "Detection Dataset. In 2018 IEEE/CVF Conference on Com-",
    "content": "puter Vision and Pattern Recognition, pages 8769\u20138778, Salt"
  },
  {
    "heading": "Lake City, UT, 2018. IEEE. 3",
    "content": "[52] Ivor D. Williams, Courtney S. Couch, Oscar Beijbom,"
  },
  {
    "heading": "Thomas A. Oliver, Bernardo Vargas-Angel, Brett D. Schu-",
    "content": "macher, and Russell E. Brainard. Leveraging Automated Im-\nage Analysis Tools to Transform Our Capacity to Assess Sta-\ntus and Trends of Coral Reefs. Frontiers in Marine Science,\n6:222, 2019. 1\n[53] Mathew Wyatt, Ben Radford, Nikolaus Callow, Mohammed"
  },
  {
    "heading": "Bennamoun, and Sharyn Hickey. Using ensemble methods",
    "content": "to improve the robustness of deep learning for image clas-\nsification in marine environments. Methods in Ecology and"
  },
  {
    "heading": "Evolution, 13(6):1317\u20131328, 2022. 3",
    "content": "[54] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su,"
  },
  {
    "heading": "Unknown",
    "content": "Jun Zhu, Lionel M. Ni, and Heung-Yeung Shum. DINO:"
  },
  {
    "heading": "Unknown",
    "content": "DETR with Improved DeNoising Anchor Boxes for End-to-"
  },
  {
    "heading": "End Object Detection, 2022. 5, 6",
    "content": "[55] Lijun Zhang, Jiawen Fan, Yi Qiu, Zhe Jiang, Qingsong Hu,"
  },
  {
    "heading": "Bowen Xing, and Jingxiang Xu. Marine zoobenthos recog-",
    "content": "nition algorithm based on improved lightweight YOLOv5."
  },
  {
    "heading": "Ecological Informatics, 80:102467, 2024. 2",
    "content": "[56] Zhinuo Zhou, Ge-Yi Fu, Yi Fang, Ye Yuan, Hong-Bin Shen,"
  },
  {
    "heading": "Unknown",
    "content": "Chun-Sheng Wang, Xue-Wei Xu, Peng Zhou, and Xiaoyong"
  },
  {
    "heading": "Pan. EchoAI: A deep-learning based model for classifica-",
    "content": "tion of echinoderms in global oceans. Frontiers in Marine"
  },
  {
    "heading": "Science, 10:1147690, 2023. 2",
    "content": "[57] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,\nand Jifeng Dai. Deformable DETR: Deformable Transform-\ners for End-to-End Object Detection, 2021. 5, 6\n11\nAutomated Detection of Antarctic Benthic Organisms in High-Resolution In Situ"
  },
  {
    "heading": "Unknown",
    "content": "Imagery to Aid Biodiversity Monitoring"
  },
  {
    "heading": "Unknown",
    "content": "Supplementary Material"
  },
  {
    "heading": "A. Dataset Composition",
    "content": "Table S1. Counts and areas for the Weddell Sea Benthic Dataset classes, for both the original large-scale and patched images. Bolded\ncounts denote the most abundant classes used for 10-class evaluation."
  },
  {
    "heading": "Unknown",
    "content": "Whole Image Dataset"
  },
  {
    "heading": "Unknown",
    "content": "Patched Dataset"
  },
  {
    "heading": "Unknown",
    "content": "Count"
  },
  {
    "heading": "Unknown",
    "content": "Area (px2)"
  },
  {
    "heading": "Unknown",
    "content": "Count"
  },
  {
    "heading": "Unknown",
    "content": "Area (px2)"
  },
  {
    "heading": "Class Label",
    "content": "All"
  },
  {
    "heading": "Unknown",
    "content": "Train"
  },
  {
    "heading": "Unknown",
    "content": "Validation"
  },
  {
    "heading": "Test",
    "content": "Min\nMax\nAvg\nAll"
  },
  {
    "heading": "Unknown",
    "content": "Train"
  },
  {
    "heading": "Unknown",
    "content": "Validation"
  },
  {
    "heading": "Test",
    "content": "Min\nMax\nAvg\nactiniarian\n165\n118\n22\n25\n111\n51328\n6821\n777\n553\n105\n119\n111\n51328\n5512\nalcyonium\n280\n266\n6\n8\n266\n104355\n6773\n1269\n1202\n24\n43\n60\n98203\n5508\nanthomastus\n89\n60\n22\n7\n198\n30478\n4666\n371\n260\n80\n31\n106\n30478\n3810\nascidian cnemidocarpa verrucosa\n10\n2\n4\n4\n4689\n91344\n21113\n55\n15\n20\n20\n1517\n91344\n15311\nascidian distaplia\n32\n24\n1\n7\n1303\n1432444\n68092\n186\n146\n4\n36\n539\n250000\n34927\nascidian pyura bouvetensis\n66\n41\n21\n4\n981\n188218\n16652\n351\n203\n126\n22\n275\n153083\n12022\nasteroidia\n156\n111\n23\n22\n144\n569242\n9572\n643\n482\n74\n87\n26\n250000\n8279\nastrochlamys\n720\n528\n127\n65\n475\n136010\n13402\n3366\n2439\n556\n371\n110\n126777\n10608\nbenthic fish\n71\n52\n14\n5\n309\n179118\n40603\n474\n349\n92\n33\n309\n139500\n23828\nbryozoan\n15\n8\n5\n2\n386\n45004\n14134\n88\n45\n35\n8\n386\n45004\n9918\ncrinoid\n26\n19\n3\n4\n491\n20989\n6422\n119\n78\n14\n27\n232\n20989\n5377\ncrustaceans\n461\n338\n79\n44\n554\n399481\n12799\n2469\n1817\n424\n228\n253\n173560\n9494\ncucumber\n355\n287\n41\n27\n62\n9041\n1522\n1447\n1166\n164\n117\n56\n9041\n1415\ncup coral\n4757\n2807\n1611\n339\n31\n13756\n520\n18552\n11262\n6039\n1251\n12\n13756\n482\ndemosponges\n2211\n1517\n340\n354\n7\n258424\n2283\n8960\n6216\n1312\n1432\n7\n194988\n2003\nechinoid\n11\n6\n2\n3\n1975\n12170\n4098\n50\n24\n12\n14\n817\n12170\n3607\nglass sponge\n2308\n1612\n477\n219\n29\n92647\n1603\n9144\n6295\n1930\n919\n13\n92647\n1508\ngorgonian\n1144\n903\n113\n128\n62\n303363\n9045\n5396\n4248\n546\n602\n62\n219445\n7274\nhydroid solitary\n25\n17\n5\n3\n2519\n97165\n14968\n133\n94\n25\n14\n378\n97165\n11492\nophiosabine\n3075\n1853\n694\n528\n219\n50859\n3125\n12783\n7630\n2897\n2256\n68\n50859\n2768\nophiuroid 5 arms\n1885\n1293\n280\n312\n93\n748890\n17419\n8819\n5961\n1349\n1509\n72\n250000\n13686\npencil urchin\n78\n51\n20\n7\n700\n36549\n6419\n338\n219\n86\n33\n130\n36549\n5296\npycnogonid\n11\n7\n2\n2\n2381\n25210\n13854\n59\n40\n9\n10\n747\n25210\n9264\nstylasterids\n13295\n9547\n2011\n1736\n4\n74999\n1720\n53523\n38529\n7543\n7451\n4\n69524\n1560\nworm tubes\n35\n19\n2\n14\n168\n25355\n4591\n157\n82\n8\n67\n168\n25355\n3958"
  },
  {
    "heading": "Total",
    "content": "31280\n21486\n5925\n3869\n129529\n89355\n23474\n16700"
  },
  {
    "heading": "Unknown",
    "content": "1"
  },
  {
    "heading": "B. Data Augmentation Strategy Details",
    "content": "Tab. S2 presents the Albumentations-based [6] data augmentation techniques used in this study, categorised by the augmen-\ntation strategy in which they were employed. Probabilities for all augmentations were set to 0.50. For Random Sized BBox\nSafe Crop, the height and width parameters were set to the patch size. All other parameters were set to the Albumentations\ndefault.\nDespite its name, Pixel Dropout is classified as a spatial transformation. This augmentation operates non-uniformly\nacross the image by randomly selecting specific spatial coordinates at which to drop pixels. Consequently, it alters the spatial\nstructure of the image rather than applying a uniform change across all pixels.\nTable S2. A list of data augmentation techniques provided by the Albumentations library, along with the augmentation strategy in which\neach technique was applied."
  },
  {
    "heading": "Unknown",
    "content": "Augmentation"
  },
  {
    "heading": "Unknown",
    "content": "Strategy"
  },
  {
    "heading": "Unknown",
    "content": "Pixel"
  },
  {
    "heading": "Unknown",
    "content": "Spatial"
  },
  {
    "heading": "Unknown",
    "content": "Both"
  },
  {
    "heading": "Horizontal Flip",
    "content": "\u2717\n\u2713\n\u2713"
  },
  {
    "heading": "Motion Blur",
    "content": "\u2713\n\u2717\n\u2713"
  },
  {
    "heading": "Pixel Dropout",
    "content": "\u2717\n\u2713\n\u2713"
  },
  {
    "heading": "Random Brightness and Contrast",
    "content": "\u2713\n\u2717\n\u2713"
  },
  {
    "heading": "Random Shadow",
    "content": "\u2713\n\u2717\n\u2713"
  },
  {
    "heading": "Random Sized BBox Safe Crop",
    "content": "\u2717\n\u2713\n\u2713"
  },
  {
    "heading": "Vertical Flip",
    "content": "\u2717\n\u2713\n\u2713"
  },
  {
    "heading": "Unknown",
    "content": "2"
  },
  {
    "heading": "C. Weddell Sea Benthic Dataset High-Resolution Examples",
    "content": "Example Weddell Sea Benthic Dataset test set image outputs. Predicted organism bounding boxes, class labels, and confi-\ndence scores shown after reprojection and postprocessing. Confidence threshold = 0.60.\nFigure S1. HOTKEY 2019 03 31 at 13 30 13 IMG 0853. Original size: 3799\u00d73798 px.\n3\nFigure S2. HOTKEY 2019 03 31 at 13 21 23 IMG 0816. Original size: 2975\u00d72964 px.\n4\nFigure S3. TIMER 2019 03 06 at 05 40 47 IMG 0253. Original size: 5760\u00d73840 px."
  },
  {
    "heading": "Unknown",
    "content": "5"
  },
  {
    "heading": "D. Effect of Abundance on Model Performance",
    "content": "Organism abundance was shown to have a large effect on framework performance. See 6.2 for further discussion.\nFigure S4. Confusion matrix for the optimal framework configuration, ordered by abundance. Red lines indicate the top-10 most abundant\nclasses. Confidence threshold = 0.60."
  },
  {
    "heading": "Unknown",
    "content": "6"
  },
  {
    "heading": "E. SAHI Postprocessing Limitations",
    "content": "Large objects, split over a high number of patches, may fail to merge into a single coherent bounding box after Non-Maximum\nMerging via SAHI. See 6.3 for further discussion. Confidence threshold = 0.60.\nFigure S5. A single ophiuroid 5 arms, represented by two bounding boxes after postprocessing. Cropped and enlarged for clarity.\n7"
  }
]