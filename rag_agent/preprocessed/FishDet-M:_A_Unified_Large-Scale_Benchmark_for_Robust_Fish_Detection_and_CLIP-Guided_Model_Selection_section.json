[
  {
    "heading": "Unknown",
    "content": "MANUSCRIPT SUBMITTED TO IEEE TRANSACTIONS ON IMAGE PROCESSING, 2025"
  },
  {
    "heading": "Unknown",
    "content": "1"
  },
  {
    "heading": "Unknown",
    "content": "FishDet-M: A Unified Large-Scale Benchmark for"
  },
  {
    "heading": "Unknown",
    "content": "Robust Fish Detection and CLIP-Guided Model"
  },
  {
    "heading": "Unknown",
    "content": "Selection in Diverse Aquatic Visual Domains"
  },
  {
    "heading": "Unknown",
    "content": "Muayad Abujabal, Lyes Saad Saoud, and Irfan Hussain"
  },
  {
    "heading": "Abstract\u2014Accurate fish detection in underwater imagery is",
    "content": "essential for ecological monitoring, aquaculture automation, and\nrobotic perception. However, practical deployment remains limited\nby fragmented datasets, heterogeneous imaging conditions, and\ninconsistent evaluation protocols. To address these gaps, we present"
  },
  {
    "heading": "FishDet-M, the largest unified benchmark for fish detection,",
    "content": "comprising 13 publicly available datasets spanning diverse aquatic\nenvironments including marine, brackish, occluded, and aquarium\nscenes. All data are harmonized using COCO-style annotations\nwith both bounding boxes and segmentation masks, enabling\nconsistent and scalable cross-domain evaluation. We systematically\nbenchmark 28 contemporary object detection models, covering\nthe YOLOv8 to YOLOv12 series, R-CNN based detectors, and"
  },
  {
    "heading": "DETR based models. Evaluations are conducted using standard",
    "content": "metrics including mAP, mAP@50, and mAP@75, along with\nscale-specific analyses (APS, APM, APL) and inference profiling\nin terms of latency and parameter count. The results highlight the\nvarying detection performance across models trained on FishDet-\nM, as well as the trade-off between accuracy and efficiency across\nmodels of different architectures. To support adaptive deployment,\nwe introduce a CLIP-based model selection framework that\nleverages vision-language alignment to dynamically identify the\nmost semantically appropriate detector for each input image. This\nzero-shot selection strategy achieves high performance without\nrequiring ensemble computation, offering a scalable solution\nfor real-time applications. FishDet-M establishes a standardized\nand reproducible platform for evaluating object detection in\ncomplex aquatic scenes. All datasets, pretrained models, and\nevaluation tools are publicly available to facilitate future research\nin underwater computer vision and intelligent marine systems.\nIndex Terms\u2014fish detection, underwater vision, object detection,\nbenchmark dataset, CLIP-based model selection, YOLO, deep\nlearning, marine robotics, ecological monitoring"
  },
  {
    "heading": "Unknown",
    "content": "I. INTRODUCTION"
  },
  {
    "heading": "Accurate fish detection in underwater environments remains",
    "content": "a major challenge due to the visual degradation caused by tur-\nbidity, occlusion, light scattering, and dynamic backgrounds [1],\n[2], [3]. These conditions reduce contrast, distort color, and\nintroduce clutter, impairing the reliability of vision-based\ndetection models. Occlusions from vegetation, rocks, or dense\nschools of fish further obscure targets [4], [5]. Although deep\nlearning has advanced underwater perception, its effectiveness\nis constrained by fragmented datasets that often lack diversity,\nconsistent annotation formats, and representation of complex\nreal-world conditions [6], [7], [8], [9].\nThis work was conducted at the Khalifa University Center for Autonomous"
  },
  {
    "heading": "Robotic Systems (KUCARS), Abu Dhabi, United Arab Emirates.",
    "content": "The authors are with Khalifa University, Abu Dhabi, UAE (e-mails:\n100057733@ku.ac.ae; lyes.saoud@ku.ac.ae; irfan.hussain@ku.ac.ae).\nTo address these limitations, several domain-specific datasets\nhave emerged. The SmallFish dataset focuses on detecting\nsmall targets in poor visibility [1], DUFish captures densely\npacked schooling behavior [10], and DePondFi introduces\npond-based fish imagery from naturally challenging condi-\ntions [8]. In parallel, detection models have evolved to address\nunderwater visual distortions. FishDet-YOLO incorporates\nenhancement modules for low-contrast targets [11], YOLOv8"
  },
  {
    "heading": "TF introduces transformer-enhanced refinement and class-",
    "content": "sensitive learning [12], and the IDLAFD UWSN framework\nleverages hybrid architectures to improve detection under blur\nand occlusion [13]."
  },
  {
    "heading": "Despite these advances, the field still lacks a unified",
    "content": "benchmark capable of supporting large scale and cross-domain\nevaluation in realistic aquatic environments. To bridge this gap,\nwe introduce FishDet-M, a consolidated benchmark constructed\nby harmonizing 13 publicly available datasets into a single\nannotation format aligned with the COCO protocol. FishDet-"
  },
  {
    "heading": "M spans diverse underwater environments including coral",
    "content": "reefs, aquaculture tanks, and brackish waters, and provides\n296,885 annotated fish instances across 105,556 images. This\nbenchmark supports the evaluation of 28 leading detection\nmodels, including YOLO variants, DETR-based architectures,\nand region proposal networks."
  },
  {
    "heading": "The contributions of this work are summarized as follows:",
    "content": "\u2022 FishDet-M Benchmark: We introduce a unified bench-\nmark dataset for fish detection that integrates 13 publicly\navailable underwater datasets into a harmonized annota-\ntion format compatible with the COCO standard. The\ndataset includes diverse environments, species, and visual\nconditions.\n\u2022 Comprehensive Evaluation of Detection Models: We\nevaluate 28 advanced object detection architectures includ-\ning YOLOv8 through YOLOv12, DETR-based models,\nand R-CNN models. The evaluation uses standardized\nmetrics such as mean average precision (mAP), scale-\naware scores (APS, APM, APL), inference time, and\nparameter count.\n\u2022 CLIP-Based Adaptive Model Selection: We propose\na context-aware mechanism that utilizes vision-language\nalignment with CLIP to automatically select the most\nappropriate detection model based on input image content,\nenabling dynamic and robust inference.\n\u2022 Performance Insights and Deployment Guidelines:"
  },
  {
    "heading": "We analyze model robustness under conditions such",
    "content": "as occlusion and poor visibility, providing actionable\narXiv:2507.17859v1  [cs.CV]  23 Jul 2025\nABUJABAL et al.: FISHDET-M: A UNIFIED LARGE-SCALE BENCHMARK FOR ROBUST FISH DETECTION\nFig. 1: Interactive application interface for comparing detection outputs on FishDet-M. Supports multiple models, bounding box\noverlays, and threshold control.\nrecommendations for selecting models suitable for real-\ntime ecological monitoring, aquaculture management, and\nrobotic deployment.\n\u2022 Public Release to Support Reproducibility: The full\ndataset, source code, checkpoints, and evaluation tools\nare released publicly through our GitHub repository\nto promote reproducible research and facilitate further\ndevelopment."
  },
  {
    "heading": "Unknown",
    "content": "II. RELATED WORKS"
  },
  {
    "heading": "Unknown",
    "content": "A. Underwater Fish Detection Challenges and Solutions"
  },
  {
    "heading": "Underwater fish detection remains a challenging task due",
    "content": "to the complex optical properties of marine environments."
  },
  {
    "heading": "Turbidity, light scattering, and occlusion degrade visibility",
    "content": "and introduce background clutter [3], [6], making accurate\ndetection difficult, particularly for deformable and low contrast\ntargets."
  },
  {
    "heading": "To mitigate these effects, recent methods combine underwater",
    "content": "specific image enhancement, such as illumination correction\nand color restoration, with adapted deep learning detectors [14],"
  },
  {
    "heading": "TABLE I: Representative fish detection datasets used in",
    "content": "underwater computer vision research"
  },
  {
    "heading": "Unknown",
    "content": "Dataset"
  },
  {
    "heading": "Unknown",
    "content": "Environment"
  },
  {
    "heading": "Unknown",
    "content": "Size"
  },
  {
    "heading": "Unknown",
    "content": "Annotations Species"
  },
  {
    "heading": "Unknown",
    "content": "Diversity"
  },
  {
    "heading": "Unknown",
    "content": "SmallFish [1]"
  },
  {
    "heading": "Murky tanks",
    "content": "5,000"
  },
  {
    "heading": "BBoxes",
    "content": "Low"
  },
  {
    "heading": "Unknown",
    "content": "DePondFi [8]"
  },
  {
    "heading": "Pond",
    "content": "8,000"
  },
  {
    "heading": "Unknown",
    "content": "BBoxes"
  },
  {
    "heading": "Unknown",
    "content": "Medium"
  },
  {
    "heading": "Unknown",
    "content": "DUFish [10]"
  },
  {
    "heading": "Open water",
    "content": "6,300"
  },
  {
    "heading": "BBoxes",
    "content": "and"
  },
  {
    "heading": "Unknown",
    "content": "Masks"
  },
  {
    "heading": "Unknown",
    "content": "High"
  },
  {
    "heading": "Unknown",
    "content": "FishTrack23 [17]"
  },
  {
    "heading": "Varied habi-",
    "content": "tats\n20K"
  },
  {
    "heading": "Tracks",
    "content": "and"
  },
  {
    "heading": "Unknown",
    "content": "BBoxes"
  },
  {
    "heading": "Unknown",
    "content": "High"
  },
  {
    "heading": "Unknown",
    "content": "OcclusionSet [4]"
  },
  {
    "heading": "Reef zones",
    "content": "3,500"
  },
  {
    "heading": "Unknown",
    "content": "BBoxes"
  },
  {
    "heading": "Medium",
    "content": "[11]. However, performance still drops in scenes with severe\nocclusion or high visual complexity [15]."
  },
  {
    "heading": "Dataset limitations further compound these issues. Many",
    "content": "existing datasets focus on controlled settings with limited\nspecies diversity or environmental variation [1], [16]. Newer\ndatasets like DUFish [10] and FishTrack23 [17] improve\ndiversity and realism, but the broader dataset landscape remains\nfragmented. Table I summarizes representative datasets.\n2\nABUJABAL et al.: FISHDET-M: A UNIFIED LARGE-SCALE BENCHMARK FOR ROBUST FISH DETECTION"
  },
  {
    "heading": "Deep learning has improved detection performance with",
    "content": "models such as YOLO, Cascade R-CNN, and DETR [18], [19],\n[20], enhanced by attention modules, deformable layers, and\ndomain adaptation [11], [21], [12]. Hybrid convolutional and\ntransformer based architectures further improve robustness [12],\nwhile fine tuning on augmented datasets aids generalization to\nunseen conditions."
  },
  {
    "heading": "Challenges persist due to occlusion, camouflage, and inter-",
    "content": "species variability [4], [22]. Addressing these requires context\naware training and robust augmentation strategies. Evaluation\nprotocols are expanding beyond mean average precision and\nintersection over union to include scale aware metrics, latency,\nand model size [1], [10], supporting real time deployment."
  },
  {
    "heading": "Despite advances, cross domain variation and inconsistent",
    "content": "benchmarks limit generalizability [23]. Overcoming these\nissues remains essential for enabling reliable fish detection\nin diverse marine environments."
  },
  {
    "heading": "Unknown",
    "content": "B. Fish Detection Datasets and Benchmarks"
  },
  {
    "heading": "The effectiveness of detection models relies heavily on",
    "content": "diverse and well-annotated datasets. Many earlier datasets\nare restricted to clear or shallow environments, limiting their\ngeneralizability. Real-world aquatic scenes vary significantly\nin turbidity, lighting, and species composition, necessitating\ndiverse benchmarks for robust evaluation [9], [1]."
  },
  {
    "heading": "Recent efforts have addressed these limitations by expanding",
    "content": "ecological and visual variability. DePondFi [9] offers annotated\npond imagery where YOLO models achieve high mAP@50\nscores. Fish4Knowledge supports both detection and classifica-\ntion in marine environments, with FishNet detector achieving\nover 92 percent detection precision [24]. SmallFish is tailored\nto small object detection in murky conditions [1]."
  },
  {
    "heading": "FishTrack23 [17] emphasizes dense tracking across habitats,",
    "content": "while LifeCLEF 2014 [25] serves as a benchmark for fish\nrecognition. Other datasets like UOMT [26] and FS48 [27] ad-\ndress salient object detection and re-identification. OzFish [28],\n[29] include species-level annotations for fish in Australian\nshores."
  },
  {
    "heading": "Table II provides a comparative overview of these datasets,",
    "content": "highlighting environments, tasks, and public availability. De-\nspite their contributions, the field still lacks unified annotation\nstandards, benchmark protocols, and cross-study comparability."
  },
  {
    "heading": "Upcoming datasets such as FishDet-M aim to address these",
    "content": "challenges through structured metadata and realistic test\nconditions."
  },
  {
    "heading": "Unknown",
    "content": "C. Deep Learning Models for Underwater Object Detection"
  },
  {
    "heading": "Deep learning has substantially advanced underwater detec-",
    "content": "tion performance, particularly in environments where image\nquality is degraded by turbidity, occlusion, and variable\nlighting [11], [21], [28]. Foundational models like YOLO [12],"
  },
  {
    "heading": "Faster R-CNN [30], and DETR have been widely adapted for",
    "content": "marine detection pipelines."
  },
  {
    "heading": "To address visual limitations, researchers have introduced",
    "content": "attention mechanisms, deformable layers, and fusion modules\nthat enhance feature representation for partially visible targets."
  },
  {
    "heading": "Transformers have also been incorporated into convolutional",
    "content": "backbones, enabling long-range feature modeling in low-\ncontrast conditions [12], [13]."
  },
  {
    "heading": "Unknown",
    "content": "Specialized variants such as FishDet-YOLO and YOLOv8-"
  },
  {
    "heading": "TF focus on underwater data distributions and address class",
    "content": "imbalance [11], [12].Domain adaptation techniques, including\nadversarial learning and style transfer, further help bridge per-\nformance gaps between synthetic and real-world datasets [31]."
  },
  {
    "heading": "Hybrid models that fuse convolution and transformer com-",
    "content": "ponents leverage the spatial localization strengths of CNNs\nand the semantic context modeling of transformers. This\ncombination enhances detection accuracy in cluttered, occluded,\nand dynamic marine scenes [12]."
  },
  {
    "heading": "These architectures are increasingly designed with de-",
    "content": "ployment in mind. Their applications range from ecological\nmonitoring and fishery management to robotic exploration and\nreal-time fish tracking [32], [33], [34]."
  },
  {
    "heading": "Unknown",
    "content": "D. Occlusion Handling and Species Variability"
  },
  {
    "heading": "Occlusion is a persistent challenge in underwater scenes,",
    "content": "particularly in crowded habitats or reef zones. Solutions include"
  },
  {
    "heading": "3D geometric modeling, plan mirror-based occlusion mitigation,",
    "content": "and rotated bounding box regression [35], [36]. Transformer-\nenhanced YOLO models and approaches using repulsion\nloss have also shown improved performance on overlapping\ntargets [4]."
  },
  {
    "heading": "Generative augmentation using models such as DCGAN",
    "content": "and UIEGAN contributes occlusion-rich samples, enhancing\ntraining diversity [37], [38]. Temporal tracking methods further\nimprove robustness by associating occluded objects across\nframes [39]."
  },
  {
    "heading": "Species variability adds further complexity due to mor-",
    "content": "phological and chromatic diversity. Fine-grained models use\nattention mechanisms, patch localization, and multi-branch\narchitectures to detect subtle inter-species differences [40], [41]."
  },
  {
    "heading": "Camouflaged species, which exploit background matching or",
    "content": "polarized light, remain especially challenging [42]."
  },
  {
    "heading": "Advanced strategies such as active detection and few-shot",
    "content": "learning are being used to adapt models efficiently with\nminimal data [43]. Lightweight models like MobileNetV2,\nwhen fine-tuned with semantic modules, offer promising trade-\noffs between performance and computational load [44].\nTable III outlines the key approaches for addressing occlusion\nand species-level variation in detection systems."
  },
  {
    "heading": "Unknown",
    "content": "E. Evaluation Metrics and Benchmarking Protocols"
  },
  {
    "heading": "Reliable benchmarking in underwater detection depends",
    "content": "on consistent and comprehensive evaluation criteria. Standard\nmetrics such as mAP, IoU, precision, and recall are commonly\nreported [46], [47], but variations in implementation and dataset\nusage often hinder reproducibility [48]."
  },
  {
    "heading": "To address detection at different object scales, researchers",
    "content": "employ APS, APM, and APL metrics that reflect performance\non small, medium, and large objects. These are critical for\nunderstanding behavior in turbid conditions, where smaller fish\nare more likely to be missed [49], [50]."
  },
  {
    "heading": "Practical deployments also require runtime metrics such as",
    "content": "inference time, memory usage, and model size. These determine\n3\nABUJABAL et al.: FISHDET-M: A UNIFIED LARGE-SCALE BENCHMARK FOR ROBUST FISH DETECTION\nTABLE II: Representative fish detection datasets and benchmarks."
  },
  {
    "heading": "Unknown",
    "content": "Dataset"
  },
  {
    "heading": "Unknown",
    "content": "Environment"
  },
  {
    "heading": "Unknown",
    "content": "Main Task"
  },
  {
    "heading": "Unknown",
    "content": "Performance Highlights"
  },
  {
    "heading": "Unknown",
    "content": "Availability"
  },
  {
    "heading": "Unknown",
    "content": "Link"
  },
  {
    "heading": "Unknown",
    "content": "DePondFi [9]"
  },
  {
    "heading": "Unknown",
    "content": "Pond, South India"
  },
  {
    "heading": "Unknown",
    "content": "Real-time object detection"
  },
  {
    "heading": "YOLOv8: mAP@50 = 0.92; Ensemble = 0.94",
    "content": "\u2714"
  },
  {
    "heading": "Unknown",
    "content": "Link"
  },
  {
    "heading": "Unknown",
    "content": "Fish4Knowledge-2010 [24]"
  },
  {
    "heading": "Unknown",
    "content": "Marine scenes"
  },
  {
    "heading": "Detection and classification",
    "content": "mAP = 92.3%, Accuracy = 89.7%\n\u2714"
  },
  {
    "heading": "Unknown",
    "content": "Link"
  },
  {
    "heading": "Unknown",
    "content": "SmallFish [1]"
  },
  {
    "heading": "Unknown",
    "content": "Murky, cluttered"
  },
  {
    "heading": "Unknown",
    "content": "Detection of small targets"
  },
  {
    "heading": "Enhanced mAP via Fish-Finder algorithm",
    "content": "\u2717\n\u2013"
  },
  {
    "heading": "Unknown",
    "content": "FishTrack23 [17]"
  },
  {
    "heading": "Unknown",
    "content": "Multi-habitat"
  },
  {
    "heading": "Multi-object tracking",
    "content": "20,000 expert tracks\n\u2714"
  },
  {
    "heading": "Unknown",
    "content": "Link"
  },
  {
    "heading": "Unknown",
    "content": "LifeCLEF 2014 [25]"
  },
  {
    "heading": "Unknown",
    "content": "Video footage"
  },
  {
    "heading": "Unknown",
    "content": "Detection and recognition"
  },
  {
    "heading": "Evaluated in open competition",
    "content": "\u2714"
  },
  {
    "heading": "Unknown",
    "content": "Link"
  },
  {
    "heading": "Unknown",
    "content": "UOMT [26]"
  },
  {
    "heading": "Unknown",
    "content": "Mixed marine"
  },
  {
    "heading": "Unknown",
    "content": "Salient object detection"
  },
  {
    "heading": "Supports multitask learning",
    "content": "\u2714"
  },
  {
    "heading": "Unknown",
    "content": "Link"
  },
  {
    "heading": "Unknown",
    "content": "FS48 [27]"
  },
  {
    "heading": "Unknown",
    "content": "Controlled lighting"
  },
  {
    "heading": "Unknown",
    "content": "Re-identification"
  },
  {
    "heading": "Multi-view detection using FSNet",
    "content": "\u2717\n\u2013"
  },
  {
    "heading": "Unknown",
    "content": "DeepFish [28]"
  },
  {
    "heading": "Unknown",
    "content": "Tropical Australia"
  },
  {
    "heading": "Unknown",
    "content": "Classification and sizing"
  },
  {
    "heading": "Evaluated with SOTA models",
    "content": "\u2714"
  },
  {
    "heading": "Unknown",
    "content": "Link"
  },
  {
    "heading": "Unknown",
    "content": "OzFish [21]"
  },
  {
    "heading": "Unknown",
    "content": "Coastal marine"
  },
  {
    "heading": "Unknown",
    "content": "YOLO-based detection"
  },
  {
    "heading": "Robust YOLOv8/NAS results",
    "content": "\u2714"
  },
  {
    "heading": "Unknown",
    "content": "Link"
  },
  {
    "heading": "TABLE III: Summary of methods addressing occlusion, species",
    "content": "variability, and camouflage."
  },
  {
    "heading": "Unknown",
    "content": "Technique"
  },
  {
    "heading": "Description",
    "content": "3D\ntracking\nmodels [15], [36]"
  },
  {
    "heading": "Geometric modeling for occlusion tracking using",
    "content": "mirror imaging and spatial cues"
  },
  {
    "heading": "Rotating box detec-",
    "content": "tion [35]"
  },
  {
    "heading": "Uses oriented boxes to resolve overlaps in dense",
    "content": "scenes"
  },
  {
    "heading": "Unknown",
    "content": "Enhanced"
  },
  {
    "heading": "YOLO",
    "content": "models [4]"
  },
  {
    "heading": "Incorporates modules and repulsion loss for",
    "content": "occlusion robustness"
  },
  {
    "heading": "Synthetic data genera-",
    "content": "tion [37], [38]"
  },
  {
    "heading": "Adversarial networks used to balance datasets",
    "content": "with rare and occluded samples"
  },
  {
    "heading": "Underwater image en-",
    "content": "hancement [37]"
  },
  {
    "heading": "Unknown",
    "content": "Improves contrast and clarity prior to detection"
  },
  {
    "heading": "Multi-object",
    "content": "tracking [39]"
  },
  {
    "heading": "Unknown",
    "content": "Maintains temporal consistency in sequential data"
  },
  {
    "heading": "Vision",
    "content": "transformers\nwith FGVC [40]"
  },
  {
    "heading": "Discriminative attention modules for fine-grained",
    "content": "classification"
  },
  {
    "heading": "Dual-branch",
    "content": "fusion\nnetworks [41]"
  },
  {
    "heading": "Integrates inter-species similarity and semantic",
    "content": "fusion"
  },
  {
    "heading": "Attention",
    "content": "mechanisms [45]"
  },
  {
    "heading": "Enhances localized feature extraction in camou-",
    "content": "flage detection"
  },
  {
    "heading": "Unknown",
    "content": "Polarocrypsis [42]"
  },
  {
    "heading": "Unknown",
    "content": "Biological camouflage using polarization cues"
  },
  {
    "heading": "Active detection mod-",
    "content": "els [43]"
  },
  {
    "heading": "Unknown",
    "content": "Epistemic uncertainty used for sample selection"
  },
  {
    "heading": "Unknown",
    "content": "Transfer learning [44]"
  },
  {
    "heading": "Fine-tunes pretrained models using edge and",
    "content": "texture-aware modules\nwhether models can be implemented on embedded devices or\nunderwater robots with limited computational resources [51]."
  },
  {
    "heading": "Image quality assessment metrics, including UWEQM [52]",
    "content": "and QDE [48], are also important. These help quantify\nthe impact of enhancement and restoration steps on overall\ndetection reliability. Recent multi-exposure and fusion-based\nschemes offer richer evaluations that integrate contrast, chroma,\nand structural consistency [53]."
  },
  {
    "heading": "Standardizing benchmarking protocols and dataset splits will",
    "content": "improve comparability across studies. New efforts should aim to\ncombine performance metrics with deployment considerations\nand perceptual quality assessments to guide model selection\nfor real-world marine applications."
  },
  {
    "heading": "F. Applications in Aquaculture, Robotics, and Marine Moni-",
    "content": "toring"
  },
  {
    "heading": "Fish detection models play a vital role across aquaculture,",
    "content": "robotics, and environmental monitoring. In aquaculture, deep\nlearning enables automated fish counting, size estimation, and\nhealth assessment under turbid and low-light conditions [54],\n[55], [56]. Lightweight models such as AquaYOLO and CUIB"
  },
  {
    "heading": "YOLO [57], [58] are optimized for embedded deployment and",
    "content": "integrate with smart feeding and water monitoring systems [59]."
  },
  {
    "heading": "TABLE IV: Overview of the FishDet-M dataset splits and",
    "content": "annotations."
  },
  {
    "heading": "Unknown",
    "content": "Feature"
  },
  {
    "heading": "Unknown",
    "content": "Training Set"
  },
  {
    "heading": "Unknown",
    "content": "Validation Set"
  },
  {
    "heading": "Unknown",
    "content": "Test Set"
  },
  {
    "heading": "Number of Images",
    "content": "83,093\n10,654\n11,809"
  },
  {
    "heading": "Number of Fish Instances",
    "content": "228,558\n32,961\n35,366"
  },
  {
    "heading": "Annotated Species",
    "content": ">50\n>50\n>50"
  },
  {
    "heading": "Bounding Boxes",
    "content": "Yes\nYes\nYes"
  },
  {
    "heading": "Unknown",
    "content": "Environmental Diversity"
  },
  {
    "heading": "Unknown",
    "content": "Broad"
  },
  {
    "heading": "Unknown",
    "content": "Broad"
  },
  {
    "heading": "Unknown",
    "content": "Broad"
  },
  {
    "heading": "In underwater robotics, fish detection supports navigation,",
    "content": "obstacle avoidance, and targeted sampling, which are essential\nfor autonomous missions such as coral reef surveys, pollution\ntracking, and marine mapping [60], [61], [62], [63]. Recent\nworks have also emphasized aquaculture infrastructure inspec-\ntion and defect segmentation using semantic segmentation and\ndetection pipelines [64].\nIn conservation, detection models assist biodiversity tracking,\nmariculture zone mapping, and ecosystem health monitoring."
  },
  {
    "heading": "Integration with drones and satellite imaging enables detec-",
    "content": "tion of trends driven by environmental changes and human\nactivity [65], [66], [67]."
  },
  {
    "heading": "Despite progress, variability in species, morphology, and",
    "content": "water conditions continues to challenge robustness. Efforts\nin model compression, domain adaptation, and multi-sensor\nfusion are ongoing. Efficient models such as AASNet and\nquantized YOLO variants are promising for edge deployment\nin marine settings [68], [69]. These applications demonstrate the\ngrowing importance of fish detection in sustainable aquaculture,\nautonomous exploration, and ecological assessment [23], [54],\n[67], [70]."
  },
  {
    "heading": "Unknown",
    "content": "III. DATA RECORDS"
  },
  {
    "heading": "The FishDet-M benchmark comprises 105,556 images and",
    "content": "296,885 annotated fish instances, partitioned into training, vali-\ndation, and test sets using stratified sampling to ensure balanced\nrepresentation across habitat types, visibility conditions, and\nfish densities. Key statistics for each split are summarized in"
  },
  {
    "heading": "Unknown",
    "content": "Table IV."
  },
  {
    "heading": "FishDet-M exhibits extensive visual heterogeneity, with",
    "content": "image resolutions ranging from 78\u00d753 to 4608\u00d73456 pixels\n(mean: 715\u00d7468) and instance densities spanning from single\nfish to dense aggregations with up to 256 individuals per frame\n(mean: 2.81 instances per image).\nObject-level statistics are visualized in Fig. 2. The distribu-\ntion of bounding box areas in Fig. 2a shows a right-skewed\n4\nABUJABAL et al.: FISHDET-M: A UNIFIED LARGE-SCALE BENCHMARK FOR ROBUST FISH DETECTION\nFig. 2: Distribution of object-level statistics across FishDet-"
  },
  {
    "heading": "M. (a) Bounding box area. (b) Aspect ratio. (c) Number of",
    "content": "instances per image (log scale).\nFig. 3: Visual property distributions. (a) Image resolution. (b)\nRGB intensity histograms. (c) Object area vs. image resolution\n(log-log scale).\ncurve dominated by small and medium-sized objects. Aspect\nratios in Fig. 2b peak between 1 and 2. The histogram of\ninstances per image in Fig. 2c, plotted on a log scale, reveals\na long-tail pattern where most images contain few fish while\na smaller subset includes densely packed schools."
  },
  {
    "heading": "Unknown",
    "content": "Visual properties at the image level are summarized in"
  },
  {
    "heading": "Fig. 3. The image resolution histogram in Fig. 3a illustrates",
    "content": "the diversity in spatial coverage. RGB intensity histograms in"
  },
  {
    "heading": "Fig. 3b reveal that most scenes are low-light and dominated",
    "content": "by green-blue hues, typical of underwater capture [71]. The\nobject-to-image resolution heatmap in Fig. 3c, shown on a log-\nlog scale, emphasizes the prevalence of small objects across\nboth low- and high-resolution imagery."
  },
  {
    "heading": "Additional underwater-specific imaging characteristics are",
    "content": "highlighted in Fig. 4. The saturation histogram in Fig. 4a shows\na dominance of muted tones, consistent with light absorption\nand scattering . Contrast distribution in Fig. 4b, computed via\ngrayscale standard deviation, confirms low overall contrast."
  },
  {
    "heading": "The channel ratio plots in Fig. 4c illustrate strong blue and",
    "content": "green biases relative to red, confirming the spectral distortion\ntypical of aquatic environments [71]."
  },
  {
    "heading": "Additional spatial insights are provided in Fig. 5. The",
    "content": "histogram of mean image intensities in Fig. 5a serves as a\nproxy for visibility degradation due to haze and backscatter."
  },
  {
    "heading": "The bounding box center heatmap in Fig. 5b reveals a central",
    "content": "bias in object placement, reflecting typical framing tendencies\nin human-operated data acquisition."
  },
  {
    "heading": "To facilitate exploratory analysis and model transparency,",
    "content": "we developed an interactive desktop GUI as shown in Fig. 1.\nFig. 4: Underwater visual characteristics. (a) Saturation. (b)"
  },
  {
    "heading": "Unknown",
    "content": "Contrast (std. dev.). (c) Color channel ratios (Blue/Red,"
  },
  {
    "heading": "Green/Red).",
    "content": "Fig. 5: Scene-level statistics. (a) Image mean intensity (haze\nproxy). (b) Spatial distribution of bounding box centers."
  },
  {
    "heading": "The tool enables comparative evaluation of detection outputs",
    "content": "from YOLO-based and transformer-based models, with support\nfor adjusting confidence thresholds and overlaying predicted\nbounding boxes. Additional metadata such as mAP scores and\ninference times are presented in real time, assisting researchers\nin identifying model suitability for varied aquatic conditions."
  },
  {
    "heading": "Unknown",
    "content": "IV. METHODS"
  },
  {
    "heading": "Dataset Aggregation and Annotation. FishDet-M con-",
    "content": "solidates 13 underwater fish detection datasets, combining\npublicly available repositories and datasets. Data originate from\ncoral reefs, aquariums, coastal zones, and estuaries, comprising\nboth still images and video frames. All annotations were\nstandardized to the COCO format[72], with bounding boxes\nharmonized into a unified, species-agnostic fish category. This\nensured annotation integrity across varying formats and naming\nconventions. Invalid or ambiguous annotations were removed\nduring quality control."
  },
  {
    "heading": "Environmental and Visual Diversity. FishDet-M covers a",
    "content": "broad range of real-world conditions including clear and turbid\nwaters, shallow and deep scenes, artificial and natural lighting,\nmotion blur, and object occlusion. These variations simulate\npractical deployment scenarios and improve model robustness.\nThe dataset also includes terrestrial and laboratory views for\nextended diversity and evaluation of domain transfer."
  },
  {
    "heading": "Harmonization and Quality Assurance. Dataset integration",
    "content": "involved coordinate format unification, validation of box\ndimensions, and manual correction of missing or erroneous\nannotations. All bounding boxes follow the COCO format\n5\nABUJABAL et al.: FISHDET-M: A UNIFIED LARGE-SCALE BENCHMARK FOR ROBUST FISH DETECTION\nFig. 6: Comparative positioning of major fish detection datasets\nacross four key axes: (a) task complexity vs. modality rich-\nness, (b) data volume vs. annotation quality, (c) ecological\ndiversity vs. visual challenge, and (d) deployment readiness vs.\nreproducibility. Bubble size indicates dataset scale (number of\nannotated fish instances).\nxmin, ymin, width, height [72]. Rigorous validation steps ensured\ndataset consistency and excluded corrupted samples."
  },
  {
    "heading": "Partitioning Strategy. A source-aware stratified split main-",
    "content": "tained proportional representation of each dataset within\ntraining (80%), validation (10%), and test (10%) sets. This\npreserved the ecological and visual diversity of smaller datasets\nand prevented bias from large contributors like FishNet[73]."
  },
  {
    "heading": "Dataset Selection Criteria. Selection was guided by eight",
    "content": "criteria: task complexity, modality richness, volume, annotation\nquality, ecological diversity, visual challenge and deployment\nreadiness. Representative datasets included DeepFish [28],"
  },
  {
    "heading": "FishNet [73], Brackish-MOT [32], and TrashCan 1.0 [33].",
    "content": "The complete list of source datasets is listed in table V. Non-\nrepresentative or duplicate frames were removed to focus on\nannotated fish instances and prevent data leaks. To visualize\nrepresentativeness, Fig. 6 presents a four-panel bubble chart\ncontrasting key dimensions (e.g., task complexity vs. modality\nrichness), with FishDet-M consistently positioned in the upper-\nright quadrant, highlighting its diversity, annotation volume,\nand readiness for real-world deployment."
  },
  {
    "heading": "CLIP-Based Model Selection. We integrated a CLIP-based",
    "content": "module to automatically select the most appropriate detection\nmodel from 28 candidates using semantic similarity between\nimage embeddings and prompt texts as shown in Fig. 7. This\nmechanism enables context-aware inference without manual\nselection or heuristics."
  },
  {
    "heading": "Unknown",
    "content": "Technical Validation. We benchmarked 28 models across"
  },
  {
    "heading": "Unknown",
    "content": "YOLOv8\u2013v12 [18], [74], [75], [76], YOLO-NAS[77], Cascade"
  },
  {
    "heading": "R-CNN[19], Sparse R-CNN[78], DETR variants[20], [79],",
    "content": "[80], RetinaNet[81], and MobileNetv2-SSD[82]. Models were\ntrained on FishDet-M using defualt hyperparameters. Eval-\nuation used mAP, mAP50, mAP75, APS, APM, and APL,\nprecision-recall curves, and F1 scores. Efficiency was assessed\nvia FPS, model size, and latency."
  },
  {
    "heading": "Hardware and Software. All experiments were run on",
    "content": "an RTX 4090 GPU with an Intel i9-14900K CPU and 64GB"
  },
  {
    "heading": "Unknown",
    "content": "RAM under Windows 11. Codebases included PyTorch [83],"
  },
  {
    "heading": "Fig. 7: CLIP-based model selection pipeline. The input image",
    "content": "is embedded via the CLIP image encoder, and its similarity\nis computed with a set of model-specific textual prompts\nembedded using the CLIP text encoder. The model with\nthe highest alignment score is automatically selected for\ndownstream detection."
  },
  {
    "heading": "Ultralytics [84] and MMDetection[85]. All detection metrics",
    "content": "were computed using pycocotools[86] with mixed-precision\nevaluation."
  },
  {
    "heading": "Unknown",
    "content": "V. RESULTS AND DISCUSSION"
  },
  {
    "heading": "This section provides a comprehensive evaluation of object",
    "content": "detection models for underwater fish detection, presenting both\nquantitative performance metrics and qualitative analyses of\nmodel behavior. We assess model performance on the aggregate\nFishDet-M benchmark, examine their efficiency characteristics,\nand investigate their generalization capabilities across various\nsub-test splits, reflecting diverse underwater conditions."
  },
  {
    "heading": "Unknown",
    "content": "A. Quantitative Evaluation"
  },
  {
    "heading": "Unknown",
    "content": "We evaluated 28 object detection models on the FishDet-"
  },
  {
    "heading": "M benchmark to analyze performance and computational",
    "content": "efficiency across a range of architectures. Table VI presents\naccuracy, scale-specific detection, and inference metrics."
  },
  {
    "heading": "The YOLO family consistently achieved the highest de-",
    "content": "tection accuracy. YOLO12x reached 0.491 mAP, followed\nby YOLO12l (0.487) and YOLO11l (0.484). These models\nperformed strongly across small, medium, and large object\nscales (APS, APM, APL), confirming their adaptability to\nvarying fish sizes. YOLO-NAS-l also provided strong results\n(0.470 mAP) with efficient inference."
  },
  {
    "heading": "Lighter models such as YOLOv8n and YOLOv10n offered",
    "content": "excellent speed-accuracy balance. YOLOv8n achieved 251 FPS\nusing only 3.2 million parameters, making it ideal for real-time\nand resource-constrained scenarios."
  },
  {
    "heading": "Unknown",
    "content": "In contrast, transformer-based models including DETR,"
  },
  {
    "heading": "RT-DETR-l, and Deformable-DETR showed lower mAP",
    "content": "(0.317\u20130.390), struggled with small object detection, and\nexhibited higher latency (15 to 45 milliseconds), making them\nless suitable for real-time deployment in aquatic environments."
  },
  {
    "heading": "Region proposal models such as Cascade R-CNN (0.449",
    "content": "mAP) offered solid accuracy but lower speed. Single-stage\nalternatives like RetinaNet (0.448 mAP) and FCOS (0.475\nmAP) provided better efficiency at 30\u201338 FPS.\n1) Precision-Recall Analysis: The Precision-Recall curves in"
  },
  {
    "heading": "Fig. 8a show most models begin with high precision and drop",
    "content": "as recall increases. MobileNetV2-SSD declines steeply at recall\n6\nABUJABAL et al.: FISHDET-M: A UNIFIED LARGE-SCALE BENCHMARK FOR ROBUST FISH DETECTION\nTABLE V: Summary of existing datasets used as image sources for constructing our comprehensive FishDet-M dataset, detailing\ntheir types, sizes, and descriptions"
  },
  {
    "heading": "Unknown",
    "content": "Dataset"
  },
  {
    "heading": "Unknown",
    "content": "Type"
  },
  {
    "heading": "Unknown",
    "content": "Size"
  },
  {
    "heading": "Unknown",
    "content": "Description"
  },
  {
    "heading": "Brackish-MOT [32]",
    "content": "MOT\n89 videos\nHazy underwater videos with multi-object fish tracking bounding boxes"
  },
  {
    "heading": "Unknown",
    "content": "UIIS [34]"
  },
  {
    "heading": "Instance Seg.",
    "content": "4628 images\nGeneral-purpose underwater instance segmentation across categories"
  },
  {
    "heading": "Unknown",
    "content": "TrashCan 1.0 [33]"
  },
  {
    "heading": "Object Detection",
    "content": "7212 images"
  },
  {
    "heading": "Unknown",
    "content": "Debris detection dataset including fish annotations"
  },
  {
    "heading": "Unknown",
    "content": "Fish4Knowledge-2023 [87]"
  },
  {
    "heading": "Fish Detection",
    "content": "1897 images"
  },
  {
    "heading": "Unknown",
    "content": "Bounding-box labeled images of fish from underwater videos"
  },
  {
    "heading": "Unknown",
    "content": "Fish Dataset [16]"
  },
  {
    "heading": "Detection, Seg.",
    "content": "1850 images\nAquarium images of Crucian carp with detection and segmentation masks"
  },
  {
    "heading": "Unknown",
    "content": "FishNet [73]"
  },
  {
    "heading": "Cls., Det",
    "content": "94,532 images\nLarge-scale dataset for multi-species classification with bounding boxes"
  },
  {
    "heading": "Unknown",
    "content": "Fish Video [88]"
  },
  {
    "heading": "Fish Detection",
    "content": "322 images"
  },
  {
    "heading": "Unknown",
    "content": "Video-derived frames with annotated fish instances"
  },
  {
    "heading": "Unknown",
    "content": "FISH-video [89]"
  },
  {
    "heading": "Fish Detection",
    "content": "506 images"
  },
  {
    "heading": "Unknown",
    "content": "Fish tank video frames with annotated fish instances"
  },
  {
    "heading": "Unknown",
    "content": "Med. Fish [90]"
  },
  {
    "heading": "Fish Detection",
    "content": "1247 images"
  },
  {
    "heading": "Unknown",
    "content": "Labeled fish images from Mediterranean Sea species"
  },
  {
    "heading": "Unknown",
    "content": "DeepFish [28]"
  },
  {
    "heading": "Fish Det., Seg.",
    "content": "3200 / 620"
  },
  {
    "heading": "Unknown",
    "content": "Images with labeled fish instances in tropical Australia"
  },
  {
    "heading": "Unknown",
    "content": "Aquarium [91]"
  },
  {
    "heading": "Fish Detection",
    "content": "638 images"
  },
  {
    "heading": "Unknown",
    "content": "Aquarium animal dataset with fish-focused bounding boxes"
  },
  {
    "heading": "Unknown",
    "content": "AAS [92]"
  },
  {
    "heading": "Fish Detection",
    "content": "4239 images"
  },
  {
    "heading": "Unknown",
    "content": "Multi-class marine dataset with 72% fish representation"
  },
  {
    "heading": "Unknown",
    "content": "FishExtend [93]"
  },
  {
    "heading": "Fish Detection",
    "content": "3122 images"
  },
  {
    "heading": "Unknown",
    "content": "Diverse fish images under various environmental conditions"
  },
  {
    "heading": "Unknown",
    "content": "FishDet-M (Ours)"
  },
  {
    "heading": "Fish Detection",
    "content": "105,556 images\nUnified benchmark combining 13 datasets for large-scale, diverse fish detection\nTABLE VI: Detection performance summary of 28 deep learning object detection models, including YOLO, R-CNN, and\nTransformer-based architectures, evaluated on the FishDet-M test set. Metrics reported include mAP@50, mAP@50:95, and\ninference speed (ms), Training is done using Ultralytics [84], mmdetection [85] and Supergradients [77]"
  },
  {
    "heading": "Model",
    "content": "mAP\nmAP50\nmAP75\nAPS\nAPM\nAPL"
  },
  {
    "heading": "Unknown",
    "content": "Params (M)"
  },
  {
    "heading": "Inf. Speed (ms)",
    "content": "FPS"
  },
  {
    "heading": "YOLOv8n [18]",
    "content": "0.433\n0.776\n0.445\n0.251\n0.409\n0.602\n3.2\n3.97\n251.78"
  },
  {
    "heading": "YOLOv8s [18]",
    "content": "0.455\n0.808\n0.469\n0.274\n0.434\n0.618\n11.2\n4.02\n248.85"
  },
  {
    "heading": "YOLOv8m [18]",
    "content": "0.471\n0.828\n0.490\n0.286\n0.456\n0.632\n25.9\n5.10\n196.01"
  },
  {
    "heading": "YOLOv8l [18]",
    "content": "0.478\n0.834\n0.505\n0.294\n0.466\n0.638\n43.7\n6.12\n163.50"
  },
  {
    "heading": "YOLOv8x [18]",
    "content": "0.481\n0.837\n0.505\n0.297\n0.472\n0.638\n68.2\n7.07\n141.52"
  },
  {
    "heading": "YOLOv10n [74]",
    "content": "0.442\n0.784\n0.460\n0.261\n0.413\n0.610\n2.3\n4.98\n200.77"
  },
  {
    "heading": "YOLOv10s [74]",
    "content": "0.461\n0.812\n0.483\n0.277\n0.439\n0.626\n7.2\n5.03\n198.91"
  },
  {
    "heading": "YOLOv10m [74]",
    "content": "0.476\n0.833\n0.501\n0.295\n0.461\n0.633\n15.4\n6.65\n150.41"
  },
  {
    "heading": "YOLO11n [75]",
    "content": "0.450\n0.796\n0.465\n0.262\n0.426\n0.620\n2.6\n5.24\n190.68"
  },
  {
    "heading": "YOLO11s [75]",
    "content": "0.471\n0.824\n0.491\n0.283\n0.460\n0.629\n9.4\n5.27\n189.71"
  },
  {
    "heading": "YOLO11m [75]",
    "content": "0.480\n0.838\n0.502\n0.297\n0.468\n0.635\n20.1\n6.52\n153.45"
  },
  {
    "heading": "YOLO11l [75]",
    "content": "0.484\n0.842\n0.506\n0.302\n0.470\n0.639\n25.3\n9.72\n102.90"
  },
  {
    "heading": "YOLO11x [75]",
    "content": "0.483\n0.840\n0.509\n0.300\n0.472\n0.639\n56.9\n9.76\n102.40"
  },
  {
    "heading": "YOLO12n [76]",
    "content": "0.443\n0.789\n0.456\n0.255\n0.415\n0.616\n2.6\n7.56\n132.20"
  },
  {
    "heading": "YOLO12s [76]",
    "content": "0.470\n0.823\n0.490\n0.281\n0.451\n0.634\n9.3\n7.76\n128.89"
  },
  {
    "heading": "YOLO12m [76]",
    "content": "0.483\n0.841\n0.507\n0.299\n0.470\n0.642\n20.2\n8.12\n123.17"
  },
  {
    "heading": "YOLO12l [76]",
    "content": "0.487\n0.847\n0.516\n0.306\n0.483\n0.639\n26.4\n13.66\n73.20"
  },
  {
    "heading": "YOLO12x [76]",
    "content": "0.491\n0.848\n0.521\n0.315\n0.485\n0.641\n59.1\n13.41\n74.59"
  },
  {
    "heading": "YOLO-NAS-l [77]",
    "content": "0.470\n0.805\n0.488\n0.285\n0.453\n0.620\n66.9\n16.01\n62.5"
  },
  {
    "heading": "Faster R-CNN [30]",
    "content": "0.379\n0.690\n0.374\n0.192\n0.348\n0.538\n41.0\n34.8\n28.7"
  },
  {
    "heading": "Cascade R-CNN [19]",
    "content": "0.449\n0.758\n0.470\n0.260\n0.422\n0.607\n69.1\n40.5\n24.7"
  },
  {
    "heading": "Sparse R-CNN [78]",
    "content": "0.357\n0.615\n0.375\n0.127\n0.305\n0.565\n62\n50.4\n19.9"
  },
  {
    "heading": "DETR [20]",
    "content": "0.390\n0.708\n0.390\n0.154\n0.335\n0.600\n41.0\n36.0\n27.8"
  },
  {
    "heading": "Deformable-DETR [80]",
    "content": "0.317\n0.648\n0.269\n0.12\n0.24\n0.507\n34.0\n45\n22.1"
  },
  {
    "heading": "RT-DETR-l [79]",
    "content": "0.381\n0.696\n0.383\n0.156\n0.354\n0.588\n32\n15.1\n66"
  },
  {
    "heading": "RetinaNet [81]",
    "content": "0.448\n0.764\n0.465\n0.238\n0.416\n0.627\n34.0\n26.6\n37.6"
  },
  {
    "heading": "MobileNetv2-SSD [82]",
    "content": "0.288\n0.519\n0.294\n0.032\n0.185\n0.554\n3\n40\n25"
  },
  {
    "heading": "FCOS-r50-FPN [94]",
    "content": "0.475\n0.838\n0.472\n0.203\n0.383\n0.599\n32.1\n29.7\n33.7"
  },
  {
    "heading": "FishDet-M-CLIP",
    "content": "0.444\n0.742\n0.472\n0.235\n0.415\n0.620\n\u2013\n12.46\n80.28\n0.4, and other models such as Sparse R-CNN and Deformable-"
  },
  {
    "heading": "DETR also exhibit earlier degradation. At IoU=0.75 Fig. 8b,",
    "content": "the precision starts lower and declines earlier, particularly for\ntransformer models.\n2) F1 Score: F1 score trends for IoU thresholds of 0.50\nand 0.75 are shown in Fig. 8c and 8d . YOLO variants peaked\nearly and declined smoothly, while Sparse R-CNN and DETR\npeaked later. Deformable-DETR displayed a flatter curve with\nearly decline. At stricter IoU thresholds, models showed more\nuniform patterns, with FCOS and MobileNetV2-SSD dropping\nsharply beyond 0.6 confidence.\n3) Evaluation on Sub-Test Splits: To better understand\nthe contribution of each constituent dataset, we conducted\na detailed evaluation of model performance on the individual\ntest partitions of the thirteen source datasets. Results indicate\nnotable variability depending on dataset complexity. For\nexample, YOLO12x achieved an mAP of 0.846 on DeepFish\n[28], which features underwater scenes with large prominent\nfish instances. In contrast, the same model scored 0.371 on"
  },
  {
    "heading": "FishDataset [16], where overlapping and occluded, similar-",
    "content": "looking fish in a tank present significant visual challenges."
  },
  {
    "heading": "Across the full benchmark, FishDet-M produced consistent",
    "content": "7\nABUJABAL et al.: FISHDET-M: A UNIFIED LARGE-SCALE BENCHMARK FOR ROBUST FISH DETECTION"
  },
  {
    "heading": "Fig. 8: F1 score and Precision-Recall curves for 28 object",
    "content": "detection models at IoU thresholds of 0.50 and 0.75. Subfigures\n(a) and (b) show F1 score trends, and (c) and (d) depict\nprecision-recall curves.\nperformance in the range of 0.481 to 0.491 for the highest-\nperforming models (see Table VI), demonstrating generalization\nacross diverse visual domains."
  },
  {
    "heading": "The individual datasets can be broadly grouped into three",
    "content": "performance tiers based on their environmental characteristics."
  },
  {
    "heading": "Datasets in the high-performance tier, such as DeepFish",
    "content": "[28] and FishVideo [89], typically include large fish with\nminimal background interference and stable lighting condi-\ntions. The moderate tier includes datasets like FishNet [73]\nand Fish4Knowledge [87], which involve variable fish sizes,\nmixed lighting conditions, and moderate occlusion. The most\nchallenging tier comprises datasets such as TrashCan 1.0 [33]\nand Brackish-MOT [32], where visual noise, low image quality,\nhigh turbidity, and frequent occlusion significantly hinder\ndetection accuracy. Detailed metrics for all evaluated models on\neach dataset are provided in the supplementary material (Table\nSI through SXIII ). These results confirm that while FishDet-M\ndoes not achieve the highest accuracy on any single dataset,\nit consistently promotes generalizable feature learning across\ndomains. This is further evidenced by the narrow variance in\nmodel performance across subsets. An average precision value\nof \u20131 indicates that no qualifying objects were present in the\ncorresponding sub-test split based on size thresholds."
  },
  {
    "heading": "B. Qualitative and Generalization Analysis",
    "content": "In addition to quantitative metrics, visual inspection and cross\ndomain testing provide further insight into model robustness\nunder diverse underwater conditions. Fig. 9 presents outputs"
  },
  {
    "heading": "TABLE VII: Models Performance Comparison on unseen test",
    "content": "data from a different source with sample size of 1500 [95]"
  },
  {
    "heading": "Model",
    "content": "mAP\nmAP50\nmAP75\nAPS\nAPM\nAPL"
  },
  {
    "heading": "YOLOv8n [18]",
    "content": "0.542\n0.826\n0.607\n0.252\n0.460\n0.630"
  },
  {
    "heading": "YOLOv8s [18]",
    "content": "0.575\n0.853\n0.646\n0.295\n0.492\n0.661"
  },
  {
    "heading": "YOLOv8m [18]",
    "content": "0.596\n0.867\n0.673\n0.286\n0.511\n0.684"
  },
  {
    "heading": "YOLOv8l [18]",
    "content": "0.615\n0.879\n0.699\n0.322\n0.523\n0.705"
  },
  {
    "heading": "YOLOv8x [18]",
    "content": "0.611\n0.879\n0.689\n0.303\n0.516\n0.704"
  },
  {
    "heading": "YOLOv10n [74]",
    "content": "0.579\n0.842\n0.660\n0.242\n0.486\n0.675"
  },
  {
    "heading": "YOLOv10s [74]",
    "content": "0.592\n0.859\n0.671\n0.252\n0.496\n0.688"
  },
  {
    "heading": "YOLOv10m [74]",
    "content": "0.622\n0.877\n0.704\n0.286\n0.519\n0.718"
  },
  {
    "heading": "YOLO11n [75]",
    "content": "0.584\n0.842\n0.662\n0.228\n0.489\n0.686"
  },
  {
    "heading": "YOLO11s [75]",
    "content": "0.607\n0.863\n0.685\n0.266\n0.515\n0.704"
  },
  {
    "heading": "YOLO11m [75]",
    "content": "0.609\n0.866\n0.688\n0.272\n0.511\n0.705"
  },
  {
    "heading": "YOLO11l [75]",
    "content": "0.622\n0.875\n0.706\n0.287\n0.521\n0.717"
  },
  {
    "heading": "YOLO11x [75]",
    "content": "0.629\n0.882\n0.715\n0.302\n0.534\n0.720"
  },
  {
    "heading": "YOLO12n [76]",
    "content": "0.586\n0.865\n0.660\n0.255\n0.497\n0.680"
  },
  {
    "heading": "YOLO12s [76]",
    "content": "0.626\n0.887\n0.713\n0.290\n0.535\n0.719"
  },
  {
    "heading": "YOLO12m [76]",
    "content": "0.625\n0.886\n0.713\n0.285\n0.532\n0.721"
  },
  {
    "heading": "YOLO12l [76]",
    "content": "0.629\n0.876\n0.715\n0.298\n0.534\n0.723"
  },
  {
    "heading": "YOLO12x [76]",
    "content": "0.625\n0.874\n0.706\n0.303\n0.525\n0.721"
  },
  {
    "heading": "YOLO-NAS-l [77]",
    "content": "0.602\n0.840\n0.683\n0.283\n0.511\n0.695"
  },
  {
    "heading": "Faster R-CNN [30]",
    "content": "0.452\n0.729\n0.498\n0.156\n0.337\n0.552"
  },
  {
    "heading": "Cascade R-CNN [19]",
    "content": "0.566\n0.817\n0.644\n0.219\n0.457\n0.669"
  },
  {
    "heading": "Sparse R-CNN R50 FPN [78]",
    "content": "0.486\n0.736\n0.554\n0.116\n0.336\n0.620"
  },
  {
    "heading": "DETR [20]",
    "content": "0.540\n0.827\n0.607\n0.143\n0.388\n0.677"
  },
  {
    "heading": "Deformable-DETR [80]",
    "content": "0.297\n0.709\n0.074\n0.048\n0.221\n0.372"
  },
  {
    "heading": "RT-DETR-l [79]",
    "content": "0.501\n0.759\n0.556\n0.136\n0.366\n0.644"
  },
  {
    "heading": "RetinaNet [81]",
    "content": "0.578\n0.842\n0.654\n0.236\n0.475\n0.676"
  },
  {
    "heading": "MobileNetV2-SSD [82]",
    "content": "0.414\n0.656\n0.453\n0.006\n0.188\n0.601"
  },
  {
    "heading": "FCOS [94]",
    "content": "0.583\n0.852\n0.651\n0.271\n0.484\n0.678\nfrom 28 models across four representative images, selected to\nhighlight common failure scenarios."
  },
  {
    "heading": "In camouflage conditions, where fish resemble the back-",
    "content": "ground, most models produce inaccurate or oversized bounding\nboxes due to low contrast. Occlusion by coral or overlapping\nstructures leads to missed or fragmented detections, especially\nwhen background textures are similar to fish bodies. Small\nobject detection remains particularly challenging, with limited\npixel footprint resulting in frequent false negatives. In dense\nscenes with multiple fish and reduced visibility, models often\nmerge instances or fail to localize targets precisely, revealing\nlimitations in crowded environments.\nTo evaluate generalization, we tested all models on a separate\ndataset of 1500 unseen images from a different source [95]."
  },
  {
    "heading": "As shown in Table VII, top models from the YOLO family,",
    "content": "including YOLO12l and YOLO11x, maintained high accuracy\n(mAP approximately 0.63) across object scales. Lightweight\nvariants such as YOLOv8n and YOLOv10n also performed\nreliably. In contrast, transformer based detectors such as"
  },
  {
    "heading": "Deformable DETR showed a noticeable drop in accuracy,",
    "content": "particularly for small objects, while region proposal models\nlike Faster R-CNN showed moderate transferability. Single\nstage detectors such as RetinaNet and FCOS proved more\nstable under domain shift."
  },
  {
    "heading": "Together, these visual and cross dataset evaluations confirm",
    "content": "the adaptability of the models trained on FishDet-M and\nhighlight persistent limitations including camouflage, occlusion,\nsmall object detection, and dense scene separation. These\nremain open challenges for reliable fish detection in natural\nunderwater environments."
  },
  {
    "heading": "Unknown",
    "content": "C. CLIP-Based Model Selection Analysis"
  },
  {
    "heading": "We analyzed the behavior of the CLIP-guided model selector",
    "content": "to understand its preferences across the FishDet-M dataset. As\nshown in Fig. 10, YOLOv8x was the most frequently selected\nmodel, followed by YOLOv8l, YOLOv12m, and YOLOv11n."
  },
  {
    "heading": "These models likely dominated due to stronger feature repre-",
    "content": "sentation and better alignment with CLIP\u2019s semantic cues. In\n8\nABUJABAL et al.: FISHDET-M: A UNIFIED LARGE-SCALE BENCHMARK FOR ROBUST FISH DETECTION\nGT\nY8n\nY8s\nY8m\nY8l\nY8x"
  },
  {
    "heading": "Unknown",
    "content": "Y10n"
  },
  {
    "heading": "Unknown",
    "content": "Y10s"
  },
  {
    "heading": "Unknown",
    "content": "Y10m"
  },
  {
    "heading": "Unknown",
    "content": "Y11n"
  },
  {
    "heading": "Unknown",
    "content": "Y11s"
  },
  {
    "heading": "Unknown",
    "content": "Y11m"
  },
  {
    "heading": "Unknown",
    "content": "Y11l"
  },
  {
    "heading": "Unknown",
    "content": "Y11x"
  },
  {
    "heading": "Y12n",
    "content": "GT"
  },
  {
    "heading": "Unknown",
    "content": "Y12s"
  },
  {
    "heading": "Unknown",
    "content": "Y12m"
  },
  {
    "heading": "Unknown",
    "content": "Y12l"
  },
  {
    "heading": "Unknown",
    "content": "Y12x"
  },
  {
    "heading": "Unknown",
    "content": "YNAS"
  },
  {
    "heading": "Unknown",
    "content": "DETR"
  },
  {
    "heading": "DefD",
    "content": "RTD"
  },
  {
    "heading": "Unknown",
    "content": "FRCNN"
  },
  {
    "heading": "Unknown",
    "content": "CRCNN"
  },
  {
    "heading": "Unknown",
    "content": "SRCNN"
  },
  {
    "heading": "Unknown",
    "content": "RNet"
  },
  {
    "heading": "Unknown",
    "content": "FCOS"
  },
  {
    "heading": "MobSSD",
    "content": "Fig. 9: Object detection results from 28 models across four challenging underwater images. Each row shows the original image\nfollowed by predictions from the models in the following order: GT (Ground Truth), Y8, Y10, Y11, Y12 (YOLOv8\u2013v12\nvariants), YNAS (YOLO-NAS-L), DETR, DefD (Deformable DETR), RTD (RT-DETR), R-CNN (Faster, Cascade, and Sparse\nR-CNN), RNet (RetinaNet), FCOS, and MobSSD (MobileNetV2-SSD). The images represent varying detection difficulty: Image\n1 shows a camouflaged fish with background-matching color and texture; Image 2 contains a partially occluded fish among\nvisually similar corals; Image 3 includes two small fish with limited pixel footprint; and Image 4 depicts a dense school of fish\nwith significant inter-object occlusion and low contrast against the background\ncontrast, older or lighter models (e.g., YOLOv10m, YOLOv8m)\nwere selected less often."
  },
  {
    "heading": "To further examine this trend, we visualized the distribution",
    "content": "of CLIP similarity scores for each model in Fig. 11. YOLOv8x,"
  },
  {
    "heading": "YOLOv8l, and YOLOv12m consistently achieved higher",
    "content": "median scores with compact interquartile ranges, indicating\nreliable semantic alignment. Models with broader or lower\nscoring distributions were less favored, reflecting CLIP\u2019s\ntendency to prioritize stable semantic proximity."
  },
  {
    "heading": "These findings confirm that the CLIP selector does not act",
    "content": "randomly but favors models with consistent visual-language\ncorrespondence. This supports the feasibility of language-\ndriven model routing for adapting detection pipelines to diverse\nunderwater conditions, as demonstrated in recent work on bias-\naware underwater AI using CLIP similarity [96]. The specific\nprompt used for CLIP-guided selection is provided in the\nsupplementary material for full reproducibility."
  },
  {
    "heading": "Unknown",
    "content": "D. Discussion and Insights"
  },
  {
    "heading": "The evaluation on FishDet-M highlights consistent perfor-",
    "content": "mance advantages of the YOLO family, particularly YOLO12x,\nwhich offered strong accuracy across object scales and efficient\ninference suitable for real-time marine applications [76],\nFig. 10: Frequency of model selections based on CLIP similarity\nacross the full FishDet-M dataset.\n[75], [51]. Lightweight YOLO variants also performed well,\nbalancing speed and accuracy effectively."
  },
  {
    "heading": "Unknown",
    "content": "Transformer based detectors such as DETR, Deformable"
  },
  {
    "heading": "DETR, and RT DETR-l showed limited performance, especially",
    "content": "on small objects, and incurred higher latency [20], [80], [79]."
  },
  {
    "heading": "Region based models like Cascade R-CNN and single stage",
    "content": "detectors such as RetinaNet and FCOS [19], [81], [94] achieved\nmoderate accuracy with better efficiency but did not match\n9\nABUJABAL et al.: FISHDET-M: A UNIFIED LARGE-SCALE BENCHMARK FOR ROBUST FISH DETECTION"
  },
  {
    "heading": "Fig. 11: Distribution of CLIP similarity scores across all",
    "content": "candidate models. Each box summarizes the semantic alignment\nover the entire dataset."
  },
  {
    "heading": "Unknown",
    "content": "YOLO\u2019s top results ."
  },
  {
    "heading": "F1 score and precision recall analyses revealed that models",
    "content": "behaved similarly at lower IoU thresholds but diverged under\nstricter matching (IoU 0.75), where lightweight and transformer\nmodels were more sensitive to localization demands. This\nunderscores the need to evaluate models across varying\nthresholds to assess reliability."
  },
  {
    "heading": "Evaluation across individual datasets showed wide perfor-",
    "content": "mance variation from 0.359 mAP on FishDataset [16] to 0.846\non DeepFish [28]. This reflects the diversity of underwater\nconditions. The merged FishDet-M benchmark provided a\nbalanced training distribution, enabling generalizable models\nthat avoid overfitting to specific domains."
  },
  {
    "heading": "Generalization tests on an unseen dataset confirmed this",
    "content": "robustness. Many YOLO variants performed comparably or\nbetter on new data, reinforcing the value of diverse, consoli-\ndated training sets for deployment in real-world underwater\napplications [95]."
  },
  {
    "heading": "Unknown",
    "content": "We also evaluated an adaptive model selection system,"
  },
  {
    "heading": "FishDet-M-CLIP, which used CLIP similarity to select contex-",
    "content": "tually appropriate models. Although its accuracy (mAP 0.444)\nand speed (80 FPS) did not surpass the top YOLO models, it\noutperformed many traditional and transformer based models,\nvalidating the feasibility of language guided model routing for\nvariable conditions."
  },
  {
    "heading": "Qualitative results revealed some failure cases including",
    "content": "inaccurate localization in camouflage scenes, missed detections\nunder occlusion, and poor handling of small or blurred objects."
  },
  {
    "heading": "False positives around coral and complex structures also",
    "content": "persisted, emphasizing ongoing challenges in fine grained\ndiscrimination and context-aware detection under underwater\nconditions."
  },
  {
    "heading": "Unknown",
    "content": "E. Dataset and Ethics"
  },
  {
    "heading": "FishDet-M is a large-scale benchmark for fish detection",
    "content": "across diverse underwater settings [18]. It merges 13 publicly\navailable and licensed datasets covering marine, brackish, and\naquarium environments into a fish detection dataset. This\nintegration addresses prior inconsistencies in dataset structure\nand evaluation [9]."
  },
  {
    "heading": "The dataset includes 105,556 images and 296,885 fish",
    "content": "annotations, with stratified splits for training, validation, and\ntesting that maintain variation in habitat, clarity, and density."
  },
  {
    "heading": "Image sizes range from 78 \u00d7 53 to 4608 \u00d7 3456 pixels,",
    "content": "averaging 711\u00d7465, with fish counts from 1 to 256 per image.\nAnnotations are labeled under a species-agnostic Fish category\nto prioritize general detection while allowing downstream\nclassification. A thorough quality assurance process ensured\nformat consistency, label standardization, and removal of\ncorrupted or ambiguous data."
  },
  {
    "heading": "All data originate from ethically sourced repositories [28],",
    "content": "[92], [73], [93], [88]. The release aims to advance reproducible\nresearch in marine science, aquaculture, and conservation [10],\n[97]."
  },
  {
    "heading": "Unknown",
    "content": "VI. LIMITATIONS AND FUTURE DIRECTIONS"
  },
  {
    "heading": "Despite its comprehensiveness, FishDet-M retains several",
    "content": "limitations. Real-world underwater environments remain highly\nvariable, with extreme turbidity, dynamic lighting, and cluttered\nbackgrounds posing significant challenges for current detection\nmodels [1], [32]. Future datasets may incorporate synthetic\naugmentation or multisensor data to improve robustness under\nsuch conditions."
  },
  {
    "heading": "At present, FishDet-M focuses exclusively on object detec-",
    "content": "tion and localization. However, downstream applications such\nas behavioral analysis and species-specific tracking require\nricher annotations, including pose, individual identity, and\nactivity labels [98]. Expanding the annotation scope would\nenable deeper biological insights."
  },
  {
    "heading": "While YOLO-based models outperform others in handling",
    "content": "small or occluded fish, limitations remain in complex scenes [4].\nMulti-scale feature fusion and context-aware detection architec-\ntures could help mitigate these issues and enhance robustness\nunder occlusion and scale variation."
  },
  {
    "heading": "Generalization analysis further underscores the need to",
    "content": "diversify training data. Although models trained on FishDet-M\nperform well on external benchmarks, broader inclusion of\ngeographic regions, water types, and depths would enhance\nglobal applicability."
  },
  {
    "heading": "In terms of adaptive inference, our CLIP-guided model",
    "content": "selector shows promise by dynamically routing inputs to suit-\nable detectors. However, this strategy introduces computational\noverhead and marginally reduces accuracy compared to the\ntop fixed models. Exploring optimized prompts, confidence-\nweighted voting, or ensemble strategies may offer better\nspeed\u2013accuracy tradeoffs for real-time systems."
  },
  {
    "heading": "Explainability remains an open frontier. Integrating XAI",
    "content": "techniques could elucidate model decisions and failure modes\nin complex underwater environments. This is critical for high-\nstakes applications in marine conservation and aquaculture,\nwhere trust and interpretability are essential."
  },
  {
    "heading": "As reported in Table VII, top-performing detectors such",
    "content": "asvYOLOv12x achieve mAP values exceeding 0.62 on unseen\n10\nABUJABAL et al.: FISHDET-M: A UNIFIED LARGE-SCALE BENCHMARK FOR ROBUST FISH DETECTION\ndata, outperforming their scores on the FishDet-M test set."
  },
  {
    "heading": "This suggests a strong generalization capacity resulting from",
    "content": "training on an aggregated dataset such as FishDet-M. This\ntrend extends beyond YOLO, as other detectors such as Faster"
  },
  {
    "heading": "R-CNN (0.452 mAP), RetinaNet (0.578 mAP), and DETR",
    "content": "(0.540 mAP) also exhibit higher scores."
  },
  {
    "heading": "FishDet-M provides a strong foundation for advancing fish",
    "content": "detection research. Continued progress will depend on extend-\ning annotation depth, improving cross-domain generalization,\nand refining adaptive and interpretable inference strategies for\ndynamic underwater settings."
  },
  {
    "heading": "Unknown",
    "content": "VII. CONCLUSION"
  },
  {
    "heading": "FishDet-M is a unified benchmark dataset developed to",
    "content": "address persistent challenges in underwater fish detection."
  },
  {
    "heading": "By merging 13 heterogeneous sources into a standardized",
    "content": "repository, it resolves issues related to fragmentation and\ninconsistent evaluation protocols. We benchmarked 28 object\ndetection models, including YOLOv8 through YOLOv12,"
  },
  {
    "heading": "Transformer based architectures, and R-CNN variants. The",
    "content": "results indicate the consistent superiority of YOLO models,\nparticularly YOLO12x and YOLO11m, in balancing detection\naccuracy, robustness across object scales, and inference effi-\nciency. Transformer models exhibited reduced performance\nwhen detecting small or occluded fish and imposed greater\ncomputational demands. Evaluation across diverse aquatic\nconditions revealed substantial variation in detection difficulty."
  },
  {
    "heading": "Unknown",
    "content": "Generalization tests demonstrated that models trained on"
  },
  {
    "heading": "FishDet-M maintain reliable performance on previously unseen",
    "content": "data, underscoring the dataset\u2019s utility in supporting adaptable\nand field deployable detection systems. By releasing FishDet-M\nalongside its annotation scripts and trained models, we aim\nto foster reproducible research and accelerate innovation in\nmarine science, ecological monitoring, and intelligent aquatic\nsystems."
  },
  {
    "heading": "Unknown",
    "content": "CODE AVAILABILITY"
  },
  {
    "heading": "All FishDet-M resources, including the dataset, bench-",
    "content": "marking scripts, pretrained models, and documentation, are\npublicly available through the official project page: https:\n//lyessaadsaoud.github.io/FishDet-M/."
  },
  {
    "heading": "Unknown",
    "content": "ACKNOWLEDGMENTS"
  },
  {
    "heading": "This work is supported in part by Khalifa University Center",
    "content": "for Autonomous Robotic Systems (KUCARS) under Award"
  },
  {
    "heading": "RC1-2018-KUCARS and in part by CIRA under Award",
    "content": "8474000419 and 8434000534."
  },
  {
    "heading": "Unknown",
    "content": "DECLARATION"
  },
  {
    "heading": "During the preparation of this work, we used Grammarly",
    "content": "and AI tools to improve the English grammar and flow of the\npaper."
  },
  {
    "heading": "REFERENCES",
    "content": "[1] L. Liu, J. Wu, H. Zhao, H. Kong, T. Zheng, B. Qu, and H. Yu, \u201cFish-\nfinder: A robust small target detection method for aquaculture fish in\nlow-quality underwater images,\u201d Journal of Fish Biology, vol. 106, no. 3,\npp. 908\u2013920, 2025.\n[2] A. Salman, S. Maqbool, A. H. Khan, A. Jalal, and F. Shafait, \u201cReal-time\nfish detection in complex backgrounds using probabilistic background\nmodelling,\u201d Ecological Informatics, vol. 51, pp. 44\u201351, 2019.\n[3] S. Fayaz, S. A. Parah, and G. J. Qureshi, \u201cUnderwater object detection:\narchitectures and algorithms \u2013 a comprehensive review,\u201d Multimedia\nTools and Applications, vol. 81, no. 15, pp. 20 871\u201320 916, June 2022.\n[4] E. Li, Q. Wang, J. Zhang, W. Zhang, H. Mo, and Y. Wu, \u201cFish detection\nunder occlusion using modified you only look once v8 integrating real-\ntime detection transformer features,\u201d Applied Sciences, vol. 13, no. 23,\n2023.\n[5] P. Anantha Prabha, S. Sachin, U. Srinithish, M. Deva Priya, and\nS. Karthick, \u201cAutomated underwater fish species recognition using deep\nlearning-based techniques,\u201d in Proceedings of International Conference\non Recent Trends in Computing, R. P. Mahapatra, S. K. Peddoju, S. Roy,\nand P. Parwekar, Eds."
  },
  {
    "heading": "Singapore: Springer Nature Singapore, 2024, pp.",
    "content": "807\u2013815.\n[6] V. Pagire and A. Phadke, \u201cUnderwater fish detection and classification\nusing deep learning,\u201d in 2022 International Conference on Intelligent\nController and Computing for Smart Power (ICICCSP), 2022, pp. 1\u20134.\n[7] D. Marrable, K. Barker, S. Tippaya, M. Wyatt, S. Bainbridge, M. Stowar,\nand J. Larke, \u201cAccelerating species recognition and labelling of fish\nfrom underwater video with machine-assisted deep learning,\u201d Frontiers\nin Marine Science, vol. Volume 9 - 2022, 2022.\n[8] A. Sasithradevi, R. Suganya, P. Prakash, S. Mohamed Mansoor Roomi,\nM. Vijayalakshmi, S. Nathan, P. Kasthuri, J. Persiya, L. Brighty Ebenezer,\nS. Jain, S. Verma, S. Balasubramanian, M. Sai Subramaniam, T. Sai Sri-\nram, M. Pranav Phanindra Sai, C. Raj, A. Yadav, R. Payak, S. Paul Choud-\nhury, and R. Singh, \u201cDepondfi\u201923 challenge on real-time pond environ-\nment: Methods and results,\u201d IEEE Access, vol. 12, pp. 157 975\u2013157 987,\n2024.\n[9] V. Mohankumar and S. Anbalagan, \u201cA benchmark dataset and ensemble\nyolo method for enhanced underwater fish detection,\u201d ETRI Journal.\n[Online]. Available: https://doi.org/10.4218/etrij.2024-0383\n[10] Y. Jiang, Y. Wang, Y. Zhang, Q. Guo, M. Zhao, and H. Qin, \u201cA feature-\nenhanced and adaptive routing framework for fish school detection on\nauvs for degraded underwater imaging environments,\u201d IEEE Internet of"
  },
  {
    "heading": "Things Journal, vol. 11, no. 10, pp. 18 335\u201318 350, May 2024.",
    "content": "[11] C. Yang, J. Xiang, X. Li, and Y. Xie, \u201cFishdet-yolo: Enhanced underwater\nfish detection with richer gradient flow and long-range dependency\ncapture through mamba-c2f,\u201d Electronics, vol. 13, no. 18, 2024.\n[12] C. Shah, M. M. Nabi, S. Y. Alaba, I. A. Ebu, J. Prior, M. D. Campbell,\nR. Caillouet, M. D. Grossi, T. Rowell, F. Wallace, J. E. Ball, and\nR. Moorhead, \u201cYolov8-tf: Transformer-enhanced yolov8 for underwater\nfish species recognition with class imbalance handling,\u201d Sensors, vol. 25,\nno. 6, 2025.\n[13] M. A. Duhayyim, H. M. Alshahrani, F. N. Al-Wesabi, M. Alamgeer, A. M.\nHilal, and M. A. Hamza, \u201cIntelligent deep learning based automated fish\ndetection model for uwsn,\u201d Computers, Materials & Continua, vol. 70,\nno. 3, pp. 5871\u20135887, 2022.\n[14] P. J. Reddy, M. Malathi, and A. N. Julaiha, \u201cDeep fish: An approach to\nfish species identification through deep learning techniques,\u201d in Emerging\nTrends in Expert Applications and Security, V. S. Rathore, V. Piuri,"
  },
  {
    "heading": "Unknown",
    "content": "R. Babo, and V. Tiwari, Eds."
  },
  {
    "heading": "Singapore: Springer Nature Singapore,",
    "content": "2024, pp. 261\u2013272.\n[15] M. Jia-Fa, X. Gang, S. Wei-Guo, and X. Liu, \u201cA 3d occlusion tracking\nmodel of the underwater fish targets,\u201d in 2015 IEEE International\nConference on Electro/Information Technology (EIT), May 2015, pp.\n082\u2013086.\n[16] Q. Liu, X. Gong, J. Li, H. Wang, R. Liu, D. Liu, R. Zhou, T. Xie,\nR. Fu, and X. Duan, \u201cA multitask model for realtime fish detection and\nsegmentation based on YOLOv5,\u201d PeerJ Computer Science, vol. 9, p.\ne1262, 2023.\n[17] M. Dawkins, J. Prior, B. Lewis, R. Faillettaz, T. Banez, M. Salvi, A. Rollo,\nJ. Simon, M. Campbell, M. Lucero, A. Chaudhary, B. Richards, and\nA. Hoogs, \u201cFishtrack23: An ensemble underwater dataset for multi-\nobject tracking,\u201d in 2024 IEEE/CVF Winter Conference on Applications\nof Computer Vision (WACV), Jan 2024, pp. 7152\u20137161.\n[18] G. Jocher, A. Chaurasia, and J. Qiu, \u201cUltralytics yolov8,\u201d 2023. [Online]."
  },
  {
    "heading": "Available: https://github.com/ultralytics/ultralytics",
    "content": "11\nABUJABAL et al.: FISHDET-M: A UNIFIED LARGE-SCALE BENCHMARK FOR ROBUST FISH DETECTION\n[19] Z. Cai and N. Vasconcelos, \u201cCascade r-cnn: High quality object detection\nand instance segmentation,\u201d IEEE Transactions on Pattern Analysis and"
  },
  {
    "heading": "Machine Intelligence, p. 1\u20131, 2019.",
    "content": "[20] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, \u201cEnd-to-end object detection with transformers,\u201d in\nComputer Vision \u2013 ECCV 2020: 16th European Conference, Glasgow,"
  },
  {
    "heading": "Unknown",
    "content": "UK, August 23\u201328, 2020, Proceedings, Part I."
  },
  {
    "heading": "Unknown",
    "content": "Berlin, Heidelberg:"
  },
  {
    "heading": "Springer-Verlag, 2020, p. 213\u2013229.",
    "content": "[21] K. Chieza, D. Brown, J. Connan, and D. Salie, \u201cAutomated fish detection\nin underwater environments: Performance analysis of yolov8 and yolo-\nnas,\u201d in Artificial Intelligence Research, A. Gerber, J. Maritz, and A. W."
  },
  {
    "heading": "Unknown",
    "content": "Pillay, Eds."
  },
  {
    "heading": "Cham: Springer Nature Switzerland, 2025, pp. 334\u2013351.",
    "content": "[22] J. Fabic, I. Turla, J. Capacillo, L. David, and P. C. Naval, \u201cFish population\nestimation and species classification from underwater video sequences\nusing blob counting and shape analysis,\u201d in 2013 IEEE International"
  },
  {
    "heading": "Underwater Technology Symposium (UT), March 2013, pp. 1\u20136.",
    "content": "[23] L. Saad Saoud, Z. Niu, L. Seneviratne, and I. Hussain, \u201cReal-time and\nresource-efficient multi-scale adaptive robotics vision for underwater\nobject detection and domain generalization,\u201d in Proceedings of the IEEE\nInternational Conference on Image Processing (ICIP), 2024, pp. 3917\u2013\n3923.\n[24] M. Iqtait, M. H. Alqaryouti, A. E. Sadeq, A. Aburomman, M. Baniata,\nZ. Mustafa, and H. Y. Chan, \u201cEnhanced fish species detection and\nclassification using a novel deep learning approach,\u201d International Journal\nof Advanced Computer Science and Applications, vol. 15, no. 10, 2024.\n[25] C. Spampinato, S. Palazzo, B. Boom, and R. B. Fisher, \u201cOverview\nof the lifeclef 2014 fish task,\u201d in Working Notes for CLEF 2014\nConference, Sheffield, UK, September 15-18, 2014, ser. CEUR Workshop\nProceedings, L. Cappellato, N. Ferro, M. Halvey, and W. Kraaij, Eds.,\nvol. 1180. CEUR-WS.org, 2014, pp. 616\u2013624. [Online]. Available: https:\n//ceur-ws.org/Vol-1180/CLEF2014wn-Life-SpampinatoEt2014.pdf\n[26] Y. Wei, Y. Wang, B. Zhu, C. Lin, D. Wu, X. Xue, and R. Wang,\n\u201cUnderwater detection: A brief survey and a new multitask dataset,\u201d\nInternational Journal of Network Dynamics and Intelligence, vol. 3,\nno. 4, p. 100025, 2024, published: 25 December 2024.\n[27] S. Fan, C. Song, H. Feng, and Z. Yu, \u201cTake good care of your fish: fish\nre-identification with synchronized multi-view camera system,\u201d Frontiers\nin Marine Science, vol. Volume 11 - 2024, 2024.\n[28] A. Saleh, I. H. Laradji, D. A. Konovalov, M. Bradley, D. Vazquez, and\nM. Sheaves, \u201cA realistic fish-habitat dataset to evaluate algorithms for\nunderwater visual analysis,\u201d Scientific Reports, vol. 10, no. 1, p. 14671,\n9 2020.\n[29] Australian Institute of Marine Science (AIMS), University of Western\nAustralia (UWA), and Curtin University, \u201cOzfish dataset - machine\nlearning dataset for baited remote underwater video stations,\u201d 2019,\naccessed 22-Jul-2025.\n[30] S. Ren, K. He, R. Girshick, and J. Sun, \u201cFaster r-cnn: towards real-time\nobject detection with region proposal networks,\u201d p. 91\u201399, 2015.\n[31] L. Saad Saoud, M. Elmezain, A. Sultan, M. Heshmat, L. Seneviratne,\nand I. Hussain, \u201cSeeing through the haze: A comprehensive review of\nunderwater image enhancement techniques,\u201d IEEE Access, vol. 12, pp.\n145 206\u2013145 233, 2024.\n[32] M. Pedersen, D. Lehotsk\u00fd, I. Nikolov, and T. B. Moeslund, \u201cBrackishmot:\nThe brackish multi-object tracking dataset,\u201d in Image Analysis, R. Gade,"
  },
  {
    "heading": "Unknown",
    "content": "M. Felsberg, and J.-K. K\u00e4m\u00e4r\u00e4inen, Eds."
  },
  {
    "heading": "Unknown",
    "content": "Cham: Springer Nature"
  },
  {
    "heading": "Switzerland, 2023, pp. 17\u201333.",
    "content": "[33] J. Hong, M. Fulton, and J. Sattar, \u201cTrashcan: A semantically-\nsegmented\ndataset\ntowards\nvisual\ndetection\nof\nmarine\ndebris,\u201d\narXiv preprint arXiv:2007.08097, 2020. [Online]. Available: https:\n//arxiv.org/abs/2007.08097\n[34] S. Lian, H. Li, R. Cong, S. Li, W. Zhang, and S. Kwong, \u201cWatermask:\nInstance segmentation for underwater imagery,\u201d in 2023 IEEE/CVF\nInternational Conference on Computer Vision (ICCV), Oct 2023, pp.\n1305\u20131315.\n[35] P. Zhang, L. Wang, G. Wang, and D. Li, \u201cEornet: An improved rotating\nbox detection model for counting juvenile fish under occlusion and\noverlap,\u201d Engineering Applications of Artificial Intelligence, vol. 124, p.\n106528, 2023.\n[36] J. Mao, G. Xiao, W. Sheng, Z. Qu, and Y. Liu, \u201cResearch on realizing\nthe 3d occlusion tracking location method of fish\u2019s school target,\u201d"
  },
  {
    "heading": "Neurocomputing, vol. 214, pp. 61\u201379, 2016.",
    "content": "[37] M. Sudhakara, M. J. Meena, K. R. Madhavi, P. Anjaiah, and\nL.\nP.\nK,\n\u201cFish\nclassification\nusing\ndeep\nlearning\non\nsmall\nscale and low-quality images,\u201d International Journal of Intelligent"
  },
  {
    "heading": "Systems and Applications in Engineering, vol. 10, no. 1s, pp.",
    "content": "279\u2013288,"
  },
  {
    "heading": "October",
    "content": "2022,\nresearch"
  },
  {
    "heading": "Article.",
    "content": "[Online]."
  },
  {
    "heading": "Available:",
    "content": "https://www.ijisae.org/index.php/IJISAE/article/view/2292\n[38] C. Qiu, S. Zhang, C. Wang, Z. Yu, H. Zheng, and B. Zheng, \u201cImproving\ntransfer learning and squeeze- and-excitation networks for small-scale\nfine-grained fish image classification,\u201d IEEE Access, vol. 6, pp. 78 503\u2013\n78 512, 2018.\n[39] R. van Essen, A. Mencarelli, A. van Helmond, L. Nguyen, J. Batsleer,\nJ.-J. Poos, and G. Kootstra, \u201cAutomatic discard registration in cluttered\nenvironments using deep learning and object tracking: class imbalance,\nocclusion, and a comparison to human review,\u201d ICES Journal of Marine"
  },
  {
    "heading": "Science, vol. 78, no. 10, pp. 3834\u20133846, 11 2021.",
    "content": "[40] R. J. M. Veiga and J. M. F. Rodrigues, \u201cFine-grained fish classification\nfrom small to large datasets with vision transformers,\u201d IEEE Access,\nvol. 12, pp. 113 642\u2013113 660, 2024.\n[41] X. Geng, J. Gao, Y. Zhang, and R. Wang, \u201cA dual-branch feature fusion\nneural network for fish image fine-grained recognition,\u201d The Visual"
  },
  {
    "heading": "Computer, vol. 40, no. 10, pp. 6883\u20136896, October 2024.",
    "content": "[42] P. C. Brady, A. A. Gilerson, G. W. Kattawar, J. M. Sullivan, M. S. Twar-\ndowski, H. M. Dierssen, M. Gao, K. Travis, R. I. Etheredge, A. Tonizzo,\nA. Ibrahim, C. Carrizo, Y. Gu, B. J. Russell, K. Mislinski, S. Zhao, and\nM. E. Cummings, \u201cOpen-ocean fish reveal an omnidirectional solution\nto camouflage in polarized environments,\u201d Science, vol. 350, no. 6263,\npp. 965\u2013969, 2015.\n[43] C. Shah, M. M. Nabi, S. Y. Alaba, R. Caillouet, J. Prior, M. Campbell,\nM. D. Grossi, F. Wallace, J. E. Ball, and R. Moorhead, \u201cActive detection\nfor fish species recognition in underwater environments,\u201d in Ocean\nSensing and Monitoring XVI, W. Hou and L. J. Mullen, Eds., vol. 13061,"
  },
  {
    "heading": "Unknown",
    "content": "International Society for Optics and Photonics."
  },
  {
    "heading": "SPIE, 2024, p. 130610D.",
    "content": "[44] S. Berlia, V. K. Singh, M. Kumar, R. Mahato, and M. Mishra, \u201cEnhanced\nfish species identification using transfer learning on balanced datasets,\u201d\nin 2024 15th International Conference on Computing Communication\nand Networking Technologies (ICCCNT), June 2024, pp. 1\u20135.\n[45] J. Zhai, L. Han, Y. Xiao, M. Yan, Y. Wang, and X. Wang, \u201cFew-shot\nfine-grained fish species classification via sandwich attention covamnet,\u201d"
  },
  {
    "heading": "Frontiers in Marine Science, vol. Volume 10 - 2023, 2023.",
    "content": "[46] S. T. K R, K. S. Ananda Kumar, S. P. R, and V. L, \u201cComparative\nanalysis of neural architectures for underwater object detection,\u201d in 2024\nSecond International Conference on Advances in Information Technology\n(ICAIT), vol. 1, July 2024, pp. 1\u20137.\n[47] M. Abdalhafez, I. M. H. AbdelDaiam, M. E. H. Eltaib, and M. Ab-\ndelrahim, \u201cEnhanced detection and classification of underwater objects\nusing rov and computer vision,\u201d JES. Journal of Engineering Sciences,\nvol. 52, no. 2, pp. 73\u201386, 2024.\n[48] M. Shen, M. Yang, J. Zhong, H. Liu, and C. Pan, \u201cUnderwater image\nquality evaluation: A comprehensive review,\u201d IET Image Processing,\nvol. 19, no. 1, p. e70068, 2025.\n[49] H. Ma, Y. Zhang, S. Sun, W. Zhang, M. Fei, and H. Zhou, \u201cWeighted\nmulti-error information entropy based you only look once network\nfor underwater object detection,\u201d Engineering Applications of Artificial"
  },
  {
    "heading": "Intelligence, vol. 130, p. 107766, 2024.",
    "content": "[50] S. Bhalla, A. Kumar, and R. Kushwaha, \u201cA novel underwater marine\ndataset with diverse scenarios for robust object detection,\u201d in Proceedings\nof the 2024 Sixteenth International Conference on Contemporary"
  },
  {
    "heading": "Unknown",
    "content": "Computing, ser. IC3-2024."
  },
  {
    "heading": "Unknown",
    "content": "New York, NY, USA: Association for"
  },
  {
    "heading": "Computing Machinery, 2024, p. 1\u201311.",
    "content": "[51] W. Liu, Q. Ma, P. Liu, and H. Zhao, \u201cA performance evaluation\nmethod for distant early warning sonar mobile area search,\u201d in 2023\n3rd International Conference on Electronic Information Engineering and"
  },
  {
    "heading": "Computer Science (EIECS), Sep. 2023, pp. 360\u2013364.",
    "content": "[52] P. Guo, H. Liu, D. Zeng, T. Xiang, L. Li, and K. Gu, \u201cAn underwater\nimage quality assessment metric,\u201d IEEE Transactions on Multimedia,\nvol. 25, pp. 5093\u20135106, 2023.\n[53] Q. Jiang, X. Yi, L. Ouyang, J. Zhou, and Z. Wang, \u201cToward dimension-\nenriched underwater image quality assessment,\u201d IEEE Transactions on\nCircuits and Systems for Video Technology, vol. 35, no. 2, pp. 1385\u20131398,"
  },
  {
    "heading": "Feb 2025.",
    "content": "[54] M. Elmezain, L. Saad Saoud, A. Sultan, M. Heshmat, L. Seneviratne,\nand I. Hussain, \u201cAdvancing underwater vision: A survey of deep learning\nmodels for underwater object recognition and tracking,\u201d IEEE Access,\nvol. 12, 2025, early Access.\n[55] W. Akram, A. Casavola, N. Kapetanovi\u00b4c, and N. Mi\u0161kovic, \u201cA visual\nservoing scheme for autonomous aquaculture net pens inspection using\nrov,\u201d Sensors, vol. 22, no. 9, p. 3525, 2022.\n[56] W. Akram, T. Hassan, H. Toubar, M. Ahmed, N. Mi\u0161kovic, L. Seneviratne,\nand I. Hussain, \u201cAquaculture defects recognition via multi-scale semantic\nsegmentation,\u201d Expert systems with applications, vol. 237, p. 121197,\n2024.\n[57] M. Vijayalakshmi and A. Sasithradevi, \u201cAquayolo: Advanced yolo-based\n12\nABUJABAL et al.: FISHDET-M: A UNIFIED LARGE-SCALE BENCHMARK FOR ROBUST FISH DETECTION\nfish detection for optimized aquaculture pond monitoring,\u201d Scientific"
  },
  {
    "heading": "Reports, vol. 15, no. 1, p. 6151, February 2025.",
    "content": "[58] Q. Zhang and S. Chen, \u201cResearch on improved lightweight fish detection\nalgorithm based on yolov8n,\u201d Journal of Marine Science and Engineering,\nvol. 12, no. 10, 2024.\n[59] B. Teixeira, A. P. Lima, C. Pinho, D. Viegas, N. Dias, H. Silva,\nand J. Almeida, \u201cFeedfirst: Intelligent monitoring system for indoor\naquaculture tanks,\u201d in OCEANS 2022, Hampton Roads, Oct 2022, pp.\n1\u20137.\n[60] Y.-S. Ryuh, G.-H. Yang, J. Liu, and H. Hu, \u201cA school of robotic fish for\nmariculture monitoring in the sea coast,\u201d Journal of Bionic Engineering,\nvol. 12, no. 1, pp. 37\u201346, 2015.\n[61] H. Wang, Z. Gao, B. Li, and N. Gao, \u201cResearch on robotic fish swarm\nnetwork technology based on underwater acoustic communication,\u201d in\n2023 IEEE International Conference on Image Processing and Computer"
  },
  {
    "heading": "Applications (ICIPCA), Aug 2023, pp. 475\u2013479.",
    "content": "[62] M. Ahmed, A. B. Bakht, T. Hassan, W. Akram, A. Humais, L. Seneviratne,\nS. He, D. Lin, and I. Hussain, \u201cVision-based autonomous navigation\nfor unmanned surface vessel in extreme marine conditions,\u201d in 2023\nIEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS)."
  },
  {
    "heading": "IEEE, 2023, pp. 7097\u20137103.",
    "content": "[63] M. U. Din, A. Humais, W. Akram, M. Alblooshi, L. Saad Saoud,\nA. Alblooshi, L. Seneviratne, and I. Hussain, \u201cMarine X: Design and\nimplementation of unmanned surface vessel for vision guided navigation,\u201d\nin 2023 21st International Conference on Advanced Robotics (ICAR)."
  },
  {
    "heading": "IEEE, 2023, pp. 226\u2013231.",
    "content": "[64] W. Akram, A. B. Bakht, M. U. Din, L. Seneviratne, and I. Hussain,\n\u201cEnhancing aquaculture net pen inspection: A benchmark study on\ndetection and semantic segmentation,\u201d IEEE Access, 2024.\n[65] W. Zhang, L. Zhang, Y. Zhong, P. Lin, and F. Zhang, \u201cRecognition\nand calculation of fish rafts in mariculture on the basis of artificial\nintelligence,\u201d in Proceedings of the 2023 International Conference on\nWireless Communications, Networking and Applications, P. Siarry, M. A."
  },
  {
    "heading": "Unknown",
    "content": "Jabbar, S. K. S. Cheung, and X. Li, Eds."
  },
  {
    "heading": "Unknown",
    "content": "Singapore: Springer Nature"
  },
  {
    "heading": "Singapore, 2025, pp. 203\u2013210.",
    "content": "[66] L. Falconer, S. Halstensen, S. F. Rin\u00f8, C. Noble, T. Dale, R. Alvestad,\nand E. Ytteborg, \u201cMarine aquaculture sites have huge potential as data\nproviders for climate change assessments,\u201d Aquaculture, vol. 595, p.\n741519, 2025.\n[67] L. Saad Saoud, A. Sultan, M. Elmezain, M. Heshmat, L. Seneviratne,\nand I. Hussain, \u201cBeyond observation: Deep learning for animal behavior\nand ecological conservation,\u201d Ecological Informatics, vol. 85, p. 102893,\n2024.\n[68] J. Kong, S. Tang, J. Feng, L. Mo, and X. Jin, \u201cAasnet: A novel image\ninstance segmentation framework for fine-grained fish recognition via\nlinear correlation attention and dynamic adaptive focal loss,\u201d Applied"
  },
  {
    "heading": "Sciences, vol. 15, no. 7, 2025.",
    "content": "[69] W. Wang, B. He, and L. Zhang, \u201cHigh-accuracy real-time fish detection\nbased on self-build dataset and rird-yolov3,\u201d Complexity, vol. 2021, no. 1,\np. 4761670, 2021.\n[70] M. U. Din, A. B. Bakht, W. Akram, Y. Dong, L. Seneviratne, and\nI. Hussain, \u201cBenchmarking vision-based object tracking for usvs in\ncomplex maritime environments,\u201d IEEE Access, 2025.\n[71] L. Hong, X. Wang, G. Zhang, and M. Zhao, \u201cUsod10k: A new benchmark\ndataset for underwater salient object detection,\u201d IEEE Transactions on"
  },
  {
    "heading": "Image Processing, vol. 34, pp. 1602\u20131615, 2025.",
    "content": "[72] COCO Consortium, \u201cCoco - common objects in context,\u201d 2014,\naccessed: 2025-07-15. [Online]. Available: https://cocodataset.org\n[73] F. F. Khan, X. Li, A. J. Temple, and M. Elhoseiny, \u201cFishnet: A large-scale\ndataset and benchmark for fish recognition, detection, and functional trait\nprediction,\u201d in 2023 IEEE/CVF International Conference on Computer"
  },
  {
    "heading": "Vision (ICCV), 2023, pp. 20 439\u201320 449.",
    "content": "[74] A. Wang, H. Chen, L. Liu, Y. Wang, Y. Zhong, C. Shan, Z. Guo, C. Xu,\nand S. Chen, \u201cYolov10: Real-time end-to-end object detection,\u201d arXiv\npreprint arXiv:2405.14458, 2024, https://arxiv.org/abs/2405.14458.\n[75] G. Jocher and J. Qiu, \u201cUltralytics yolo11,\u201d 2024. [Online]. Available:\nhttps://github.com/ultralytics/ultralytics\n[76] Y. Tian, Q. Ye, and D. Doermann, \u201cYolov12: Attention-centric\nreal-time object detectors,\u201d 2025. [Online]. Available: https://github.com/\nsunsmarterjie/yolov12\n[77] S. Aharon, Louis-Dupont, Ofri Masad, K. Yurkova, Lotem Fridman,\nLkdci, E. Khvedchenya, R. Rubin, N. Bagrov, B. Tymchenko, T. Keren,"
  },
  {
    "heading": "A. Zhilko, and Eran-Deci, \u201cSuper-gradients,\u201d 2021.",
    "content": "[78] P. Sun, R. Zhang, Y. Jiang, T. Kong, C. Xu, W. Zhan, M. Tomizuka,\nZ. Yuan, and P. Luo, \u201cSparse r-cnn: An end-to-end framework for\nobject detection,\u201d IEEE Transactions on Pattern Analysis and Machine"
  },
  {
    "heading": "Intelligence, vol. 45, no. 12, pp. 15 650\u201315 664, Dec 2023.",
    "content": "[79] Y. Zhao, W. Lv, S. Xu, J. Wei, G. Wang, Q. Dang, Y. Liu, and J. Chen,\n\u201cDetrs beat yolos on real-time object detection,\u201d pp. 16 965\u201316 974, June\n2024.\n[80] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, \u201cDeformable\ndetr: Deformable transformers for end-to-end object detection,\u201d 2021.\n[Online]. Available: https://arxiv.org/abs/2010.04159\n[81] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll\u00e1r, \u201cFocal loss for\ndense object detection,\u201d vol. 42, no. 2, Feb 2020, pp. 318\u2013327.\n[82] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and\nA. C. Berg, \u201cSsd: Single shot multibox detector,\u201d pp. 21\u201337, 2016.\n[83] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. K\u00f6pf,\nE. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner,\nL. Fang, J. Bai, and S. Chintala, PyTorch: an imperative style, high-\nperformance deep learning library."
  },
  {
    "heading": "Unknown",
    "content": "Red Hook, NY, USA: Curran"
  },
  {
    "heading": "Associates Inc., 2019.",
    "content": "[84] G. Jocher, J. Qiu, and A. Chaurasia, \u201cUltralytics YOLO,\u201d Jan. 2023.\n[Online]. Available: https://github.com/ultralytics/ultralytics\n[85] K. Chen, J. Wang, J. Pang, Y. Cao, Y. Xiong, X. Li, S. Sun, W. Feng,\nZ. Liu, J. Xu, Z. Zhang, D. Cheng, C. Zhu, T. Cheng, Q. Zhao, B. Li,\nX. Lu, R. Zhu, Y. Wu, J. Dai, J. Wang, J. Shi, W. Ouyang, C. C.\nLoy, and D. Lin, \u201cMmdetection: Open mmlab detection toolbox and\nbenchmark,\u201d 2019. [Online]. Available: https://arxiv.org/abs/1906.07155\n[86] T.-Y. Lin and M. C. Consortium, \u201cpycocotools: Coco api for python,\u201d\nhttps://github.com/cocodataset/cocoapi, 2015, accessed: 2025-07-15.\n[87] g18L5754, \u201cFish4knowledge dataset dataset,\u201d https://universe.roboflow.\ncom/g18l5754/fish4knowledge-dataset, oct 2023, visited on 2025-\n05-21. [Online]. Available: https://universe.roboflow.com/g18l5754/\nfish4knowledge-dataset\n[88] A. MURME, \u201cFish video dataset,\u201d https://universe.roboflow.com/\naarjoo-murme/fish-video-ls42k,\nnov\n2023,\nvisited\non\n2025-05-\n21. [Online]. Available: https://universe.roboflow.com/aarjoo-murme/\nfish-video-ls42k\n[89] seoultech, \u201cFish-video dataset,\u201d https://universe.roboflow.com/seoultech/\nfish-video, may 2022, visited on 2025-05-21. [Online]. Available:\nhttps://universe.roboflow.com/seoultech/fish-video\n[90] I. A. Catal\u00e1n, A. \u00c1lvarez Ellacur\u00eda, J.-L. Lisani, J. S\u00e1nchez, G. Vizoso,\nA. E. Heinrichs-Maquil\u00f3n, H. Hinz, J. Al\u00f3s, M. Signarioli, J. Aguzzi,\nM. Francescangeli, and M. Palmer, \u201cAutomatic detection and classification\nof coastal mediterranean fish from underwater images: Good practices\nfor robust training,\u201d Frontiers in Marine Science, vol. Volume 10 - 2023,\n2023.\n[91] aquarium, \u201cdetect aqurium dataset,\u201d https://universe.roboflow.com/\naquarium-lrui2/detect-aqurium, sep 2023, visited on 2025-06-30. [Online].\nAvailable: https://universe.roboflow.com/aquarium-lrui2/detect-aqurium\n[92] M.-Q. Le, T.-N. Le, T. V. Nguyen, I. Echizen, and M.-T. Tran, \u201cAquatic\nanimal species (aas),\u201d 2023, data set.\n[93] PAFD, \u201cFish clean dataset,\u201d https://universe.roboflow.com/pafd/fish-clean,\nfeb 2023, visited on 2025-06-30. [Online]. Available: https://universe.\nroboflow.com/pafd/fish-clean\n[94] Z. Tian, C. Shen, H. Chen, and T. He, \u201cFcos: Fully convolutional one-\nstage object detection,\u201d pp. 9626\u20139635, Oct 2019.\n[95] test, \u201cfish_dataset_florence_1 dataset,\u201d https://universe.roboflow.com/\ntest-fflhx/fish_dataset_florence_1, Aug. 2024, accessed on 2025-07-20.\n[96] L. Saad Saoud and I. Hussain, \u201cEba-ai: Ethics-guided bias-aware ai\nfor efficient underwater image enhancement and coral reef monitoring,\u201d\n2025. [Online]. Available: https://arxiv.org/abs/2507.15036\n[97] A. Salman, S. A. Siddiqui, F. Shafait, A. Mian, M. R. Shortis, K. Khurshid,\nA. Ulges, and U. Schwanecke, \u201cAutomatic fish detection in underwater\nvideos by a deep neural network-based hybrid motion learning system,\u201d\nICES Journal of Marine Science, vol. 77, no. 4, pp. 1295\u20131307, 02 2019.\n[98] A. Jalal, A. Salman, A. Mian, S. Ghafoor, and F. Shafait, \u201cDeepfins:\nCapturing dynamics in underwater videos for fish detection,\u201d Ecological"
  },
  {
    "heading": "Informatics, vol. 86, p. 103013, 2025.",
    "content": "13"
  }
]