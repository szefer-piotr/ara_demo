[
  {
    "heading": "Unknown",
    "content": "Improving Generative Ad Text on Facebook using"
  },
  {
    "heading": "Reinforcement Learning",
    "content": "Daniel R. Jiang\u2020,\u2217, Alex Nikulkov\u2020, Yu-Chia Chen, Yang Bai, Zheqing Zhu"
  },
  {
    "heading": "Meta Platforms, Menlo Park, California, USA.",
    "content": "\u2020These authors contributed equally to this work.\n\u2217Corresponding author. Email: drjiang@meta.com"
  },
  {
    "heading": "Abstract",
    "content": "Generative artificial intelligence (AI), in particular large language models (LLMs), is poised\nto drive transformative economic change. LLMs are pre-trained on vast text data to learn general\nlanguage patterns, but a subsequent post-training phase is critical to align them for specific\nreal-world tasks. Reinforcement learning (RL) is the leading post-training technique, yet its\neconomic impact remains largely underexplored and unquantified. We examine this question\nthrough the lens of the first deployment of an RL-trained LLM for generative advertising on\nFacebook. Integrated into Meta\u2019s Text Generation feature, our model, \u201cAdLlama,\u201d powers\nan AI tool that helps advertisers create new variations of human-written ad text. To train\nthis model, we introduce reinforcement learning with performance feedback (RLPF), a post-\ntraining method that uses historical ad performance data as a reward signal. In a large-scale\n10-week A/B test on Facebook spanning nearly 35,000 advertisers and 640,000 ad variations,\nwe find that AdLlama improves click-through rates by 6.7% (\ud835\udc5d= 0.0296) compared to a\nsupervised imitation model trained on curated ads. This represents a substantial improvement\nin advertiser return on investment on Facebook. We also find that advertisers who used AdLlama\ngenerated more ad variations, indicating higher satisfaction with the model\u2019s outputs. To our\n1\narXiv:2507.21983v1  [cs.LG]  29 Jul 2025\nknowledge, this is the largest study to date on the use of generative AI in an ecologically valid\nsetting, offering an important data point quantifying the tangible impact of RL post-training.\nFurthermore, the results show that RLPF is a promising and generalizable approach for metric-\ndriven post-training that bridges the gap between highly capable language models and tangible\noutcomes."
  },
  {
    "heading": "Unknown",
    "content": "1"
  },
  {
    "heading": "Introduction",
    "content": "Generative artificial intelligence (AI) is increasingly being recognized for its transformative po-\ntential across industries, driving innovations in content creation [1, 2, 3, 4, 5], education [6, 7, 8],\nmedicine [9, 10, 11, 12], and complex decision-making [13, 14, 15]. It is also widely believed\nto have the potential for significant economic impact [16, 17, 18, 19, 20], with pioneering work\nshowing quantifiable benefits in the areas of customer support [21, 22], enterprise information tasks\n[23], legal tasks [24, 25], consulting work [26], software development [27, 28, 18], email marketing\n[29], online advertising [30], and a variety of professional writing tasks [31]. However, turning\nthis potential into consistent real-world impact depends critically on how models are fine-tuned and\naligned during a phase of model training called post-training.\nThe initial phase for large language models (LLM) training is called pre-training, where the\nmodel learns general language patterns and world knowledge from large-scale unlabeled text data.\nHowever, to perform effectively in real-world applications, generative AI systems require some form\nof post-training\u2014a suite of methods designed to adapt pre-trained foundation models to specific\ndownstream tasks [32]. A common approach involves supervised fine-tuning (SFT), in which\nmodels are trained to follow human instructions by mimicking curated target responses [33, 34].\nAnother example is the use of reinforcement learning from human feedback (RLHF) to train the\nmodel on human preference data [35], which has been shown to be highly effective in domains\nlike summarization [36], chatbot assistants [37], and instruction-following [38]. More recently,\nresearchers have found that reinforcement learning (RL) using verifiable rewards (i.e., objective\nsignals from domains like math or coding that can be automatically validated) can lead to marked\n2\nimprovements in LLM reasoning and problem-solving capabilities [32, 39].\nAs noted above, there is a broadening body of research focused on quantifying the impact of\nLLMs across various sectors. However, the specific impact of the post-training phase has been\nrelatively underexplored, despite its critical role in the development of virtually all prominent LLM\nmodels. In this paper, we present new findings on the concrete impact of RL for post-training.\nOur analysis is conducted through the perspective of the online advertising industry, which is a\nmajor driver of the global economy: worldwide expenditure in online ads is projected to reach $513\nbillion USD in 2025 [40] and is expected to represent 63% of the total global advertising revenue\nacross all mediums [41].\nSpecifically, we focus on Meta\u2019s Text Generation product, which uses an LLM to generate\nvariations of an advertiser\u2019s human-written ad text. This allows each advertiser to select multiple\nversions of the ad to show to potential customers, taking advantage of Meta\u2019s ad delivery systems\nto show the most performant variants. The initial version of the Text Generation product used an\nLLM that was trained in a naive manner, by fine-tuning it to imitate the style of a set of curated ads\nusing supervised learning (SFT).\nIn this paper, we take the next step and describe our efforts in using reinforcement learning to\nimprove the Text Generation LLM, with the measurable goal of writing more engaging ad text.\nWe do this by directly using performance (i.e., click-through rates) as a reward signal. We call\nthe approach reinforcement learning with performance feedback (RLPF). By viewing each user\u2019s\nbehavior (i.e., click or no click) on each ad impression as a small piece of human feedback, RLPF\ncan be thought of as an extension of the well-known RLHF technique [35, 33], except that each\npiece of ad text is being \u201crated\u201d by thousands of humans (i.e., Facebook users who see an ad\nimpression) via click or no click signals. In that sense, one can view RLPF as falling somewhere in\nbetween traditional RLHF and the recently explored direction of RL with verifiable rewards [32].\nWe report on the results of a large-scale online experiment (i.e., A/B test) conducted on Facebook\nthat compared the effect of providing advertisers with the RLPF-trained model versus the naive SFT-\nbased imitation model. Our experiment and analysis encompass approximately 35,000 advertisers"
  },
  {
    "heading": "Unknown",
    "content": "3"
  },
  {
    "heading": "Unknown",
    "content": "Reward model"
  },
  {
    "heading": "Facebook ad",
    "content": "performance data"
  },
  {
    "heading": "Unknown",
    "content": "AdLlama"
  },
  {
    "heading": "Unknown",
    "content": "Base LLM"
  },
  {
    "heading": "Unknown",
    "content": "Reinforcement Learning with"
  },
  {
    "heading": "Unknown",
    "content": "Performance Feedback"
  },
  {
    "heading": "Unknown",
    "content": "Large-scale"
  },
  {
    "heading": "Advertiser A/B Test",
    "content": "RL\n. . .\n. . .\n+6.7% advertiser CTR\n\u201cDiscover your next \nfavorite book!\u201d"
  },
  {
    "heading": "Reward: 2.1%",
    "content": "1.5% \n2.1% \n2.5% \n3.0% \n..."
  },
  {
    "heading": "Unknown",
    "content": "Ncontrol = 17,632"
  },
  {
    "heading": "Ntest = 17,217",
    "content": "Figure 1: Overview of our contributions. Our first contribution, illustrated in the left panel, is\nRLPF, a reinforcement learning (RL) approach to post-training an LLM based on an aggregate\nperformance metric. We apply RLPF to a generative AI feature in Meta\u2019s Ads Manager that helps\nadvertisers generate new ad text variations. To do this, we use Facebook ad performance data (i.e.,\nclick-through rates) to train a reward model, which can score the effectiveness of a piece of ad text.\nSubsequently, we align the LLM toward this reward model using RL, which involves iteratively\ngenerating and scoring ad text, illustrated in lower part of the figure. The resulting LLM is called\n\u201cAdLlama.\u201d Our second contribution, illustrated in the right panel, are the results of a large-scale\nadvertiser A/B test, encompassing approximately 35,000 advertisers and 640,000 ad variations,\nwhich showed that advertisers who received AdLlama achieved 6.7% higher advertiser-level click-\nthrough rates (CTR). To our knowledge, this study is the largest reported so far that investigates\nthe use of generative AI in an ecologically valid setting.\nand 640,000 ad variations, observed over 10 weeks. The results indicate that when equipped with\nthe RLPF model, advertisers achieved a click-through rate (CTR) increase of 6.7% compared to the\nnaive imitation model. In addition, the number of ad variations created by the advertiser increased\nby 18.5%, suggesting that advertisers were more willing to adopt AI suggestions when they came\nfrom AdLlama.\nThe implications of this result are significant. First, this improvement highlights the effec-\ntiveness of reinforcement learning as a methodology for metric-driven post-training of LLMs,\n4\nespecially in business use cases. Second, it offers a valuable data point by quantifying the benefits\nof RL-based post-training of LLMs in the domain of online advertising, contributing an important\ninsight in the broader ongoing effort to understand the implications of generative AI.\nTo our knowledge, is the largest study to date examining the use of generative AI in a real-world,\necologically valid context. Previous analyses often relied on smaller-scale experiments involving a\nfew thousand participants [26, 27, 22, 28, 29] or controlled laboratory settings using crowd workers\n[31, 24, 25, 30, 20]. These limitations largely reflect the high computational costs of training and\ndeploying such models, as well as the scarcity of opportunities for large-scale field deployment."
  },
  {
    "heading": "Unknown",
    "content": "Original human-written ad text"
  },
  {
    "heading": "LLM-generated variations",
    "content": "Figure 2: The user interface of the Meta Text Generation product. Here we show an example of\nhow the Text Generation product functions during the ad creation process. The advertiser provides\nan original version of the ad text (\u201cSpend your weekend with a new book! Come visit the bookstore\ntoday.\u201d), which serves as input to Text Generation. The underlying LLM then produces a set of ad\ntext variations, allowing the advertiser to pick and choose the ones they prefer to use. The advertiser\ncan also opt to generate additional variants via the \u201cGenerate more\u201d button."
  },
  {
    "heading": "Unknown",
    "content": "2"
  },
  {
    "heading": "Meta\u2019s Text Generation Product",
    "content": "The Text Generation product [42] is a generative AI feature within Meta Ads Manager that allows\nadvertisers to experiment with multiple versions of their ad text. The feature works by taking an\n5\nadvertiser\u2019s original ad text as input and then using an underlying LLM to suggest new variations,\nwhich may, for example, emphasize key selling points or add additional creative messaging.\n2.1"
  },
  {
    "heading": "User Interface",
    "content": "The user interface of the Text Generation product is illustrated in Figure 2. The flow is as follows.\nAdvertisers first input an original ad text. The underlying LLM then generates and displays multiple\ntext variations. Advertisers can then select the variations they want to use, edit them directly in the\ntext box, or even add their own custom variations via the \u201cAdd text option\u201d button [43]. In Figure 2,\nwe show an example where the advertiser has selected a total of four text variations, the original\nad text along with three AI-generated variations. The option to continue generating additional text\nvariations for consideration is available via the \u201cGenerate more\u201d button, but the advertiser is limited\nto selecting a maximum of five variations for delivery to users.\nWe note that although the Text Generation interface appears during the ad creation process,\nadvertisers are never required to select AI-written ads."
  },
  {
    "heading": "Some other possible actions that the",
    "content": "advertiser may opt to take are: (1) ignore the AI\u2019s suggestions completely and continue with the\noriginal text, (2) ignore the suggestions and instead supply multiple human-written text variations,\n(3) edit the AI-written ads before selecting them, (4) use the AI-written suggestions as inspiration\nfor additional human-written variations, or (5) make AI-inspired edits to the original human-written\nvariation after seeing the AI-generated variants. Therefore, the LLM can play an nuanced role in\nshaping ad text, regardless of whether the advertiser ultimately selects the precise wording that was\ngenerated.\n2.2"
  },
  {
    "heading": "Naive Imitation Models",
    "content": "The original (naive) version of Text Generation LLM was released to advertisers in November 2023.\nThis LLM is based on Meta\u2019s open-source foundation language model, the 7 billion (7B) parameter\nversion of Llama 2 Chat [44]. As mentioned previously, the Llama model was post-trained to im-\nitate a set of curated ads using SFT. We refer to this model as \u201cImitation LLM v1.\u201d We refer to an\n6\nimproved version of the imitation model that uses higher quality data as \u201cImitation LLM v2.\u201d The\nmain difference between Imitation LLM v1 and v2 is that while the v1 training dataset is fully based\non synthetic generations from a larger LLM, the v2 data additionally included human-written (i.e.,\ncontractor-written) examples. These training examples, whether synthetic or human-written, were\ncurated by asking either the LLM or human to rewrite existing ads using specific instructions, such\nas \u201cparaphrase and shorten,\u201d \u201cmake clear,\u201d \u201cmake actionable,\u201d \u201cempathize,\u201d \u201cpose as a question,\u201d\nor \u201cfocus selling point.\u201d We detail the training process of both Imitation LLM models in Section"
  },
  {
    "heading": "A.4 of the supplementary materials.",
    "content": "The work presented in this paper is subsequent to the initial release of the Text Generation prod-\nuct. Our goal in this paper is to improve the initial imitation-based Text Generation LLM, focusing\non quantifiably improving advertiser performance relative to the original model, as measured in\nterms of click-through rate (CTR). We tackle this problem using a new idea, reinforcement learning\non aggregate performance feedback signals."
  },
  {
    "heading": "Unknown",
    "content": "3"
  },
  {
    "heading": "Methods",
    "content": "In this section, we first describe the methodology (including data preparation, reward model design,\nand reinforcement learning) for training the new version of the Text Generation LLM, which we\ncall \u201cAdLlama.\u201d We then discuss the design of the experiment (i.e., A/B test) used in quantifying\nthe performance improvement of the new model compared to the naive imitation model.\n3.1"
  },
  {
    "heading": "Reinforcement Learning with Performance Feedback (RLPF)",
    "content": "Although pre-trained LLMs have absorbed vast amounts of knowledge, they are typically not ready\nfor widespread use. A crucial step before deployment to users is to align the model for use on\ndownstream tasks, which may include summarization [36], answering questions [45], engaging in\ndialogue [46], or following a wide range of instructions from users [33]. The primary approach\nfor LLM alignment involves collecting preference data from human labelers, who compare two\n7\nFigure 3: Pairwise training data from historical multitext performance outcomes. Based on\nmultitext data, data where advertisers change only the ad text while keeping all other ad components\nfixed, we are able to construct a pairwise training data point that distinguishes between text variations\nthat are preferred and not preferred, as measured by CTR. On the left, we show historical ad pairs\nthat have different text, but the same image. On the right, we show that each ad pair contributes to\na single preference row for our training data.\nresponses and indicate which is better."
  },
  {
    "heading": "The model is then fine-tuned on the preference data,",
    "content": "encouraging it to generate responses more similar to those preferred by humans [35, 33]. This\nprocess is known as reinforcement learning with human feedback (RLHF). Because the \u201cquality\u201d\nof LLM responses for many standard LLM tasks (e.g., open-ended dialogue, creative writing) is\nsubjective and hard to quantify in nature, training on human preferences is often the closest that\none can come to a well-defined optimization objective.\nOur key insight is that, unlike the above LLM use-cases, the task of crafting effective ad text\ncan be clearly associated with a quantifiable and measurable objective: the CTR1 of the ad. Note\nthat such a setup holds anytime there is a concrete performance metric and exists beyond online\n1The CTR is defined as the ratio of number of ad clicks to the total number of ad impressions (i.e., views). We\nnote that CTR is typically considered a proxy metric for true goal of conversion rate (the number of conversions, or\npurchases, per impression). However, because conversion rate is a far noisier metric to work with due to the sparsity\nof conversions, we resort to CTR.\n8\nadvertising (e.g., e-commerce, AI customer support agents, ed tech). We propose the following\ngeneral approach, which can be thought of as a metric-driven extension of RLHF:\n1. First, using aggregate performance metrics, we train a performance reward model, a model\nthat can assign a reward (i.e., a score) to a piece of text, where more performant text are given\nhigher scores.\n2. Subsequently, we use the reward model as an interactive environment to perform RL fine-\ntuning, where the goal is to fine-tune the LLM to be more likely to generate high-reward\ntext.\nWe now describe how we train a CTR-based performance reward model. Even prior to the\nlaunch of the Text Generation product, there existed a common practice among advertisers of\ntesting multiple (human-written) text variants for a single ad using one of Meta\u2019s advertiser tools\ncalled \u201cMultiple Text Optimization\u201d [47]. This practice enables us to observe historical ad data\nwhere, except for the text, all other components of the ad\u2014such as the image, title, and targeting\ncriteria\u2014remain constant. We refer to this as multitext data.\nFrom the multitext data, we are able to construct preference pairs, where the higher CTR text\nis marked as \u201cmore preferred\u201d and the lower CTR is marked as \u201cless preferred.\u201d See Figure 3\nfor an illustration of this process. We call this the pairwise dataset, which supports the standard\nBradley-Terry preference-based approach to reward model training [48, 35]. We also considered\na more naive reward modeling approach via a pointwise dataset, where each row is simply the\nad text and its resulting CTR. We can then use a standard supervised learning to train a reward\nmodel directly using CTR as labels. However, we found that the pointwise reward model was less\ncapable of discerning ordering (or ranks) between similar pieces of ad text, which is ultimately\nmore important than purely predicting CTR [49, 50]. Further details on the data curation and\nreward model training are in Sections A.1 and A.2 of the supplementary materials.\nGiven a trained reward model \ud835\udc5f\ud835\udf03, we use the proximal policy optimization (PPO) algorithm\n[33, 51] to align the LLM with high-performance ad text. We added a length penalty to counteract"
  },
  {
    "heading": "Unknown",
    "content": "9"
  },
  {
    "heading": "Unknown",
    "content": "Prior work:"
  },
  {
    "heading": "Unknown",
    "content": "Supervised fine-tuning"
  },
  {
    "heading": "Unknown",
    "content": "Synthetic LLM examples"
  },
  {
    "heading": "Unknown",
    "content": "Human-written examples"
  },
  {
    "heading": "Unknown",
    "content": "This paper:"
  },
  {
    "heading": "Reinforcement learning with",
    "content": "performance feedback"
  },
  {
    "heading": "Unknown",
    "content": "Historical performance data"
  },
  {
    "heading": "AdLlama",
    "content": "(our new contribution)"
  },
  {
    "heading": "Imitation LLM v2",
    "content": "(control version)"
  },
  {
    "heading": "Llama 2 Chat 7B",
    "content": "Figure 4: AdLlama versus Imitation LLM v2. Both AdLlama and Imitation LLM v2 originate\nfrom a base 7B Llama 2 Chat model. The difference is that AdLlama is further trained via RLPF\nand historical ad performance data, while Imitation LLM v2 is only trained using SFT to imitate\na set of curated examples (includes both LLM-generated synthetic examples and human-written\nexamples).\nthe tendency for the model to generate ad text that is too long, leading to the following PPO\noptimization formulation:\nmax\n\ud835\udf0b\ud835\udf19"
  },
  {
    "heading": "E\ud835\udc65\u223cDLLM,\ud835\udc66\u223c\ud835\udf0b\ud835\udf19(\ud835\udc66|\ud835\udc65)",
    "content": "\u0002\n\ud835\udc5f\ud835\udf03(\ud835\udc65, \ud835\udc66) \u2212\ud835\udefdKL[\ud835\udf0b\ud835\udf19(\u00b7 | \ud835\udc65), \ud835\udf0bref(\u00b7 | \ud835\udc65)] \u2212\ud835\udefclength(\ud835\udc66)\n\u0003\n,\nwhere \ud835\udf0b\ud835\udf19is the LLM, DLLM is the pointwise dataset used for LLM post-training, KL(\ud835\udc5d, \ud835\udc5e) is\nthe Kullback-Leibler divergence between two probability distributions \ud835\udc5dand \ud835\udc5e, length(\ud835\udc66) is the\nnumber of tokens in \ud835\udc66, and \ud835\udefdand \ud835\udefcare weight parameters.\nFull details of the approach are given in Section A.2 of the supplementary materials. We used\nthe RLPF technique to improve Imitation LLM v2, which, like Imitation LLM v1, is based on\nthe 7B version of Llama 2 Chat [46]. We refer to our RLPF-based ad text generation model as\n\u201cAdLlama.\u201d An illustrative comparison of the models is given in Figure 4, where we emphasize\nboth the difference in training method (RLPF versus SFT) and the training data (historical ad\nperformance versus curated examples).\n10"
  },
  {
    "heading": "Unknown",
    "content": "Nov. 23, 2023"
  },
  {
    "heading": "Unknown",
    "content": "Feb. 16, 2024"
  },
  {
    "heading": "Unknown",
    "content": "Apr. 25, 2024"
  },
  {
    "heading": "Unknown",
    "content": "Production: Imitation LLM v1"
  },
  {
    "heading": "Unknown",
    "content": "Launch of initial Text Generation model"
  },
  {
    "heading": "Unknown",
    "content": "Control: Imitation LLM v2"
  },
  {
    "heading": "Unknown",
    "content": "Test: AdLlama"
  },
  {
    "heading": "Unknown",
    "content": "A/B test start"
  },
  {
    "heading": "A/B test end",
    "content": "Figure 5: A/B test timeline. Our A/B test ran from February 16, 2024 until April 25, 2024. Prior\nto that, Imitation LLM v1 was launched on November 23, 2023 as the initial version of the Text"
  },
  {
    "heading": "Generation LLM.",
    "content": "3.2"
  },
  {
    "heading": "Experiment Design",
    "content": "We conducted a large-scale A/B test (i.e., randomized control trial) to evaluate the impact of RLPF\ntraining on advertiser performance by comparing the AdLlama model against Imitation LLM v2.\nThe A/B test ran for 10 weeks, from February 16, 2024 until April 25, 2024, on \ud835\udc41= 34,849\nadvertisers based in the United States. We randomize at the advertiser level: each advertiser is\nrandomly assigned either (1) the Imitation LLM v2 (\u201ccontrol\u201d) or (2) the AdLlama LLM trained\nwith RLPF (\u201ctest\u201d). Figure 5 shows the timeline of the A/B test and how it relates to the launch of\nthe initial Imitation LLM v1 model.\nOur primary focus is on advertiser-level performance, defined as performance aggregated over\nall direct-response2 ads created by the advertiser3 over the 10-week experimental period. We chose\nto examine advertiser-level performance because our goal is to understand how generative AI can\nimprove advertiser return on investment. More specifically, our analysis centers on the following\nmetrics: total engagement (clicks), total impressions (views), total number of ads created, and total\n2There are two broad categories of ads, direct-response and brand/awareness. Direct-response ads are those that\nhave a goal of optimizing for some form of engagement or immediate action from the user. Examples of direct-response\nadvertising goals include clicks, purchases, app installs, sign ups, donations, or subscriptions. On the other hand, brand\nads aim to drive awareness simply through views of the ad (or video), and the goal is not to drive engagement from\nthe user. Since the AdLlama model is designed to improve engagement, the relevant metric for evaluating AdLlama is"
  },
  {
    "heading": "CTR of direct-response ads.",
    "content": "3A frequent practice of advertisers is to duplicate existing ads, some of which may have been originally created\nprior to the experiment start date. Such ads are excluded from our results, but duplicates of ads created during the\nexperimental period are included.\n11\nnumber of ad variations created,4 all of which are defined at the advertiser level across the 10-week\nexperimental period on Facebook mobile feed.\nWe also log a number of advertiser covariates, which are listed in full in Section A.3.1 of\nthe supplementary materials, along with detailed descriptions. Two notable covariates are: the\nadvertiser\u2019s pre-experiment lifetime CTR across all Meta apps (\u201cpre exp ctr\u201d) and whether or not\nthe advertiser is \u201cnew,\u201d defined as whether the advertiser\u2019s pre-experiment ads have accumulated\nfewer than 1,000 impressions (\u201cis new advertiser\u201d). We also include the advertiser\u2019s ad creation\nbehavior during the period after Imitation LLM v1 was released, but before the experiment started\n(\u201cnov feb ad cnt\u201d, \u201cnov feb variant cnt\u201d, \u201chas created llm ad\u201d). This period of time is highlighted\nin blue in Figure 5 and represents the initial few months of the Text Generation feature being available\nto advertisers.\nWe also include other advertiser characteristics: vertical, expertise level, budget level, business\naccount status, and account age. In addition to covariate details, we also present descriptive statistics\nand the balance of these covariates across conditions in Section A.3.1 of the supplementary materials\n(see Table 3). We do not observe statistically significant imbalances between the two groups."
  },
  {
    "heading": "Unknown",
    "content": "4"
  },
  {
    "heading": "Main Results",
    "content": "4.1"
  },
  {
    "heading": "Advertiser Performance",
    "content": "We aim to evaluate the effect of AdLlama and Imitation LLM v2 on advertiser-level CTRs. Our main\nregression specification is a log-binomial5 regression model, with the left-hand side representing\n4Recall from Figure 2 that in the Text Generation product, a single \u201cad\u201d is associated with one or more \u201cvariations\u201d\nwhere the body text is altered.\n5Binomial regression is a natural candidate for modeling a CTR, since we are interested in clicks (successes) relative\nto impressions (total trials). Our use of a log link function leads to a relative risk rather than an odds ratio [52]. The\nrelative risk is more desirable in our setting since it can be directly interpretable as a ratio of CTRs.\n12\nTable 1: Log-binomial regression results on advertiser-level (engagement, impressions), with the\n\u201ctreatment\u201d variable being the indicator for using AdLlama. The variable \u201clog pre exp ctr existing\u201d\nrefers to the product 1existing(\ud835\udc56) \u00b7 log(CTRpre\n\ud835\udc56). Coefficients are reported on the link function scale\n(log scale); the 6.7% improvement quoted in the main text computed by 6.7% \u2248exp(0.0651) \u22121.\nFixed effect terms are omitted for brevity. All reported p-values are two-sided."
  },
  {
    "heading": "Dependent variable:",
    "content": "engagement/impressions\ntreatment\n0.0651\u2217\u2217(0.0299)\nlog pre exp ctr existing\n0.5931\u2217\u2217\u2217(0.0264)\nis new advertiser\n\u22122.3469\u2217\u2217\u2217(0.1569)\npre exp ad cnt\n0.0024 (0.0028)\npre exp impressions\n\u22120.0001 (0.0001)\npre exp engagement\n0.0036 (0.0049)\naccount age yr\n0.0008 (0.0038)\nnov feb ad cnt\n0.0013 (0.0019)\nnov feb variant cnt\n\u22120.0003 (0.0006)\nis business account\n\u22120.0286 (0.0424)\nhas created llm ad\n\u22120.0109 (0.0345)"
  },
  {
    "heading": "Constant",
    "content": "\u22121.1233\u2217\u2217\u2217(0.1506)"
  },
  {
    "heading": "Observations",
    "content": "34,849"
  },
  {
    "heading": "Note: HC1 robust standard errors in parentheses.",
    "content": "\u2217p<0.1; \u2217\u2217p<0.05; \u2217\u2217\u2217p<0.01\nthe logarithm of the CTR of advertiser \ud835\udc56:\nlog E(\ud835\udc4c\ud835\udc56/\ud835\udc5b\ud835\udc56| \ud835\udc4b\ud835\udc56) = \ud835\udefd0 + \ud835\udefd1 \u00b7 AdLlama\ud835\udc56+ \ud835\udefd2 \u00b7 1existing(\ud835\udc56) \u00b7 log(CTRpre\n\ud835\udc56) + \ud835\udefd3 \u00b7 1new(\ud835\udc56)\n+ \ud835\udf37\ud835\udc47\n4:10 \ud835\udc4d\ud835\udc56+ \ud835\udf03vert(\ud835\udc56) + \ud835\udefcbudget(\ud835\udc56) + \ud835\udf06expertise(\ud835\udc56),\n(1)\nwhere \ud835\udc4c\ud835\udc56is the number of clicks (engagement) for the \ud835\udc56-th advertiser, \ud835\udc5b\ud835\udc56is the impression count, \ud835\udc4b\ud835\udc56\nis the set of all covariates, CTRpre\n\ud835\udc56\nis the pre-experiment CTR (\u201cpre exp ctr\u201d), AdLlama\ud835\udc56is a binary\nindicator for the AdLlama LLM treatment (which is randomly assigned), 1existing\ud835\udc56is an indicator for\n\u201cis new advertiser equal to 0,\u201d 1new\ud835\udc56is an indicator for \u201cis new advertiser equal to 1,\u201d \ud835\udf03vert(\ud835\udc56) are\nvertical fixed effects, \ud835\udefcbudget(\ud835\udc56) are budget category fixed effects, \ud835\udf06expertise(\ud835\udc56) are expertise category\nfixed effects, and \ud835\udc4d\ud835\udc56is the vector of remaining numerical covariates. Note that E(\ud835\udc4c\ud835\udc56/\ud835\udc5b\ud835\udc56| \ud835\udc4b\ud835\udc56) is the"
  },
  {
    "heading": "CTR of advertiser \ud835\udc56.",
    "content": "Because some advertisers in the experiment had no pre-experiment ad impressions (or an\ninsufficient number of impressions for a reliable CTR calculation), it is not possible to naively\n13\ninclude the pre-experiment CTR as a covariate for all advertisers. Instead, we devise the following\nstrategy to properly account for pre-experiment CTR through the \ud835\udefd2 and \ud835\udefd3 terms of Equation 1.\nIf the advertiser is an \u201cexisting\u201d advertiser (i.e., has sufficient ad impressions), then we directly\nincorporate its pre-experiment CTR via the term \ud835\udefd2 \u00b7 1existing(\ud835\udc56) \u00b7 log pre exp ctr\ud835\udc56. This can be\ninterpreted as a baseline log CTR for that advertiser, which is then \u201cadjusted\u201d by the regression\nspecification based on other covariates. New advertisers do not have a reliable pre-experiment CTR,\nso we estimate a baseline log CTR using the term \ud835\udefd3 \u00b71new(\ud835\udc56). In Section A.3.2 of the supplementary\nmaterials, we show how our regression specification above leads to an intuitive interpretation.\nTable 1 presents the findings from the log-binomial regression analysis. The results indicate that\nAdLlama provides a statistically significant 6.7% increase in advertiser-level CTR (\ud835\udc5d= 0.0296\nand a standard error of 0.0299) when compared to the naive Imitation LLM v2. This roughly\ncorresponds to an absolute increase in advertiser-level CTR from 3.1% to 3.3%. Although the\nabsolute increase appears modest at first glance, a 6.7% relative increase in CTR represents a\nsubstantial improvement in an advertiser\u2019s return on investment for Facebook ads. Furthermore, on\nmature, highly-optimized ad platforms like Facebook, even small increases in CTR are typically\ndifficult to achieve.\n4.1.1"
  },
  {
    "heading": "Robustness Checks",
    "content": "We provide several robustness checks to validate the findings above. Since we did not observe\nsignificant imbalances between the control and test groups, we used model-free results as additional\nsupporting evidence (see Section A.3.3 of the supplementary materials)."
  },
  {
    "heading": "Further, in Section",
    "content": "A.3.4 of the supplementary materials, we tested various alternative CTR regression specifications,\nincluding quasi-binomial, logistic, Poisson, and quasi-Poisson regressions. These analyses yielded\nqualitatively similar effects. Finally, in Section A.3.5 of the supplementary materials, we conducted\nseparate linear regressions on clicks and impressions, providing statistical evidence that AdLlama\nincreases the total number of clicks per advertiser, while it did not affect the total number of\nimpressions delivered for the advertiser. This is further supporting evidence that CTR is increased\n14\nunder AdLlama.\n4.2"
  },
  {
    "heading": "Impact on Ad Variations Created",
    "content": "We are also interested in how AdLlama and Imitation LLM v2 affect advertisers\u2019 usage of the Text\nGeneration product. We consider two outcomes, (1) the number of ad variations created during\nthe experimental period and (2) the number of ads created during the experimental period. Recall\nthat a single \u201cad\u201d is associated with one or more \u201cvariations\u201d (see Figure 2).\nWe use a linear regression with the same covariates as we did in the log-binomial regression of\nEquation 1, except without the log-transformation6 of the pre-experimental CTR:"
  },
  {
    "heading": "Outcome\ud835\udc56= \ud835\udefd0 + \ud835\udefd1 \u00b7 AdLlama\ud835\udc56+ \ud835\udefd2 \u00b7 1existing(\ud835\udc56) \u00b7 CTRpre",
    "content": "\ud835\udc56\n+ \ud835\udefd3 \u00b7 1new(\ud835\udc56)\n+ \ud835\udf37\ud835\udc47\n4:10 \ud835\udc4d\ud835\udc56+ \ud835\udf03vert(\ud835\udc56) + \ud835\udefcbudget(\ud835\udc56) + \ud835\udf06expertise(\ud835\udc56) + \ud835\udf16\ud835\udc56.\n(2)\nHere, Outcome\ud835\udc56refers to either advertiser \ud835\udc56\u2019s variation count or ad count and \ud835\udf16\ud835\udc56is the error term.\nTable 2 reports the results of these regressions. We observe strong evidence that usage of the\nAdLlama LLM increases the number of ad variations created by the advertiser, while the total\nnumber of ads created remained statistically the same. Specifically, we see that the number of ad\nvariations increased by 3.1 (\ud835\udc5d< 0.01) from roughly 16.8 variations (Imitation LLM v2) to 19.9\n(AdLlama). This is an 18.5% increase when using the AdLlama LLM. This suggests that for each\nad, advertisers were more willing to use the Text Generation product\u2019s suggestions when they came\nfrom AdLlama compared to Imitation LLM v2."
  },
  {
    "heading": "Unknown",
    "content": "5"
  },
  {
    "heading": "Discussion",
    "content": "Our work shows that RL with performance feedback can be used to train LLMs to generate ad\ntext that resonates with advertisers and drives measurable engagement from users on Facebook.\n6We remove the log-transformation because there is no longer a log link function; see Section A.3.2 of the\nsupplementary materials for further discussion.\n15\nTable 2: Linear regression results on advertisers\u2019 ad creation behavior: \u201cvariant cnt\u201d is the number\nof ad variations created, \u201cad cnt\u201d is the number of ads created, and \u201cpre exp ctr existing\u201d refers\nto the term 1existing(\ud835\udc56) \u00b7 CTRpre\n\ud835\udc56. All other notation remains consistent with Table 1. We report\nHC1 robust standard errors. Fixed effect terms are omitted for brevity. All reported p-values are\ntwo-sided."
  },
  {
    "heading": "Dependent variable:",
    "content": "variant cnt\nad cnt\ntreatment\n3.113\u2217\u2217\u2217(0.528)\n0.040 (0.189)\npre exp ctr existing\n\u22125.915 (11.057)\n\u22126.568\u2217(3.549)\npre exp ad cnt\n3.803\u2217\u2217\u2217(0.689)\n1.564\u2217\u2217\u2217(0.280)\npre exp impressions\n0.009 (0.011)\n0.002 (0.004)\npre exp engagement\n\u22120.333 (0.254)\n\u22120.123 (0.098)\naccount age yr\n\u22120.397\u2217\u2217\u2217(0.083)\n\u22120.197\u2217\u2217\u2217(0.029)\nnov feb ad cnt\n0.750\u2217(0.441)\n0.615\u2217\u2217\u2217(0.170)\nnov feb variant cnt\n0.096 (0.137)\n\u22120.055 (0.050)\nis new advertiser\n2.656\u2217\u2217\u2217(0.776)\n0.991\u2217\u2217\u2217(0.268)\nis business account\n3.405\u2217\u2217\u2217(0.420)\n1.416\u2217\u2217\u2217(0.148)\nhas created llm ad\n2.078\u2217\u2217(1.033)\n0.280 (0.399)"
  },
  {
    "heading": "Constant",
    "content": "7.797\u2217\u2217\u2217(2.035)\n3.831\u2217\u2217\u2217(0.685)"
  },
  {
    "heading": "Observations",
    "content": "34,849\n34,849\nR2\n0.091\n0.116"
  },
  {
    "heading": "Adjusted R2",
    "content": "0.091\n0.115"
  },
  {
    "heading": "Note: HC1 robust standard errors in parentheses.",
    "content": "\u2217p<0.1; \u2217\u2217p<0.05; \u2217\u2217\u2217p<0.01\nSpecifically, our large-scale A/B test on Meta\u2019s Text Generation product shows that the RLPF-based\nmodel significantly increases advertiser-level CTRs, along with the number of ad text variations that\nadvertisers were willing to employ. These results support the concept of anchoring the fine-tuning\nprocess in real-world, aggregate performance metrics, rather than relying solely on human raters\u2019\npreference feedback or rule-based rewards.\n5.1"
  },
  {
    "heading": "Limitations",
    "content": "There are several limitations of our work that we now discuss. Our model was trained using offline\nhistorical performance data. Therefore, this is equivalent to a single round of offline RL, where there\nis no real-time interaction with the environment. To further refine our model, we could incorporate\nthe performance outcomes of LLM-generated ads in an iterative process. This approach would\nalign more closely with online RL, where the model continuously interacts (by taking actions) with\n16\nthe environment and adapts based on real-time feedback in the form of rewards and transitions [53].\nSuch a system would be more capable of adapting to new trends and perhaps even discovering new\nones through exploration, the process of experimenting with new actions (i.e., trying out new ad\ntext formats unseen in the historical data).\nOur current model primarily focuses on ad performance, but other factors are also important to\nconsider. As an example, there may be a trade-off between generating ads that perform well and\nthose that exhibit high creativity. Additionally, the model\u2019s ability to adhere to specific advertiser\ninstructions, such as maintaining a particular tone, is another important consideration. Addressing\nthese aspects would require a multi-objective optimization approach to balance various objectives\neffectively. Finally, our model currently does not take into account the human component of the\nText Generation product: before an ad text variation can be delivered to users, the advertiser must\nexplicitly select that variation for delivery. An alternative way to train the future iterations of the\nRLPF reward model is to weigh the CTR by the likelihood that the text is selected by advertisers.\nBeyond individual ad performance, platform-level factors, such as the diversity of the ad\ninventory, are also important for a positive user experience. Future work should explore strategies\nthat simultaneously consider these other factors, while also optimizing for performance.\n5.2"
  },
  {
    "heading": "Broader Implications",
    "content": "Our findings contribute to the growing body of literature on understanding the impact of LLMs.\nBy quantifying the benefits of RL-based post-training in online advertising, we provide a concrete\ndata point that highlights the potential for these models to ingest relevant performance metrics and\nsubsequently create real business impact. The ability to generate more engaging ad content not\nonly improves existing advertisers\u2019 return on investment, but could also lower the barrier to entry\nfor new and inexperienced advertisers (e.g., small businesses) by reducing the need for extensive\nmarketing expertise and resources.\nOur methodology is not limited to online advertising: the principles of RLPF can be adapted to\nother domains where aggregate performance metrics are available. By using performance data as a\n17\nfeedback mechanism, organizations can fine-tune LLMs to optimize for their desired outcomes. For\nexample, the core methodology can easily be extended to closely related settings like personalized\nemail campaigns or e-commerce product descriptions. RLPF can also be extended to settings with\nmultiple rounds of interactive feedback, such as AI customer support agents, using metrics like\nresolution rates, satisfaction scores, or user response times.\nThere are also less obvious settings where RLPF could be applied. For example, in online\nlearning platforms, student performance data (test scores and engagement metrics) could guide\nthe generation of adaptive learning content, while for certain public awareness campaigns (e.g.,\nvaccination, energy consumption), performance data could enable LLMs to rewrite communication\nmaterials to better resonate with their intended audience.\nOur work only takes the first step in demonstrating the potential of RL augmented with aggregate\nperformance feedback. We believe this is a promising and generalizable approach that bridges the\ngap between highly capable language models and tangible outcomes."
  },
  {
    "heading": "Acknowledgments",
    "content": "We gratefully acknowledge our close partnership on this project with the Monetization GenAI\nand Creative & Guidance teams at Meta: Yide Zhao, Yair Levi, Clare Zhang, Steven Barnett,\nShenghong Wang, Meilei Jiang, Jerry Pan, Sanjian Chen, Shenxiu Liu, Zhonghua Qu, Xueting"
  },
  {
    "heading": "Unknown",
    "content": "Yan, and Arghya Paul."
  },
  {
    "heading": "Author Contributions",
    "content": "A.N. and D.J. created reward model datasets and trained the reward model. D.J. and A.N. then\npost-trained the Text Generation LLM using RLPF, which resulted in the AdLlama model described\nin this paper. Y.C. curated the source datasets for both the reward model and Text Generation LLM.\nY.C. and Y.B. managed the A/B testing process. D.J. drafted the initial version of the paper; all\nauthors revised and reviewed the paper. Y.B. and Z.Z. advised on all aspects of the project.\n18"
  },
  {
    "heading": "Author Affiliations",
    "content": "All authors are either current or former employees of Meta. D.J., A.N., Y.C., and Y.B. are currently\nemployed by Meta, while Z.Z. is a former employee."
  },
  {
    "heading": "The work described in this paper was",
    "content": "conducted during Z.Z.\u2019s employment at Meta.\n19"
  },
  {
    "heading": "References",
    "content": "[1] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark\nChen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference\non Machine Learning, pages 8821\u20138831. PMLR, 2021.\n[2] OpenAI. Introducing ChatGPT, 2022. URL https://openai.com/blog/chatgpt.\n[3] Mina Lee, Percy Liang, and Qian Yang. CoAuthor: Designing a human-AI collaborative\nwriting dataset for exploring language model capabilities. In Proceedings of the 2022 CHI\nConference on Human Factors in Computing Systems, pages 1\u201319, 2022.\n[4] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni\nAleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4\ntechnical report. arXiv preprint arXiv:2303.08774, 2023.\n[5] Eric Zhou and Dokyun Lee. Generative artificial intelligence, human creativity, and art. PNAS"
  },
  {
    "heading": "Nexus, 3(3):pgae052, 2024.",
    "content": "[6] Harsh Kumar, David M Rothschild, Daniel G Goldstein, and Jake M Hofman. Math education\nwith large language models: Peril or promise? Available at SSRN 4641653, 2023.\n[7] Hamsa Bastani, Osbert Bastani, Alp Sungu, Haosen Ge, Ozge Kabakc\u0131, and Rei Mariman.\nGenerative AI without guardrails can harm learning: Evidence from high school mathematics."
  },
  {
    "heading": "PNAS (forthcoming), 2025.",
    "content": "[8] Pia Kreijkes, Viktor Kewenig, Martina Kuvalja, Mina Lee, Sylvia Vitello, Jake M Hofman,\nAbigail Sellen, Sean Rintel, Daniel G Goldstein, David M Rothschild, et al. Effects of LLM\nuse and note-taking on reading comprehension and memory: A randomised experiment in\nsecondary schools. Available at SSRN 5095149, 2025.\n[9] Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez,\n20\nTing Fang Tan, and Daniel Shu Wei Ting. Large language models in medicine. Nature"
  },
  {
    "heading": "Medicine, 29(8):1930\u20131940, 2023.",
    "content": "[10] Jan Clusmann, Fiona R Kolbinger, Hannah Sophie Muti, Zunamys I Carrero, Jan-Niklas\nEckardt, Narmin Ghaffari Laleh, Chiara Maria Lavinia L\u00a8offler, Sophie-Caroline Schwarzkopf,\nMichaela Unger, Gregory P Veldhuizen, et al. The future landscape of large language models\nin medicine. Communications Medicine, 3(1):141, 2023.\n[11] Dave Van Veen, Cara Van Uden, Louis Blankemeier, Jean-Benoit Delbrouck, Asad Aali,\nChristian Bluethgen, Anuj Pareek, Malgorzata Polacin, Eduardo Pontes Reis, Anna See-\nhofnerov\u00b4a, et al. Adapted large language models can outperform medical experts in clinical\ntext summarization. Nature Medicine, 30(4):1134\u20131142, 2024.\n[12] Andrew Sellergren, Sahar Kazemzadeh, Tiam Jaroensri, Atilla Kiraly, Madeleine Traverse,\nTimo Kohlberger, Shawn Xu, Fayaz Jamil, C\u00b4\u0131an Hughes, Charles Lau, et al. Medgemma\ntechnical report. arXiv preprint arXiv:2507.05201, 2025.\n[13] Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried,\nAndrew Goff, Jonathan Gray, Hengyuan Hu, et al. Human-level play in the game of Diplomacy\nby combining language models with strategic reasoning. Science, 378(6624):1067\u20131074,\n2022.\n[14] S\u00b4ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece\nKamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general\nintelligence: Early experiments with GPT-4. arXiv preprint arXiv:2303.12712, 2023.\n[15] Google Deepmind AlphaProof and AlphaGeometry teams. AI achieves silver-medal standard\nsolving international mathematical olympiad problems, 2024. URL https://deepmind.\ngoogle/discover/blog/ai-solves-imo-problems-at-silver-medal-level/.\n[16] Erik Brynjolfsson. The Turing trap: The promise & peril of human-like artificial intelligence."
  },
  {
    "heading": "Daedalus, 151(2):272\u2013287, 2022.",
    "content": "21\n[17] Jenna Butler, Sonia Jaffe, Nancy Baym, Mary Czerwinski, Shamsi Iqbal, Kate Nowak, Sean\nRintel, Abigail Sellen, Mihaela Vorvoreanu, Najeeb G. Abdulhzamid, Judith Amores, Reid\nAndersen, Kagonya Awori, Maxamed Axmed, danah boyd, James Brand, Georg Buscher,\nDean Carignan, Martin Chan, Adam Coleman, Scott Counts, Madeleine Daepp, Adam Four-\nney, Daniel G. Goldstein, Andy Gordon, Aaron L Halfaker, Javier Hernandez, Jake Hofman,\nJenny Lay-Flurrie, Vera Liao, Si\u02c6an Lindley, Sathish Manivannan, Charlton Mcilwain, Subigya\nNepal, Jennifer Neville, Stephanie Nyairo, Jacki O\u2019Neill, Victor Poznanski, Gonzalo Ramos,\nNagu Rangan, Lacey Rosedale, David Rothschild, Tara Safavi, Advait Sarkar, Ava Scott, Chi-\nrag Shah, Neha Parikh Shah, Teny Shapiro, Ryland Shaw, Auste Simkute, Jina Suh, Siddharth\nSuri, Ioana Tanase, Lev Tankelevitch, Adam Troy, Mengting Wan, Ryen W. White, Longqi\nYang, Brent Hecht, and Jaime Teevan. Microsoft new future of work report 2023. Microsoft\nResearch Tech Report MSR- TR-2023-34, 2023. URL https://aka.ms/nfw2023.\n[18] Manuel Hoffmann, Sam Boysel, Frank Nagle, Sida Peng, and Kevin Xu. Generative AI and\nthe nature of work. Technical report, CESifo Working Paper, 2024.\n[19] Kunal Handa, Alex Tamkin, Miles McCain, Saffron Huang, Esin Durmus, Sarah Heck, Jared\nMueller, Jerry Hong, Stuart Ritchie, Tim Belonax, et al. Which economic tasks are performed\nwith AI? Evidence from millions of Claude conversations. arXiv preprint arXiv:2503.04761,\n2025.\n[20] Harang Ju and Sinan Aral. Collaborating with AI agents: Field experiments on teamwork,\nproductivity, and performance. arXiv preprint arXiv:2503.18238, 2025.\n[21] Erik Brynjolfsson, Danielle Li, and Lindsey Raymond. Generative AI at work. The Quarterly"
  },
  {
    "heading": "Journal of Economics, page qjae044, 2025.",
    "content": "[22] Xiao Ni, Yiwei Wang, Tianjun Feng, Lauren Xiaoyuan Lu, Yitong Wang, and Congyi Zhou.\nGenerative AI in action: Field experimental evidence on worker performance in e-commerce\ncustomer service operations. Available at SSRN 5012601, 2024.\n22\n[23] Alexia Cambon, Brent Hecht, Ben Edelman, Donald Ngwe, Sonia Jaffe, Amy Heger, Mihaela\nVorvoreanu, Sida Peng, Jake Hofman, Alex Farach, et al. Early LLM-based tools for enterprise\ninformation workers likely provide meaningful boosts to productivity. Microsoft Research."
  },
  {
    "heading": "MSR-TR-2023-43, 2023.",
    "content": "[24] Jonathan H Choi and Daniel Schwarcz. AI assistance in legal analysis: An empirical study."
  },
  {
    "heading": "Available at SSRN 4539836, 2023.",
    "content": "[25] Daniel Schwarcz, Sam Manning, Patrick Barry, David R Cleveland, JJ Prescott, and Beverly\nRich. AI-powered lawyering: AI reasoning models, retrieval augmented generation, and the\nfuture of legal practice. Available at SSRN 5162111, 2025.\n[26] Fabrizio Dell\u2019Acqua, Edward McFowland III, Ethan R Mollick, Hila Lifshitz-Assaf, Katherine\nKellogg, Saran Rajendran, Lisa Krayer, Franc\u00b8ois Candelon, and Karim R Lakhani. Navigat-\ning the jagged technological frontier: Field experimental evidence of the effects of AI on\nknowledge worker productivity and quality. Available at SSRN 4573321, 2023.\n[27] Sida Peng, Eirini Kalliamvakou, Peter Cihon, and Mert Demirer."
  },
  {
    "heading": "The impact of AI on",
    "content": "developer productivity: Evidence from github copilot. arXiv preprint arXiv:2302.06590,\n2023.\n[28] Zheyuan Kevin Cui, Mert Demirer, Sonia Jaffe, Leon Musolff, Sida Peng, and Tobias Salz.\nThe effects of generative AI on high skilled work: Evidence from three field experiments with\nsoftware developers. Available at SSRN 4945566, 2024.\n[29] Panagiotis Angelopoulos, Kevin Lee, and Sanjog Misra. Causal alignment: Augmenting\nlanguage models with A/B tests. Available at SSRN 4781850, 2024.\n[30] Zenan Chen and Jason Chan. Large language model in creative work: The role of collaboration\nmodality and user expertise. Management Science, 70(12):9101\u20139117, 2024.\n23\n[31] Shakked Noy and Whitney Zhang. Experimental evidence on the productivity effects of\ngenerative artificial intelligence. Science, 381(6654):187\u2013192, 2023.\n[32] Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze\nBrahman, Lester James V Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing\nfrontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024.\n[33] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models\nto follow instructions with human feedback. Advances in Neural Information Processing"
  },
  {
    "heading": "Systems, 35:27730\u201327744, 2022.",
    "content": "[34] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny\nZhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint\narXiv:2311.07911, 2023.\n[35] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei,\nPaul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences.\narXiv preprint arXiv:1909.08593, 2019.\n[36] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec\nRadford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback.\nAdvances in Neural Information Processing Systems, 33:3008\u20133021, 2020.\n[37] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,\nDawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al."
  },
  {
    "heading": "Training a helpful",
    "content": "and harmless assistant with reinforcement learning from human feedback. arXiv preprint\narXiv:2204.05862, 2022.\n[38] Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba,\nCarlos Guestrin, Percy S Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation\n24\nframework for methods that learn from human feedback. Advances in Neural Information"
  },
  {
    "heading": "Processing Systems, 36, 2024.",
    "content": "[39] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\nShirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in\nLLMs via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.\n[40] Dentsu. Digital advertising expenditure worldwide from 2014 to 2027 (in billion U.S. dol-\nlars). Chart, Statista, December 2024. URL https://www.statista.com/statistics/\n273717/global-internet-advertising-expenditure/.\n[41] Dentsu.\nShare of digital in advertising revenue worldwide from 2023 to 2027."
  },
  {
    "heading": "Graph,",
    "content": "In Statista, December 2024. URL https://www.statista.com/statistics/375008/\nshare-digital-ad-spend-worldwide/.\n[42] Meta. About Text Generation in Meta Ads Manager, 2024. URL https://www.facebook.\ncom/business/help/180641596861873.\n[43] Meta."
  },
  {
    "heading": "Unknown",
    "content": "Create an ad with Text Generation in Meta Ads Manager, 2024."
  },
  {
    "heading": "URL https:",
    "content": "//www.facebook.com/business/help/497610041230617.\n[44] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open\nfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n[45] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christo-\npher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-\nassisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.\n[46] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,\nTimoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama:\nOpen and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n25\n[47] Meta. Creative best practices for text in ads, 2024. URL https://www.facebook.com/\nbusiness/help/223409425500940.\n[48] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei."
  },
  {
    "heading": "Unknown",
    "content": "Deep reinforcement learning from human preferences."
  },
  {
    "heading": "Unknown",
    "content": "Advances in Neural Information"
  },
  {
    "heading": "Processing Systems, 30, 2017.",
    "content": "[49] Jayanta Mandi, V\u0131ctor Bucarey, Maxime Mulamba Ke Tchomba, and Tias Guns. Decision-\nfocused learning: Through the lens of learning to rank. In International Conference on"
  },
  {
    "heading": "Machine Learning, pages 14935\u201314947. PMLR, 2022.",
    "content": "[50] Samuel Tan and Peter I Frazier. Asymptotically optimal regret for black-box predict-then-\noptimize. arXiv preprint arXiv:2406.07866, 2024.\n[51] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n[52] Mark W Donoghoe and Ian C Marschner. Logbin: An R package for relative risk regression\nusing the log-binomial model. Journal of Statistical Software, 86:1\u201322, 2018.\n[53] Richard S Sutton. Reinforcement learning: An introduction. A Bradford Book, 2018.\n[54] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization.\nIn International Conference on Machine Learning, pages 10835\u201310866. PMLR, 2023.\n[55] Keyu Nie, Yinfei Kong, Ted Tao Yuan, and Pauline Berry Burke. Dealing with ratio metrics in\nA/B testing at the presence of intra-user correlation and segments. In International Conference\non Web Information Systems Engineering (WISE), pages 563\u2013577, 2020.\n[56] Alex Deng, Ulf Knoblich, and Jiannan Lu. Applying the delta method in metric analytics:\na practical guide with novel ideas. In Proceedings of the 24th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining, pages 233\u2013242, 2018.\n26\n[57] Edward L Frome. The analysis of rates using Poisson regression models. Biometrics, pages\n665\u2013674, 1983.\nA"
  },
  {
    "heading": "Supplementary Materials",
    "content": "A.1"
  },
  {
    "heading": "Unknown",
    "content": "Training Data"
  },
  {
    "heading": "Unknown",
    "content": "A.1.1"
  },
  {
    "heading": "Reward Model",
    "content": "We obtained our reward model (RM) training data from the Multiple Text Options feature in Meta\nAds Manager [47]. This feature enables advertisers to manually submit various versions of body\ntext, title text, and description text. Multiple Text Options then automatically tests and optimizes\nto find the ad variation that most effectively engages audiences. This feature depends entirely on\nmanually-written text variations and therefore can be seen as a precursor to the AI-driven Text"
  },
  {
    "heading": "Generation feature (the focus of this paper).",
    "content": "Recall that the Text Generation feature targets the generation of ad body text. However, the CTR\nof an ad is not determined by its body text alone, but depends on multiple other factors, including\nthe ad image, the ad\u2019s targeting criteria, or the ad\u2019s vertical (e.g., engagement on real estate ads is\nexpected to be drastically different from restaurant ads). Thus, Multiple Text Options provided us\nwith an important type of data: CTR data for ads that are identical across all dimensions except for\nthe ad body text. This allows us to attribute the difference in CTR directly to the ad body text.\nWe filtered this data to include ad variations written in English with body text between 100 and\n1000 characters (we excluded ads that were too short or too long) with at least 2,000 impressions on\nFacebook. From these data, we created preference pairs of ad variations where the two individual\nvariations only differ in the ad body text, as previously illustrated in Figure 3. Our final RM training\ndataset included approximately 7 million preference pairs.\n27"
  },
  {
    "heading": "Unknown",
    "content": "A.1.2"
  },
  {
    "heading": "Language Model",
    "content": "The training data used for language model post-training (i.e., after the RM is trained) is sourced\nin the same way as described above, with the only exception being that preference pairs are not\nconstructed. We refer to this as the \u201cpointwise\u201d version of the dataset (as opposed to \u201cpairwise\u201d).\nThe pointwise dataset included approximately 5.5 million human-written text variation examples."
  },
  {
    "heading": "Note that this also includes instances",
    "content": "A.2"
  },
  {
    "heading": "Reinforcement Learning with Performance Feedback",
    "content": "We adapt the RLHF paradigm by replacing pairwise human feedback with performance feedback.\nThis change allows us to align the LLM not just with the preferences of a single human annotator\n(whose preferences might be noisy and not necessarily representative of the advertiser\u2019s goals), but\nthe ad\u2019s CTR, a real-world performance metric that summarizes how the ad interacts with both the\ndelivery system and Facebook users. Since the CTR is computed using all impressions of the ad, it\nbecomes a less noisy estimate than a label from a handful of human annotators. The RLPF training\npipeline has two main components: (1) reward model (RM) training and (2) post-training the LLM.\nFor RM training, we use the pairwise CTR preference dataset described above in Section\nA.1 of the supplementary materials, where preference pairs are decided by comparing CTRs over\nthousands of impressions (or more). We denote each row of data with a prompt \ud835\udc65and a preference\npair (\ud835\udc66\ud835\udc64, \ud835\udc66\ud835\udc59). Following prior work on RLHF [48, 35], the RM is trained with an underlying Bradley-\nTerry preference assumption, namely that the probability of ad text \ud835\udc661 being higher performing than\nad text \ud835\udc662 is given by"
  },
  {
    "heading": "P(\ud835\udc661 \u227b\ud835\udc662 | \ud835\udc65) =",
    "content": "exp(\ud835\udc5f\ud835\udf03(\ud835\udc65, \ud835\udc661))\nexp(\ud835\udc5f\ud835\udf03(\ud835\udc65, \ud835\udc661)) + exp(\ud835\udc5f\ud835\udf03(\ud835\udc65, \ud835\udc662))\n=\n1\n1 + exp[\u2212(\ud835\udc5f\ud835\udf03(\ud835\udc65, \ud835\udc661) \u2212\ud835\udc5f\ud835\udf03(\ud835\udc65, \ud835\udc662))] = \ud835\udf0e(\ud835\udc5f\ud835\udf03(\ud835\udc65, \ud835\udc661) \u2212\ud835\udc5f\ud835\udf03(\ud835\udc65, \ud835\udc662)),\nwhere \ud835\udc5f\ud835\udf03(\ud835\udc65, \ud835\udc66) is the reward model with parameters \ud835\udf03and \ud835\udf0eis the logistic function. The reward\n\ud835\udc5f\ud835\udf03(\ud835\udc65, \ud835\udc66) represents the \u201cstrength\u201d of response \ud835\udc66given prompt \ud835\udc65. Therefore, the Bradley-Terry model\n28\nrelates the preference to the relative strengths of two pieces of ad text. To fit the parameters \ud835\udf03, we\nuse maximum likelihood, arriving at the negative log-likelihood loss function:"
  },
  {
    "heading": "LRM(\ud835\udf03) = \u2212E(\ud835\udc65,\ud835\udc66\ud835\udc64,\ud835\udc66\ud835\udc59)\u223cDRM",
    "content": "\u0002\nlog \ud835\udf0e(\ud835\udc5f\ud835\udf03(\ud835\udc65, \ud835\udc66\ud835\udc64) \u2212\ud835\udc5f\ud835\udf03(\ud835\udc65, \ud835\udc66\ud835\udc59))\n\u0003\n,\nwhere DRM is the pairwise preference dataset used for RM training. We use out-of-sample pairwise\naccuracy for hyperparameter tuning (learning rate, gradient accumulation), resulting in a model\nthat reached approximately 57% out-of-sample pairwise accuracy.\nAfter RM training, we apply the proximal policy optimization (PPO) algorithm [33, 51] to\nfine-tune the LLM and align it with high-performance ad text. A length penalty was added to the\nreward to counteract a tendency for a model to generate longer text, leading to the following PPO\noptimization formulation:\nmax\n\ud835\udf0b\ud835\udf19"
  },
  {
    "heading": "E\ud835\udc65\u223cDLLM,\ud835\udc66\u223c\ud835\udf0b\ud835\udf19(\ud835\udc66|\ud835\udc65)",
    "content": "\u0002\n\ud835\udc5f\ud835\udf03(\ud835\udc65, \ud835\udc66) \u2212\ud835\udefdKL[\ud835\udf0b\ud835\udf19(\u00b7 | \ud835\udc65), \ud835\udf0bref(\u00b7 | \ud835\udc65)] \u2212\ud835\udefclength(\ud835\udc66)\n\u0003\n,\nwhere \ud835\udf0b\ud835\udf19is the LLM, DLLM is the pointwise dataset used for LLM post-training, KL(\ud835\udc5d, \ud835\udc5e) is the\nKullback-Leibler divergence between two probability distributions \ud835\udc5dand \ud835\udc5e, length(\ud835\udc66) is the number\nof tokens in \ud835\udc66, and \ud835\udefdand \ud835\udefcare weight parameters. We found that PPO can reliably increase the\nRM score, but subjective text quality started to decrease after a certain number of training steps\n(as evidenced by irrelevant or repetitive text). This is likely due to overoptimization, also known\nas reward hacking, a phenomenon where PPO starts to optimize the imperfections of the RM; see\n[54]. We deal with overoptimization by carefully selecting a model checkpoint using a combination\nof two strategies: (1) close monitoring the LLM training process using an evaluation RM trained\non a different data split and (2) a small-scale human preference labeling.\n29\nA.3"
  },
  {
    "heading": "Unknown",
    "content": "Statistical Analysis"
  },
  {
    "heading": "Unknown",
    "content": "A.3.1"
  },
  {
    "heading": "Descriptive Statistics and Covariates",
    "content": "In Table 3, we present descriptive statistics of our sample of advertisers, along with balance of\ncovariates."
  },
  {
    "heading": "Unknown",
    "content": "Table 3:"
  },
  {
    "heading": "Unknown",
    "content": "Descriptive statistics of the advertisers in our A/B test."
  },
  {
    "heading": "For categorical variables",
    "content": "(is business account, budget cat, expertise cat, vertical), group differences were assessed using\na \ud835\udf122 test. For the remaining variables, two-sample t-tests for equality of means were conducted,\nand two-sided p-values are reported. We do not observe statistically significant imbalances between\nthe groups, indicating balanced sample characteristics."
  },
  {
    "heading": "Unknown",
    "content": "Variable"
  },
  {
    "heading": "Unknown",
    "content": "Control"
  },
  {
    "heading": "Treatment",
    "content": "p-value"
  },
  {
    "heading": "Unknown",
    "content": "Imitation LLM v2"
  },
  {
    "heading": "AdLlama",
    "content": "\ud835\udc41\n17632\n17217\npre exp ctr (mean (SD))\n0.027 (0.022)\n0.027 (0.022)\n0.766\npre exp engagement (mean (SD))\n0.329 (2.556)\n0.331 (4.411)\n0.971\npre exp impressions (mean (SD))\n16.497 (144.382)\n15.811 (148.639)\n0.662\npre exp ad cnt (mean (SD))\n0.448 (1.945)\n0.454 (1.878)\n0.753\naccount age yr (mean (SD))\n3.238 (3.483)\n3.276 (3.487)\n0.307\nnov feb ad cnt (mean (SD))\n2.007 (9.038)\n2.114 (10.362)\n0.304\nnov feb variant cnt (mean (SD))\n5.983 (28.393)\n6.435 (33.546)\n0.175\nis business account = 1 (%)\n13468 (76.38)\n13113 (76.16)\n0.637\nhas created llm ad = 1 (%)\n4604 (26.11)\n4570 (26.54)\n0.366\nis new advertiser = 1 (%)\n1778 (10.08)\n1729 (10.04)\n0.912\nbudget cat (%)\n0.128\n1.Low\n8313 (47.15)\n8031 (46.65)\n2.Mid\n1901 (10.78)\n1777 (10.32)\n3.High\n7418 (42.07)\n7409 (43.03)\nexpertise cat = 2.High (%)\n2881 (16.34)\n2710 (15.74)\n0.131\nvertical (%)\n0.117"
  },
  {
    "heading": "Advertising and Marketing",
    "content": "610 (3.46)\n595 (3.46)"
  },
  {
    "heading": "Automotive",
    "content": "510 (2.89)\n512 (2.97)"
  },
  {
    "heading": "Business to Business",
    "content": "353 (2.00)\n437 (2.54)"
  },
  {
    "heading": "Consumer Packaged Goods",
    "content": "1195 (6.78)\n1130 (6.56)"
  },
  {
    "heading": "Ecommerce",
    "content": "1681 (9.53)\n1663 (9.66)"
  },
  {
    "heading": "Entertainment and Media",
    "content": "2330 (13.21)\n2318 (13.46)"
  },
  {
    "heading": "Healthcare, Pharmaceuticals, and Biotech",
    "content": "778 (4.41)\n716 (4.16)"
  },
  {
    "heading": "Other",
    "content": "1010 (5.73)\n990 (5.75)"
  },
  {
    "heading": "Professional Services",
    "content": "2825 (16.02)\n2782 (16.16)"
  },
  {
    "heading": "Publishing",
    "content": "635 (3.60)\n551 (3.20)"
  },
  {
    "heading": "Restaurants",
    "content": "454 (2.57)\n409 (2.38)"
  },
  {
    "heading": "Retail",
    "content": "3218 (18.25)\n3176 (18.45)"
  },
  {
    "heading": "Technology",
    "content": "438 (2.48)\n417 (2.42)"
  },
  {
    "heading": "Travel",
    "content": "581 (3.30)\n570 (3.31)"
  },
  {
    "heading": "Unlisted",
    "content": "1014 (5.75)\n951 (5.52)\nBelow, we give details of each of the covariates used in the study.\n30\n\u2022 pre exp ctr: Lifetime, pre-experiment CTR across all ads launched on Meta apps.\n\u2022 pre exp engagement: Lifetime, pre-experiment engagement count across all ads launched on"
  },
  {
    "heading": "Meta apps. Units are in millions.",
    "content": "\u2022 pre exp impressions: Lifetime, pre-experiment impression count across all ads launched on"
  },
  {
    "heading": "Meta apps. Units are in millions.",
    "content": "\u2022 pre exp ad cnt: Lifetime, pre-experiment ad count launched on all Meta apps. Units are in\nthousands.\n\u2022 account age yr: The number of years since the advertiser\u2019s account was created.\n\u2022 nov feb ad cnt: The number of ads created by the advertiser during the period after the\nlaunch of the initial Text Generation feature, but before the start of the A/B test.\n\u2022 nov feb variant cnt: The number of ad variants created by the advertiser during the period\nafter the launch of the initial Text Generation feature, but before the start of the A/B test.\n\u2022 is business account: A binary variable indicating whether the advertiser\u2019s account is associ-\nated with a business page.\n\u2022 has created llm ad: A binary variable indicating whether the advertiser created an LLM-\ngenerated ad during the period after the launch of the initial Text Generation feature, but\nbefore the start of the A/B test.\n\u2022 is new advertiser: A binary variable indicating whether the advertiser\u2019s ads have combined\nfor fewer than 1,000 total impressions on Meta apps (this includes many advertisers who have\nan account, but have delivered zero ad impressions).\n\u2022 budget cat: A categorical variable that groups advertisers based on their budget spend. The\npossible values are \u201c1.Low,\u201d \u201c2.Mid,\u201d and \u20183.High.\u201d\n31\n\u2022 expertise cat: A categorical variable that groups advertisers based on their expertise level\nwith Meta\u2019s ad system. High expertise means that the advertiser makes use of more advanced\nad features. The possible values are \u201c1.Low\u201d and \u201c2.High.\u201d\n\u2022 vertical: A categorical variable indicating the vertical in which the advertiser operates."
  },
  {
    "heading": "Unknown",
    "content": "Examples include \u201cRetail,\u201d \u201cTravel,\u201d or \u201cTechnology.\u201d"
  },
  {
    "heading": "A.3.2",
    "content": "Interpretation of Regression Specification under Log-link Function\nConsider the log-binomial regression model specified in (1). Exponentiating both sides, we have"
  },
  {
    "heading": "E(\ud835\udc4c\ud835\udc56/\ud835\udc5b\ud835\udc56| \ud835\udc4b\ud835\udc56) = exp\u0000\ud835\udefd2 \u00b7 1existing(\ud835\udc56) \u00b7 log(CTRpre",
    "content": "\ud835\udc56\n\u0001 + \ud835\udefd3 \u00b7 1new(\ud835\udc56)) exp(\u201cother covariates\u201d)\n=\n\u0002\n1existing(\ud835\udc56) (CTRpre\n\ud835\udc56\n\u0001 \ud835\udefd2 + 1new(\ud835\udc56) exp(\ud835\udefd3)\n\u0003\nexp(\u201cother covariates\u201d)\n= (\u201cbaseline\u201d CTR) exp(\u201cother covariates\u201d).\nThe last equality illustrates that we can interpret the term in brackets as an estimated \u201cbaseline\u201d\nCTR for advertiser \ud835\udc56based on past performance. For existing advertisers (i.e., if existing(\ud835\udc56) is\ntrue), then baseline CTR term in brackets reduces to (CTRpre\n\ud835\udc56\n\u0001 \ud835\udefd2, where we may interpret \ud835\udefd2 as an\nadjustment to convert between lifetime CTR on Meta\u2019s apps to CTR on the Facebook platform. On\nthe other hand, for new advertisers (i.e., new(\ud835\udc56) is true), we do not have an observation for their\npre-experiment CTR and the term in brackets simply reduces to exp(\ud835\udefd3). This model-based term\nserves to estimate advertiser \ud835\udc56\u2019s CTR in the absence of a pre-experiment CTR.\nTherefore, we can interpret the entire regression as a \u201cbaseline CTR\u201d further adjusted by other\ncovariates. This interpretation is possible because we incorporated a log-transformed CTRpre\n\ud835\udc56\ninto\na regression formulation with a log link function."
  },
  {
    "heading": "Unknown",
    "content": "A.3.3"
  },
  {
    "heading": "Model-free Evidence",
    "content": "It is not immediately obvious how to compute the \u201caverage\u201d CTR across the treatment and con-\ntrol groups\u2014simply computing advertiser-level CTRs and then computing sample averages treats\n32\nlow-impression advertisers and high-impression advertisers equivalently, but intuitively (absent a\nparticular regression model), high-impression advertisers offer more precise CTR observations and\nshould therefore be weighted higher.\nIt is common in A/B testing to compute the global CTR of each group by computing the ratio\nof the total number of engagements to the total number of impressions, i.e., (\u00cd\n\ud835\udc56\ud835\udc4c\ud835\udc56)/(\u00cd\n\ud835\udc56\ud835\udc5b\ud835\udc56). It\nturns out that global CTR is equivalent to an impression-weighted average, which matches our\nintuition above: (\u00cd\n\ud835\udc56\ud835\udc4c\ud835\udc56)/(\u00cd\n\ud835\udc56\ud835\udc5b\ud835\udc56) = \u00cd\n\ud835\udc56\ud835\udc64\ud835\udc56(\ud835\udc4c\ud835\udc56/\ud835\udc5b\ud835\udc56), where \ud835\udc64\ud835\udc56= \ud835\udc5b\ud835\udc56/(\u00cd\n\ud835\udc57\ud835\udc5b\ud835\udc57); see [55]. We apply the\nDelta method to compute the confidence interval around this estimate; see Equation 4 of [56] for\nan example. The model-free estimate is shown in Figure 6.\nFigure 6: Model-free CTR estimates. The control group\u2019s CTR is estimated at 0.0304, with a\n95% confidence interval of (0.0282, 0.0326). The treatment group\u2019s CTR is 0.0344, with a 95%\nconfidence interval of (0.0327, 0.0360), illustrated by the error bars in the plot. A two-sided z-test\nindicates a statistically significant difference between the groups (\ud835\udc5d= 0.0046)."
  },
  {
    "heading": "Unknown",
    "content": "A.3.4"
  },
  {
    "heading": "Binomial, Logistic, and Poisson Regressions",
    "content": "First, the full results of our main log-binomial regression specification are given in Table 4 (Table\n1 is an abbreviated version). As an alternative to the log-binomial model of Equation 1, we also\nconsider a logistic regression specification. The only difference from the log-binomial regression\nis a logit link function. These results are given in Table 5; the results are in agreement with the\n33\nlog-binomial specification.\nSo far, we have used heteroskedasticity-consistent standard errors in our models to ensure that\ninference is robust. Another approach is to use quasi-binomial regression, which introduces a\ndispersion parameter to adjust for variability exceeding that predicted by the binomial distribution,\nthereby providing more reliable standard error estimates. See Table 6 for results. We observe that\nthe standard errors under quasi-binomial specification are significantly less conservative than using\nrobust standard errors. Overall, the results are qualitatively similar.\nFinally, we consider and Poisson and quasi-Poisson regression specifications, which can be\nused to estimate rates using an \u201coffset\u201d term [57]. This is specified as follows:\nlog E(\ud835\udc4c\ud835\udc56| \ud835\udc4b\ud835\udc56) = \ud835\udefd0 + \ud835\udefd1 \u00b7 AdLlama\ud835\udc56+ \ud835\udefd2 \u00b7 1existing(\ud835\udc56) \u00b7 log(CTRpre\n\ud835\udc56) + \ud835\udefd3 \u00b7 1new(\ud835\udc56)\n+ \ud835\udf37\ud835\udc47\n4:10 \ud835\udc4d\ud835\udc56+ \ud835\udf03vert(\ud835\udc56) + \ud835\udefcbudget(\ud835\udc56) + \ud835\udf06expertise(\ud835\udc56) + log \ud835\udc5b\ud835\udc56.\nThe term log \ud835\udc5b\ud835\udc56is called the \u201coffset\u201d and its coefficient is constrained to be 1, so that when moved\nto the left hand side, we obtain log(E(\ud835\udc4c\ud835\udc56| \ud835\udc4b\ud835\udc56)/\ud835\udc5b\ud835\udc56), precisely the CTR that we wish to estimate. The\nresults for Poisson regression (with HC1 standard errors) are in Table 7 and results for the quasi-\nlikelihood variant are in Table 8. The findings are again consistent with the other specifications."
  },
  {
    "heading": "Unknown",
    "content": "A.3.5"
  },
  {
    "heading": "Separate Engagement and Impression Linear Regressions",
    "content": "Alternatively, we can consider a regression specification where engagement and impressions are\nmodeled separately as linear regressions, using the same covariates as we did above:"
  },
  {
    "heading": "Outcome\ud835\udc56= \ud835\udefd0 + \ud835\udefd1 \u00b7 AdLlama\ud835\udc56+ \ud835\udefd2 \u00b7 1existing(\ud835\udc56) \u00b7 CTRpre",
    "content": "\ud835\udc56\n+ \ud835\udefd3 \u00b7 1new(\ud835\udc56)\n+ \ud835\udf37\ud835\udc47\n4:10 \ud835\udc4d\ud835\udc56+ \ud835\udf03vert(\ud835\udc56) + \ud835\udefcbudget(\ud835\udc56) + \ud835\udf06expertise(\ud835\udc56) + \ud835\udf16\ud835\udc56,\nwhere Outcome\ud835\udc56refers to either advertiser \ud835\udc56\u2019s engagement count or impression count and \ud835\udf16\ud835\udc56is\nthe error term. The results are given in Table 9, where we find a statistically significant increase\nin engagement (+4.068, \ud835\udc5d= 0.0023) when using AdLlama, but no evidence for a change in\n34\nTable 4: The full version of Table 1 for the log-binomial regression; includes fixed effects."
  },
  {
    "heading": "Dependent variable:",
    "content": "engagement/impressions\ntreatment\n0.0651\u2217\u2217(0.0299)\nlog pre exp ctr existing\n0.5931\u2217\u2217\u2217(0.0264)\nis new advertiser\n\u22122.3469\u2217\u2217\u2217(0.1569)\npre exp ad cnt\n0.0024 (0.0028)\npre exp impressions\n\u22120.0001 (0.0001)\npre exp engagement\n0.0036 (0.0049)\naccount age yr\n0.0008 (0.0038)\nnov feb ad cnt\n0.0013 (0.0019)\nnov feb variant cnt\n\u22120.0003 (0.0006)\nis business account\n\u22120.0286 (0.0424)\nhas created llm ad\n\u22120.0109 (0.0345)\nbudget cat: 2.Mid\n0.0460 (0.0508)\nbudget cat: 3.High\n\u22120.0237 (0.0417)\nexpertise cat: 2.High\n0.0005 (0.0338)\nvertical: Automotive\n\u22120.1556 (0.1219)\nvertical: Business to Business\n\u22120.4097\u2217\u2217(0.1867)\nvertical: Consumer Packaged Goods\n\u22120.1476 (0.1161)\nvertical: Ecommerce\n\u22120.0847 (0.1149)\nvertical: Entertainment and Media\n0.0648 (0.1134)\nvertical: Healthcare, Pharmaceuticals, and Biotech\n\u22120.1418 (0.1233)\nvertical: Other\n\u22120.2076 (0.1275)\nvertical: Professional Services\n\u22120.1612 (0.1248)\nvertical: Publishing\n\u22120.0107 (0.1451)\nvertical: Restaurants\n0.0117 (0.1185)\nvertical: Retail\n\u22120.1914 (0.1179)\nvertical: Technology\n\u22120.0369 (0.1360)\nvertical: Travel\n0.0163 (0.1179)\nvertical: Unlisted\n0.1067 (0.1681)"
  },
  {
    "heading": "Constant",
    "content": "\u22121.1233\u2217\u2217\u2217(0.1506)"
  },
  {
    "heading": "Observations",
    "content": "34,849"
  },
  {
    "heading": "Note: HC1 robust standard errors in parentheses.",
    "content": "\u2217p<0.1; \u2217\u2217p<0.05; \u2217\u2217\u2217p<0.01\n35\nTable 5: Logistic regression specification results. We report HC1 robust standard errors and two-\nsided p-values."
  },
  {
    "heading": "Dependent variable:",
    "content": "engagement/impressions\ntreatment\n0.0685\u2217\u2217(0.0311)\nlog pre exp ctr existing\n0.6158\u2217\u2217\u2217(0.0277)\nis new advertiser\n\u22122.4418\u2217\u2217\u2217(0.1631)\npre exp ad cnt\n0.0024 (0.0029)\npre exp impressions\n\u22120.0001 (0.0001)\npre exp engagement\n0.0036 (0.0052)\naccount age yr\n0.0009 (0.0040)\nnov feb ad cnt\n0.0013 (0.0020)\nnov feb variant cnt\n\u22120.0003 (0.0006)\nis business account\n\u22120.0318 (0.0444)\nhas created llm ad\n\u22120.0118 (0.0357)\nbudget cat: 2.Mid\n0.0484 (0.0528)\nbudget cat: 3.High\n\u22120.0271 (0.0433)\nexpertise cat: 2.High\n0.0006 (0.0351)\nvertical: Automotive\n\u22120.1572 (0.1275)\nvertical: Business to Business\n\u22120.4172\u2217\u2217(0.1914)\nvertical: Consumer Packaged Goods\n\u22120.1497 (0.1213)\nvertical: Ecommerce\n\u22120.0848 (0.1202)\nvertical: Entertainment and Media\n0.0736 (0.1187)\nvertical: Healthcare, Pharmaceuticals, and Biotech\n\u22120.1415 (0.1286)\nvertical: Other\n\u22120.2132 (0.1330)\nvertical: Professional Services\n\u22120.1641 (0.1302)\nvertical: Publishing\n\u22120.0044 (0.1518)\nvertical: Restaurants\n0.0178 (0.1239)\nvertical: Retail\n\u22120.1952 (0.1232)\nvertical: Technology\n\u22120.0347 (0.1420)\nvertical: Travel\n0.0207 (0.1233)\nvertical: Unlisted\n0.1156 (0.1747)"
  },
  {
    "heading": "Constant",
    "content": "\u22121.0005\u2217\u2217\u2217(0.1584)"
  },
  {
    "heading": "Observations",
    "content": "34,849"
  },
  {
    "heading": "Note: HC1 robust standard errors in parentheses.",
    "content": "\u2217p<0.1; \u2217\u2217p<0.05; \u2217\u2217\u2217p<0.01\n36\nTable 6: Quasi-binomial regression specification results for both log and logit link functions. The\nfitted dispersion parameters are 8.427 and 8.433, respectively. We report two-sided p-values."
  },
  {
    "heading": "Dependent variable:",
    "content": "engagement/impressions\nlink = log\nlink = logit\ntreatment\n0.0651\u2217\u2217\u2217(0.0073)\n0.0685\u2217\u2217\u2217(0.0076)\nlog pre exp ctr existing\n0.5931\u2217\u2217\u2217(0.0069)\n0.6158\u2217\u2217\u2217(0.0073)\nis new advertiser\n\u22122.3469\u2217\u2217\u2217(0.0339)\n\u22122.4418\u2217\u2217\u2217(0.0354)\npre exp ad cnt\n0.0024\u2217(0.0012)\n0.0024\u2217(0.0012)\npre exp impressions\n\u22120.0001\u2217\u2217\u2217(0.00003)\n\u22120.0001\u2217\u2217\u2217(0.00003)\npre exp engagement\n0.0036\u2217\u2217\u2217(0.0011)\n0.0036\u2217\u2217\u2217(0.0011)\naccount age yr\n0.0008 (0.0010)\n0.0009 (0.0011)\nnov feb ad cnt\n0.0013\u2217(0.0007)\n0.0013\u2217(0.0007)\nnov feb variant cnt\n\u22120.0003 (0.0002)\n\u22120.0003 (0.0002)\nis business account\n\u22120.0286\u2217\u2217(0.0115)\n\u22120.0318\u2217\u2217\u2217(0.0120)\nhas created llm ad\n\u22120.0109 (0.0085)\n\u22120.0118 (0.0088)\nbudget cat2.Mid\n0.0460\u2217\u2217\u2217(0.0165)\n0.0484\u2217\u2217\u2217(0.0172)\nbudget cat3.High\n\u22120.0237\u2217\u2217(0.0105)\n\u22120.0271\u2217\u2217(0.0109)\nexpertise cat2.High\n0.0005 (0.0091)\n0.0006 (0.0095)\nvertical: Automotive\n\u22120.1556\u2217\u2217\u2217(0.0298)\n\u22120.1572\u2217\u2217\u2217(0.0310)\nvertical: Business to Business\n\u22120.4097\u2217\u2217\u2217(0.0316)\n\u22120.4172\u2217\u2217\u2217(0.0327)\nvertical: Consumer Packaged Goods\n\u22120.1476\u2217\u2217\u2217(0.0257)\n\u22120.1497\u2217\u2217\u2217(0.0268)\nvertical: Ecommerce\n\u22120.0847\u2217\u2217\u2217(0.0250)\n\u22120.0848\u2217\u2217\u2217(0.0261)\nvertical: Entertainment and Media\n0.0648\u2217\u2217\u2217(0.0244)\n0.0736\u2217\u2217\u2217(0.0255)\nvertical: Healthcare, Pharmaceuticals, and Biotech\n\u22120.1418\u2217\u2217\u2217(0.0323)\n\u22120.1415\u2217\u2217\u2217(0.0337)\nvertical: Other\n\u22120.2076\u2217\u2217\u2217(0.0291)\n\u22120.2132\u2217\u2217\u2217(0.0302)\nvertical: Professional Services\n\u22120.1612\u2217\u2217\u2217(0.0260)\n\u22120.1641\u2217\u2217\u2217(0.0270)\nvertical: Publishing\n\u22120.0107 (0.0274)\n\u22120.0044 (0.0286)\nvertical: Restaurants\n0.0117 (0.0333)\n0.0178 (0.0349)\nvertical: Retail\n\u22120.1914\u2217\u2217\u2217(0.0248)\n\u22120.1952\u2217\u2217\u2217(0.0258)\nvertical: Technology\n\u22120.0369 (0.0308)\n\u22120.0347 (0.0319)\nvertical: Travel\n0.0163 (0.0278)\n0.0207 (0.0290)\nvertical: Unlisted\n0.1067\u2217\u2217\u2217(0.0390)\n0.1156\u2217\u2217\u2217(0.0405)"
  },
  {
    "heading": "Constant",
    "content": "\u22121.1233\u2217\u2217\u2217(0.0355)\n\u22121.0005\u2217\u2217\u2217(0.0373)"
  },
  {
    "heading": "Observations",
    "content": "34,849\n34,849"
  },
  {
    "heading": "Note: Standard errors adjusted by the dispersion parameter.",
    "content": "\u2217p<0.1; \u2217\u2217p<0.05; \u2217\u2217\u2217p<0.01\n37\nTable 7: Poisson regression results using log impressions as an offset; see Equation A.3.4. We\nreport HC1 robust standard errors and two-sided p-values."
  },
  {
    "heading": "Dependent variable:",
    "content": "engagement/impressions\ntreatment\n0.0658\u2217\u2217(0.0299)\nlog pre exp ctr existing\n0.5913\u2217\u2217\u2217(0.0263)\nis new advertiser\n\u22122.3431\u2217\u2217\u2217(0.1569)\npre exp ad cnt\n0.0023 (0.0028)\npre exp impressions\n\u22120.0001 (0.0001)\npre exp engagement\n0.0036 (0.0049)\naccount age yr\n0.0008 (0.0038)\nnov feb ad cnt\n0.0013 (0.0019)\nnov feb variant cnt\n\u22120.0003 (0.0006)\nis business account\n\u22120.0299 (0.0425)\nhas created llm ad\n\u22120.0110 (0.0344)\nbudget cat: 2.Mid\n0.0461 (0.0506)\nbudget cat: 3.High\n\u22120.0259 (0.0417)\nexpertise cat2.High\n0.0001 (0.0338)\nvertical: Automotive\n\u22120.1515 (0.1224)\nvertical: Business to Business\n\u22120.4075\u2217\u2217(0.1862)\nvertical: Consumer Packaged Goods\n\u22120.1442 (0.1165)\nvertical: Ecommerce\n\u22120.0812 (0.1153)\nvertical: Entertainment and Media\n0.0705 (0.1137)\nvertical: Healthcare, Pharmaceuticals, and Biotech\n\u22120.1364 (0.1233)\nvertical: Other\n\u22120.2064 (0.1277)\nvertical: Professional Services\n\u22120.1581 (0.1253)\nvertical: Publishing\n\u22120.0045 (0.1451)\nvertical: Restaurants\n0.0172 (0.1187)\nvertical: Retail\n\u22120.1868 (0.1182)\nvertical: Technology\n\u22120.0330 (0.1366)\nvertical: Travel\n0.0206 (0.1182)\nvertical: Unlisted\n0.1119 (0.1686)"
  },
  {
    "heading": "Constant",
    "content": "\u22121.1313\u2217\u2217\u2217(0.1508)"
  },
  {
    "heading": "Observations",
    "content": "34,849"
  },
  {
    "heading": "Note: HC1 robust standard errors in parentheses.",
    "content": "\u2217p<0.1; \u2217\u2217p<0.05; \u2217\u2217\u2217p<0.01\n38\nTable 8: Quasi-Poisson regression specification results. The fitted dispersion parameter is 8.127."
  },
  {
    "heading": "Unknown",
    "content": "We report two-sided p-values."
  },
  {
    "heading": "Dependent variable:",
    "content": "engagement/impressions\ntreatment\n0.0658\u2217\u2217\u2217(0.0073)\nlog pre exp ctr existing\n0.5913\u2217\u2217\u2217(0.0070)\nis new advertiser\n\u22122.3431\u2217\u2217\u2217(0.0340)\npre exp ad cnt\n0.0023\u2217(0.0012)\npre exp impressions\n\u22120.0001\u2217\u2217\u2217(0.00003)\npre exp engagement\n0.0036\u2217\u2217\u2217(0.0011)\naccount age yr\n0.0008 (0.0010)\nnov feb ad cnt\n0.0013\u2217(0.0007)\nnov feb variant cnt\n\u22120.0003 (0.0002)\nis business account\n\u22120.0299\u2217\u2217\u2217(0.0115)\nhas created llm ad\n\u22120.0110 (0.0085)\nbudget cat2.Mid\n0.0461\u2217\u2217\u2217(0.0166)\nbudget cat3.High\n\u22120.0259\u2217\u2217(0.0105)\nexpertise cat2.High\n0.0001 (0.0091)\nvertical: Automotive\n\u22120.1515\u2217\u2217\u2217(0.0299)\nvertical: Business to Business\n\u22120.4075\u2217\u2217\u2217(0.0316)\nvertical: Consumer Packaged Goods\n\u22120.1442\u2217\u2217\u2217(0.0258)\nvertical: Ecommerce\n\u22120.0812\u2217\u2217\u2217(0.0251)\nvertical: Entertainment and Media\n0.0705\u2217\u2217\u2217(0.0245)\nvertical: Healthcare, Pharmaceuticals, and Biotech\n\u22120.1364\u2217\u2217\u2217(0.0324)\nvertical: Other\n\u22120.2064\u2217\u2217\u2217(0.0291)\nvertical: Professional Services\n\u22120.1581\u2217\u2217\u2217(0.0261)\nvertical: Publishing\n\u22120.0045 (0.0275)\nvertical: Restaurants\n0.0172 (0.0335)\nvertical: Retail\n\u22120.1868\u2217\u2217\u2217(0.0249)\nvertical: Technology\n\u22120.0330 (0.0308)\nvertical: Travel\n0.0206 (0.0279)\nvertical: Unlisted\n0.1119\u2217\u2217\u2217(0.0391)"
  },
  {
    "heading": "Constant",
    "content": "\u22121.1313\u2217\u2217\u2217(0.0357)"
  },
  {
    "heading": "Observations",
    "content": "34,849"
  },
  {
    "heading": "Note: Standard errors adjusted by the dispersion parameter.",
    "content": "\u2217p<0.1; \u2217\u2217p<0.05; \u2217\u2217\u2217p<0.01\n39\nimpressions. Increased engagement while impressions are held constant is consistent with the\nresult of increased CTR from the log-binomial regression of Table 1.\nTable 9: Separate linear regression results on engagements and impressions. All other notation\nremains consistent with Table 1. We report HC1 robust standard errors."
  },
  {
    "heading": "Dependent variable:",
    "content": "engagement\nimpressions\ntreatment\n4.068\u2217\u2217\u2217(1.334)\n54.637 (49.730)\npre exp ctr existing\n175.495\u2217\u2217\u2217(31.954)\n\u22121,982.590\u2217\u2217\u2217(692.366)\npre exp ad cnt\n1.248 (0.937)\n44.755 (31.544)\npre exp engagement\n0.407 (1.151)\n\u221237.671 (31.945)\npre exp impressions\n0.032\u2217(0.019)\n2.686\u2217\u2217\u2217(0.794)\naccount age yr\n0.671\u2217\u2217\u2217(0.227)\n21.168\u2217\u2217\u2217(8.094)\nnov feb ad cnt\n0.255 (0.470)\n\u22120.976 (13.499)\nnov feb variant cnt\n0.081 (0.156)\n4.943 (4.559)\nis new advertiser\n8.978\u2217\u2217\u2217(2.843)\n174.461 (207.469)\nis business account\n5.757\u2217\u2217\u2217(1.143)\n183.652\u2217\u2217\u2217(47.473)\nhas created llm ad\n5.677\u2217\u2217\u2217(1.847)\n191.082\u2217\u2217\u2217(61.552)\nbudget cat: 2.Mid\n0.553 (1.557)\n\u221232.907 (40.244)\nbudget cat: 3.High\n14.228\u2217\u2217\u2217(1.603)\n396.337\u2217\u2217\u2217(54.707)\nexpertise cat: 2.High\n\u22121.183 (2.306)\n21.213 (78.768)\nvertical: Automotive\n8.504\u2217\u2217(4.031)\n349.443\u2217\u2217(162.753)\nvertical: Business to Business\n9.267 (7.137)\n847.067 (548.643)\nvertical: Consumer Packaged Goods\n4.400 (4.069)\n249.672 (153.272)\nvertical: Ecommerce\n7.378\u2217(3.823)\n265.690\u2217(138.991)\nvertical: Entertainment and Media\n12.558\u2217\u2217\u2217(3.645)\n295.972\u2217\u2217(129.309)\nvertical: Healthcare, Pharmaceuticals, and Biotech\n1.378 (3.413)\n57.893 (130.343)\nvertical: Other\n2.752 (3.158)\n170.415 (124.527)\nvertical: Professional Services\n\u22120.467 (3.041)\n43.760 (124.657)\nvertical: Publishing\n17.857\u2217\u2217\u2217(6.602)\n410.544\u2217\u2217(208.360)\nvertical: Restaurants\n3.119 (3.548)\n75.335 (125.864)\nvertical: Retail\n4.415 (3.174)\n197.073 (126.678)\nvertical: Technology\n9.659 (8.157)\n376.426 (309.201)\nvertical: Travel\n14.820\u2217\u2217\u2217(4.668)\n406.783\u2217\u2217(166.148)\nvertical: Unlisted\n3.004 (3.623)\n17.970 (190.378)"
  },
  {
    "heading": "Constant",
    "content": "\u221211.150\u2217\u2217\u2217(3.526)\n\u2212140.400 (140.123)"
  },
  {
    "heading": "Observations",
    "content": "34,849\n34,849\nR2\n0.017\n0.015"
  },
  {
    "heading": "Adjusted R2",
    "content": "0.016\n0.015"
  },
  {
    "heading": "Note: HC1 robust standard errors in parentheses.",
    "content": "\u2217p<0.1; \u2217\u2217p<0.05; \u2217\u2217\u2217p<0.01\nA.4"
  },
  {
    "heading": "Imitation LLM Training Details",
    "content": "Imitation LLM v1 and v2 were developed internally at Meta prior to the RLPF work described in\nthis paper. Here, we give a brief description of the training process for these models.\n40\n\u2022 Both of these models used supervised fine-tuning (SFT), with the goal of imitating \u201cgood\u201d\nad text. This follows the instruction-tuning paradigm [33], where each row of training data is\nof the form (input ad text, target ad text). The input ad text represents the advertiser\u2019s original\nad text and the target ad text is a rewritten variation (i.e., an \u201cimproved\u201d version). Running\nSFT on this data makes shifts the LLM\u2019s response distribution to be more likely to generate\nthe target ad text; in other words, \u201cimitating\u201d it.\n\u2022 For Imitation LLM v1, based on 7B Llama 2 Chat, the target ad texts were synthetically\ndistilled from the 70B Llama 2 Chat model, a more capable but larger model [44]. Instructions\nused to prompt the larger model are common ones used in ad copywriting, such as \u201cparaphrase\nand shorten\u201d, \u201cmake clear\u201d, \u201cmake actionable\u201d, \u201cempathize\u201d, \u201cpose as a question\u201d, or \u201cfocus\nselling point.\u201d\n\u2022 For Imitation LLM v2, in addition to synthetically distilled target ad texts, human-written\nad texts were also added into the mix. This data was collected using similar instructions\nas for Imitation LLM v1. The synthetic dataset typically offers more creativity, but suffers\nfrom higher hallucination rates. On the other hand, the human-rewritten dataset oftentimes\nhas higher quality, but may lack diversity. By training on both, Imitation LLM v2 struck a\nbalance between creativity and quality.\n41"
  }
]